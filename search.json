[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science and Society with R",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.\nThis book is free and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Data Science and Society with R",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT/DSDA 1010 in Fall 2025 are developed by Professor Jun Yan, with help from generative AI and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our GitHub repository at https://github.com/statds/1010f25.\nStudents are welcome to contribute to the lecture notes by submitting pull requests to our GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Data Science and Society with R",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science. Examples tasks are: install needed software (or even hardware); search and find solutions to encountered problems.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-tools",
    "href": "index.html#course-tools",
    "title": "Data Science and Society with R",
    "section": "Course Tools",
    "text": "Course Tools\n\nR & RStudio for analysis\nQuarto for reproducible documents and dashboards\nGit & GitHub for version control and project management\nCommand line for automation and efficiency",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#policies-syllabus",
    "href": "index.html#policies-syllabus",
    "title": "Data Science and Society with R",
    "section": "Policies & Syllabus",
    "text": "Policies & Syllabus\nSee the course syllabus on HuskyCT.\nKey reminders: academic integrity, no AI-generated text in graded submissions, and professional email etiquette.\n\nGrading Rubrics\nBaseline (C level work)\n\nYour .qmd file knits to HTML without errors.\nYou answer questions correctly but do not use complete sentences.\nThere are typos and ‘junk code’ throughout the document.\nYou do not put much thought or effort into the reflection answers.\nYou do not follow the good styles in using R, Quarto, and Git.\n\nAverage (B level work)\n\nYou use complete sentences to answer questions.\nYou attempt every exercise/question.\n\nAdvanced (A level work)\n\nYour code is simple and concise.\nUnnecessary messages from R are hidden from being displayed in the HTML.\nYour document is typo-free.\nYou practice all the good styles of using R, Quarto, and Git.\nAt the discretion of the instructor, you give exceptionally thoughtful or insightful responses.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#homework-logistics",
    "href": "index.html#homework-logistics",
    "title": "Data Science and Society with R",
    "section": "Homework Logistics",
    "text": "Homework Logistics\n\nWorkflow of Submitting Homework Assignment\n\nClick the GitHub classroom assignment link in HuskyCT announcement.\nAccept the assignment and follow the instructions to an empty repository.\nMake a clone of the repo at an appropriate folder on your own computer with git clone.\nGo to this folder, add your qmd source, work on it, and group your changes to different, meaningful commits.\nPush your work to your GitHub repo with git push.\nCreate a new release and put the generated pdf file in it for ease of grading.\n\n\n\nHomework Requirements\n\nUse the repo from Git Classroom to submit your work. See Section 2  Project Management with Git.\n\nKeep the repo clean (no tracking generated files).\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\nMake at least 10 commits and form a style of frequent small commits.\n\nUse quarto source only. See Install R, Positron (or RStudio), and Quarto.\nFor the convenience of grading, add your standalone pdf output to a release in your repo.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#schedule-and-readings",
    "href": "index.html#schedule-and-readings",
    "title": "Data Science and Society with R",
    "section": "Schedule and Readings",
    "text": "Schedule and Readings\n\nComputing environment\n\nR4DS Ch 28-29\nHGR Ch 20-23\n\nJump start with R\n\nR4DS Ch 4-8; Ch 20-24\n\nVisualization\n\nR4DS Ch 1; Ch 9; Ch 11\nData visualization in R\n\nData manipulation\n\nR4DS Ch 3\n\nExploring data\n\nR4DS Ch 10; Ch 12-13\n\nTidy data\n\nR4DS Ch 5; Ch 19.1-19.2\n\nRelational Daa\n\nR4DS Ch 14; Ch 16; Ch 17; Ch 19.3-19.6\n\n\n\n\n\n\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "01-computing.html",
    "href": "01-computing.html",
    "title": "1  Setting up Computing Environment",
    "section": "",
    "text": "1.1 Operating Systems\nThis chapter reviews operating systems, files, folders, paths, terminals; installing R, Positron/RStudio, and Quarto.\nMost students are familiar with Windows, but data science workflows often run on macOS or Linux. To keep everyone on the same page for command-line work, we will treat macOS and Linux as a single “Unix-like” family and help Windows users bridge to the same environment via WSL (Windows Subsystem for Linux).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Computing Environment</span>"
    ]
  },
  {
    "objectID": "01-computing.html#operating-systems",
    "href": "01-computing.html#operating-systems",
    "title": "1  Setting up Computing Environment",
    "section": "",
    "text": "Note\n\n\n\nWhat is an Operating System?\nAn operating system (OS) manages your computer’s resources (CPU, memory, storage, files, and processes) and provides interfaces (GUI and terminal) for people and programs to interact with hardware.\n\n\n\n1.1.1 The big three\n\nWindows — Ubiquitous on personal laptops; historically less aligned with Unix tooling, but excellent with WSL.\nmacOS — Unix-based; ships with a terminal and many developer tools out of the box.\nLinux — Open-source family used on servers, clusters, and cloud VMs; many distributions (Ubuntu, Fedora, Debian) share common command-line tools.\n\n\n\n\n\n\n\nTip\n\n\n\nWhy this matters in data science\nReproducibility and collaboration require knowing your OS, versions, and paths. Most research servers and HPC clusters run Linux. Learning a command-line interface (CLI) gives you a common language across systems.\n\n\n\n\n1.1.2 Quick checks: what am I running?\nOpen a terminal and run one of the following:\n# macOS / Linux / WSL\nuname -a\n# Windows (PowerShell)\nver\nIf you see Linux details on a Windows laptop, you are inside WSL.\n\n\n1.1.3 File systems and paths\n\nUnix-like path style: /home/alex/project/data.csv\nWindows path style: C:\\\\Users\\\\Alex\\\\project\\\\data.csv\n\nOn macOS/Linux, your home is typically /Users/&lt;name&gt; (macOS) or /home/&lt;name&gt; (Linux). On Windows, it is usually C:\\\\Users\\\\&lt;name&gt;.\n\n\n\n\n\n\nImportant\n\n\n\nNaming habits that save you pain\nAvoid spaces in file and folder names. Prefer kebab-case or snake-case. Keep a project’s scripts, data, and reports together.\n\n\n\n\n1.1.4 Windows Subsystem for Linux (WSL)\nWSL lets you run a real Linux environment (e.g., Ubuntu) inside Windows, so your terminal commands match those of macOS/Linux users. This is the recommended setup for Windows in this course.\n\n1.1.4.1 Install WSL (Windows 11 or Windows 10 ≥ 2004)\n\nOpen PowerShell as Administrator.\n\nRun:\n\nwsl --install\n\nWhen prompted, choose Ubuntu and set a Linux username and password.\n\n(Optional) Ensure WSL2 is the default:\n\nwsl --set-default-version 2\n\nVerify:\n\nwsl --status\nwsl -l -v\nYou should see Ubuntu listed and version 2.\n\n\n1.1.4.2 Start using WSL\n\nLaunch the Ubuntu app (or run wsl in PowerShell).\n\nYou are now at a Linux shell (bash). Try:\n\npwd         # current directory in the Linux filesystem\nls          # list files\nwhoami      # your Linux username\n\n\n1.1.4.3 Sharing files between Windows and WSL\n\nWindows drives are mounted inside Linux at /mnt/c, /mnt/d, …\nExample: C:\\\\Users\\\\Alex\\\\project appears at /mnt/c/Users/Alex/project inside WSL.\n\nYour Linux home is separate (e.g., /home/alex).\n\n\n\n\n\n\n\nTip\n\n\n\nBest practice\nKeep course projects inside your Linux home (e.g., /home/&lt;name&gt;/dsda1010) to avoid path and permissions surprises.\n\n\n\n\n1.1.4.4 Line endings and Git on Windows/WSL\nConfigure Git once to avoid CRLF/LF confusion:\ngit config --global core.autocrlf input   # recommended in WSL/macOS/Linux\ngit config --global init.defaultBranch main\nAdd a .gitattributes to normalize endings:\n* text=auto\n*.qmd text eol=lf\n*.R   text eol=lf\n*.md  text eol=lf\n*.yml text eol=lf\n\n\n\n1.1.5 Terminal quickstart by OS\n\nmacOS: Open Terminal (or iTerm2). You are in a Unix shell. Commands from the book work as-is.\n\nLinux: Open your terminal (GNOME Terminal, Konsole, etc.). You are already in a Unix shell.\n\nWindows (WSL): Open the Ubuntu app (WSL). You are now in a Linux shell that matches macOS/Linux.\n\nHere are several commonly used shell commands:\n\ncd: change directory; .. means parent directory.\npwd: present working directory.\nls: list the content of a folder; -l long version; -a show hidden files; -t ordered by modification time.\nmkdir: create a new directory.\ncp: copy file/folder from a source to a target.\nmv: move file/folder from a source to a target.\nrm: remove a file a folder.\n\n\n\n1.1.6 Common pitfalls and fixes\n\n“Command not found” — The program is not installed, or your PATH does not include it.\nPermission denied — You may be in a protected folder; work in your home directory.\nStrange characters in filenames — Avoid spaces and punctuation; stick to letters, numbers, dashes.\nMixing Windows and WSL paths — Prefer working inside your WSL home. If you must access Windows files, use /mnt/c/....\n\n\n\n1.1.7 Hands-on check (to do now)\n\nOpen your terminal (macOS/Linux/WSL).\n\nConfirm your OS with uname -a (or ver in PowerShell).\n\nCreate a course folder and a notes file:\n\nmkdir -p ~/dsda1010/week1\ncd ~/dsda1010/week1\necho \"Week 1 notes\" &gt; notes.txt\nls -l",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Computing Environment</span>"
    ]
  },
  {
    "objectID": "01-computing.html#files-folders-and-paths",
    "href": "01-computing.html#files-folders-and-paths",
    "title": "1  Setting up Computing Environment",
    "section": "1.2 Files, folders, and paths",
    "text": "1.2 Files, folders, and paths\n\nHome folder: Your personal workspace.\n\nAbsolute vs relative paths:\n\n# absolute (macOS/Linux)\n/Users/alex/projects/dsda1010\n\n# absolute (Windows)\nC:\\Users\\alex\\projects\\dsda1010\n\n# relative (from a project root)\n../data/nyc311.csv\n\nGood habits\n\nAvoid spaces in file/folder names (use-kebab-case).\nKeep project files together (R scripts, data, and reports).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Computing Environment</span>"
    ]
  },
  {
    "objectID": "01-computing.html#extensions-and-line-endings",
    "href": "01-computing.html#extensions-and-line-endings",
    "title": "1  Setting up Computing Environment",
    "section": "1.3 Extensions and line endings",
    "text": "1.3 Extensions and line endings\n\nCommon types: .qmd, .R, .csv, .tsv, .parquet, .md.\nText vs binary: keep data in text formats when possible.\nLine endings: Git normalizes these; we set .gitattributes to help.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Computing Environment</span>"
    ]
  },
  {
    "objectID": "01-computing.html#terminals-and-shells",
    "href": "01-computing.html#terminals-and-shells",
    "title": "1  Setting up Computing Environment",
    "section": "1.4 Terminals and shells",
    "text": "1.4 Terminals and shells\nOpen a terminal and try:\npwd        # print working directory\nls         # list files\ncd ..      # move up one directory\nmkdir lab  # make a folder\ncd lab       # go into the newly created folder\n\n\n\n\n\n\nTip\n\n\n\nUse Tab to autocomplete file and folder names.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Computing Environment</span>"
    ]
  },
  {
    "objectID": "01-computing.html#sec-quarto",
    "href": "01-computing.html#sec-quarto",
    "title": "1  Setting up Computing Environment",
    "section": "1.5 Install R, Positron (or RStudio), and Quarto",
    "text": "1.5 Install R, Positron (or RStudio), and Quarto\n\nInstall R from CRAN.\nInstall Positron or RStudio Desktop.\nInstall Quarto\nVerify:\n\nR --version\nquarto --version\nIn RStudio, create a new Quarto document and render it.\n\n1.5.1 Your first Quarto project\nFrom a terminal:\nmkdir dsda1010\ncd dsda1010\n# Edit hw-template.qmd\nquarto render hw-template.qmd\nOpen the folder in RStudio and inspect the generated files.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Computing Environment</span>"
    ]
  },
  {
    "objectID": "01-computing.html#troubleshooting-checklist",
    "href": "01-computing.html#troubleshooting-checklist",
    "title": "1  Setting up Computing Environment",
    "section": "1.6 Troubleshooting checklist",
    "text": "1.6 Troubleshooting checklist\n\nPATH issues: can the terminal find R and quarto?\n\nPermissions: can you write to your project folder?\n\nAntivirus or VPN blocking downloads?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Computing Environment</span>"
    ]
  },
  {
    "objectID": "02-git.html",
    "href": "02-git.html",
    "title": "2  Project Management with Git",
    "section": "",
    "text": "2.1 Install and configure Git",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "02-git.html#install-and-configure-git",
    "href": "02-git.html#install-and-configure-git",
    "title": "2  Project Management with Git",
    "section": "",
    "text": "2.1.1 Install Git\n\nmacOS: Install the Command Line Tools or use Homebrew.\n\n# Option A: Trigger Apple CLT install when you first run git\nxcode-select --install\n\n# Option B: Homebrew (preferred if you use brew)\nbrew install git\n\nWindows: Use Git for Windows (includes Git Bash) or enable WSL and install Git inside Ubuntu.\n\n# WSL (Ubuntu) inside Windows\nsudo apt update && sudo apt install -y git\n\nLinux:\n\nsudo apt update && sudo apt install -y git   # Debian/Ubuntu\n# or\nsudo dnf install -y git                       # Fedora\n\n\n2.1.2 Identify yourself to Git\nSet your name and email (must match the email used on GitHub for a clean history):\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"netid@uconn.edu\"\nOptional but recommended:\n# Show colored output and a friendlier log\ngit config --global color.ui auto\ngit config --global init.defaultBranch main\n\n# Better default editor (choose one you actually use)\n# git config --global core.editor \"code --wait\"   # VS Code\n\n\n2.1.3 Connect to GitHub: HTTPS vs SSH\n\nHTTPS + Personal Access Token (PAT): simplest to start; you paste a token when Git asks for a password.\n\nSSH keys: more convenient long-term (no token prompts). Recommended if you frequently push/pull.\n\n\n2.1.3.1 Create and add an SSH key\n# Generate a modern Ed25519 key\nssh-keygen -t ed25519 -C \"netid@uconn.edu\"\n# Press Enter to accept default path; set a passphrase when prompted\n\n# Start the agent and add your key\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n\n# Print the public key and copy it\ncat ~/.ssh/id_ed25519.pub\nIn GitHub: Settings → SSH and GPG keys → New SSH key → paste the public key.\nTest the connection:\nssh -T git@github.com\n# Expect: \"Hi &lt;username&gt;! You've successfully authenticated...\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "02-git.html#github-classroom-homework-workflow",
    "href": "02-git.html#github-classroom-homework-workflow",
    "title": "2  Project Management with Git",
    "section": "2.2 GitHub Classroom: homework workflow",
    "text": "2.2 GitHub Classroom: homework workflow\nYour instructor will share a GitHub Classroom invitation link. The link automatically creates a private repository for you.\n\n2.2.1 Accept and clone the repository\n\nOpen the invitation link and accept the assignment.\n\nAfter a minute, click into your created repo (e.g., dsda1010-hw01-&lt;username&gt;).\n\nClone it once to your computer:\n\n# Using SSH (recommended)\ngit clone git@github.com:course-org/dsda1010-hw01-&lt;username&gt;.git\n\n# or using HTTPS (you will use a PAT when prompted)\n# git clone https://github.com/course-org/dsda1010-hw01-&lt;username&gt;.git\nEnter the folder, inspect starter files:\ncd dsda1010-hw01-&lt;username&gt;\nls -la\nCopy the homework template to this folder and start working on it. Make commits at appropriate stops.\n\n\n\n\n\n\nTip\n\n\n\nPro tip: If the repo includes a Quarto project, you can render it locally with quarto render before committing.\n\n\n\n\n2.2.2 Make changes, commit, and push\n# Check the current status\ngit status\n\n# Stage a specific file, or use `.` to stage all changes\ngit add README.md\n\n# Write a clear, imperative commit message\ngit commit -m \"Complete Q1 and add explanation\"\n\n# Push your work to GitHub\ngit push origin main\nRepeat the edit → add → commit → push loop as you progress. Your latest push before the deadline is your submission.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "02-git.html#core-git-operations-you-will-use-often",
    "href": "02-git.html#core-git-operations-you-will-use-often",
    "title": "2  Project Management with Git",
    "section": "2.3 Core Git operations you will use often",
    "text": "2.3 Core Git operations you will use often\n\n2.3.1 Create or initialize a repository\n# Start a new repo in an existing folder\ngit init\n\n# Connect it to a new remote repository on GitHub\ngit remote add origin git@github.com:&lt;user&gt;/&lt;repo&gt;.git\n\n# First commit and push\ngit add .\ngit commit -m \"Initial commit\"\ngit push -u origin main\n\n\n2.3.2 Inspect history and changes\ngit status\n\n# What changed since the last commit?\ngit diff\n\n# What changed in staged files?\ngit diff --staged\n\n# View history (pretty)\ngit log --oneline --graph --decorate --all\n\n\n2.3.3 Branching\n# Create and switch to a new branch\ngit switch -c feature/q2-solution\n\n# List branches\ngit branch -vv\n\n# Switch back\ngit switch main\n\n\n2.3.4 Merging and fast-forwards\n# On main, merge your feature branch\ngit switch main\ngit merge feature/q2-solution\n\n# Delete the merged branch\ngit branch -d feature/q2-solution\n\n\n2.3.5 Rebasing (optional, but good to know)\n# Rebase your work on top of updated main\ngit fetch origin\ngit rebase origin/main\n\n\n2.3.6 Fixing mistakes\n# Unstage a file you just added\ngit restore --staged path/to/file\n\n# Discard local changes in a file (careful: destructive)\ngit restore path/to/file\n\n# Amend the last commit message (if not yet pushed)\ngit commit --amend -m \"Better message\"\n\n# Revert a bad commit by creating a new inverse commit\ngit revert &lt;commit-sha&gt;\n\n\n2.3.7 Handling merge conflicts (quick recipe)\n# After a merge or rebase reports conflicts\ngit status            # see which files conflict\n\n# Open conflicted files, look for &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;\n# Edit to the desired final content, then:\ngit add path/to/conflicted-file\n\ngit commit            # completes a merge\n# or if rebasing:\ngit rebase --continue\n\n\n2.3.8 .gitignore essentials\nCreate a .gitignore in the project root:\n# Editors & OS\n.DS_Store\n.vscode/\n.Rproj.user/\n\n# Build outputs\n*_cache/\n*.html\n*.pdf\n\n# Dependencies\n.Rhistory\n.venv/\n__pycache__/\n.ipynb_checkpoints/",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "02-git.html#command-line-operations-you-should-know",
    "href": "02-git.html#command-line-operations-you-should-know",
    "title": "2  Project Management with Git",
    "section": "2.4 Command-line operations you should know",
    "text": "2.4 Command-line operations you should know\n\nUse these across macOS, Linux, and WSL. On Windows Git Bash, equivalents mostly work too.\n\n\n2.4.1 Navigation & inspection\npwd            # print working directory\nls -la         # list all files with details\ncd path/dir    # change directory\ncat file.txt   # print file contents\nhead -n 20 f   # first 20 lines\ntail -n 20 f   # last 20 lines\n\n\n2.4.2 Files & folders\nmkdir data figures scripts\nmv oldname.txt newname.txt\ncp src.txt backup/src.txt\nrm -i unwanted.tmp      # -i asks before deleting\n\n# Create a new file quickly\necho \"Title\" &gt; README.md\n\n# Edit with a CLI editor (choose one you like)\nnano README.md\n# or\nvim README.md\n\n\n2.4.3 Search & find\ngrep -n \"pattern\" file.txt        # search within a file\nrg -n \"pattern\" .                  # ripgrep (if installed) across project\nfind . -name \"*.qmd\"              # find matching files\n\n\n2.4.4 Environment & tooling\n# Check versions\npython --version\nR --version\ngit --version\nquarto --version\n\n# (Optional) create a Python virtual environment\npython -m venv .venv\nsource .venv/bin/activate     # Windows: .venv\\Scripts\\activate\npip install -r requirements.txt",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "02-git.html#end-to-end-demo-script-copypaste",
    "href": "02-git.html#end-to-end-demo-script-copypaste",
    "title": "2  Project Management with Git",
    "section": "2.5 End-to-end demo script (copy/paste)",
    "text": "2.5 End-to-end demo script (copy/paste)\n# 0) Accept GitHub Classroom invite, then clone your repo\ncd ~/courses/dsda1010\ngit clone git@github.com:course-org/dsda1010-hw01-&lt;user&gt;.git\ncd dsda1010-hw01-&lt;user&gt;\n\n# 1) Create a working branch\ngit switch -c work/q1\n\n# 2) Edit files (use your editor), then stage & commit\necho \"My answer to Q1\" &gt; answers/q1.md\ngit add answers/q1.md\ngit commit -m \"Answer Q1\"\n\n# 3) Merge into main and push\ngit switch main\ngit merge work/q1\ngit push -u origin main\n\n# 4) Pull in any upstream updates (if configured)\ngit fetch upstream\n# Merge or rebase as instructed\ngit merge upstream/main\n\n# 5) Verify on GitHub: files, commits, and CI checks (if any)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "02-git.html#troubleshooting-faq",
    "href": "02-git.html#troubleshooting-faq",
    "title": "2  Project Management with Git",
    "section": "2.6 Troubleshooting FAQ",
    "text": "2.6 Troubleshooting FAQ\nGit asks for a password on HTTPS and rejects it\nCreate a Personal Access Token on GitHub and use that instead of a password, or switch to SSH.\n“Permission denied (publickey)” when using SSH\nYour key is not added or not uploaded. Run ssh-add ~/.ssh/id_ed25519 then add the public key to GitHub Settings.\n“fatal: not a git repository”\nRun commands inside a folder that contains a .git directory, or run git init to create one.\nLine endings (Windows vs Unix)\nSet git config --global core.autocrlf input (macOS/Linux) or true (Windows) to avoid noisy diffs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "02-git.html#quick-reference-cheat-sheet",
    "href": "02-git.html#quick-reference-cheat-sheet",
    "title": "2  Project Management with Git",
    "section": "2.7 Quick reference (cheat sheet)",
    "text": "2.7 Quick reference (cheat sheet)\nstatus   → what changed\nadd      → stage changes\ncommit   → record staged snapshot\npush     → upload to remote\npull     → fetch + merge\nfetch    → download without merging\nswitch   → move between branches\nmerge    → combine histories\nrebase   → replay commits on a new base\nlog      → show history\nrestore  → discard or unstage changes\nrevert   → make an inverse commit\n\n\n\n\n\n\nImportant\n\n\n\nSubmission rule of thumb: If it is not pushed to GitHub by the deadline, it is not submitted.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "03-R.html",
    "href": "03-R.html",
    "title": "3  Jump Start with R",
    "section": "",
    "text": "3.1 Starting and Quitting R\nThis chapter gives you the minimum essentials to start using R comfortably. It assumes no prior knowledge and emphasizes good habits from the very beginning. We cover how to start and quit R, get help, understand core object types, subset objects, use basic control structures, manage your working directory, and write clean code.\nCode\n## End your R session programmatically\nq()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#starting-and-quitting-r",
    "href": "03-R.html#starting-and-quitting-r",
    "title": "3  Jump Start with R",
    "section": "",
    "text": "Start Positron, open a folder as a project, and create a new script (.R) or Quarto document (.qmd).\nRun code by highlighting lines in the editor and pressing Ctrl-Enter (Win/Linux) or Cmd-Enter (Mac). The console runs one complete line at a time.\nQuit with:\n\n\n\nWhen asked to save the workspace, choose No. Rely on scripts for reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#positron-interface",
    "href": "03-R.html#positron-interface",
    "title": "3  Jump Start with R",
    "section": "3.2 Positron Interface",
    "text": "3.2 Positron Interface\nPositron is organized into panes and a sidebar.\n\nEditor pane: main area for .R and .qmd files; supports tabs.\nConsole: interactive R prompt for quick tests.\nTerminal: a shell for system commands (e.g., git, Rscript).\nFiles: browse, create, rename, and delete items.\nEnvironment: lists objects in memory; clear with care.\nSource control: stage, commit, and view diffs in git repos.\nCommand palette: Ctrl-Shift-P or Cmd-Shift-P to search commands.\nStatus bar: shows project folder and basic status.\n\nWorking in a project\n\nOpen a folder as the project root. Use relative paths from this root.\nKeep data in data/ and scripts in R/ or src/.\n\nRunning code\n\nRun the current line or selection with Ctrl/Cmd-Enter.\nExecute a full cell in a .qmd with the Run Cell button.\n\n\n\n\n\n\n\nTip\n\n\n\nKeep the Files and Console visible. Beginners benefit from constant feedback on where they are and what ran.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#getting-help",
    "href": "03-R.html#getting-help",
    "title": "3  Jump Start with R",
    "section": "3.3 Getting Help",
    "text": "3.3 Getting Help\nR has built‑in help for every function. Every call or command you type is calling a function.\nSearch the help system on a topic:\nhelp.search(\"linear model\")\nGet the documentation of a function with known name:\n?mean\nhelp(mean)\nInspect arguments quickly for a function\n\n\nCode\nargs(mean)\n\n\nfunction (x, ...) \nNULL\n\n\nRun examples in the documentation (man page)\nexample(mean)\nPractice: find how sd() handles missing values.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#objects-in-r",
    "href": "03-R.html#objects-in-r",
    "title": "3  Jump Start with R",
    "section": "3.4 Objects in R",
    "text": "3.4 Objects in R\nEverything you store is a vector or built from vectors. Length‑one values are still vectors.\nAtomic vector types (all of fixed type):\n\n\nCode\n## Atomic vectors (length one shown; still vectors)\nnum &lt;- 3.14      ## double (numeric)\nint &lt;- 2L        ## integer\nchr &lt;- \"Ann\"     ## character\nlgc &lt;- TRUE      ## logical\n\n## A longer vector (same type throughout)\nv &lt;- c(1, 2, 3)\n\n\nHigher‑level structures built from vectors:\n\n\nCode\n## Matrix/array: same type, 2D or more\nm &lt;- matrix(1:6, nrow = 2)\n\n## List: heterogenous elements\nlst &lt;- list(name = \"Bob\", age = 25, scores = c(90, 88))\n\n## Data frame: list of equal‑length columns\n## (columns can be different atomic types)\ndf &lt;- data.frame(name = c(\"Ann\", \"Bob\"), age = c(20, 25))\n\n## Function: also an object\nsq &lt;- function(x) x^2\n\n\nInspect objects:\n\n\nCode\n## Class and structure\nclass(df)\n\n\n[1] \"data.frame\"\n\n\nCode\nstr(df)\n\n\n'data.frame':   2 obs. of  2 variables:\n $ name: chr  \"Ann\" \"Bob\"\n $ age : num  20 25\n\n\n\n\n\n\n\n\nTip\n\n\n\nPrefer str(x) for a compact view of what an object contains, its type, and its sizes.\n\n\nExercise. Create one example of each object above and check with class() and str().",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#subsetting",
    "href": "03-R.html#subsetting",
    "title": "3  Jump Start with R",
    "section": "3.5 Subsetting",
    "text": "3.5 Subsetting\nUse bracket notation consistently.\n\n\nCode\n## Vectors\nx &lt;- c(2, 4, 6, 8)\nx[2]             ## second element\n\n\n[1] 4\n\n\nCode\nx[1:3]           ## slice\n\n\n[1] 2 4 6\n\n\nCode\nx[x &gt; 5]         ## logical filter\n\n\n[1] 6 8\n\n\nCode\n## Matrices\nm &lt;- matrix(1:9, nrow = 3)\nm[2, 3]          ## row 2, col 3\n\n\n[1] 8\n\n\nCode\nm[, 1]           ## first column\n\n\n[1] 1 2 3\n\n\nCode\n## Data frames\npeople &lt;- data.frame(name = c(\"Ann\", \"Bob\"), age = c(20, 25))\npeople$age       ## column by name\n\n\n[1] 20 25\n\n\nCode\npeople[1, ]      ## first row\n\n\n\n  \n\n\n\nCode\npeople[, \"name\"] ## column by string\n\n\n[1] \"Ann\" \"Bob\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#control-structures",
    "href": "03-R.html#control-structures",
    "title": "3  Jump Start with R",
    "section": "3.6 Control Structures",
    "text": "3.6 Control Structures\n\n3.6.1 If statement (missing‑value cleaning)\n\n\nCode\n## Replace sentinel values with NA\nx &lt;- -999\nif (x == -999) {\n  x &lt;- NA\n}\nprint(x)\n\n\n[1] NA\n\n\n\n\n3.6.2 For loop (column‑wise cleaning and summary)\nUseful when applying a simple rule across columns.\n\n\nCode\n## Make a toy data frame with a sentinel value\nscores &lt;- data.frame(\n  math = c(95, -999, 88, 91),\n  eng  = c(87, 90, -999, 85),\n  sci  = c(92, 88, 94, -999)\n)\n\n## Replace -999 with NA, then compute column means\nfor (col in names(scores)) {\n  ## clean\n  bad &lt;- scores[[col]] == -999\n  scores[[col]][bad] &lt;- NA\n  ## summarize\n  m &lt;- mean(scores[[col]], na.rm = TRUE)\n  cat(col, \"mean:\", m, \"\\n\")\n}\n\n\nmath mean: 91.33333 \neng mean: 87.33333 \nsci mean: 91.33333 \n\n\n\n\n3.6.3 While loop (simulation until tolerance met)\nStop when an estimate is precise enough.\n\n\nCode\n## Estimate P(X &gt; 1.96) for N(0,1) via Monte Carlo\n## Stop when stderr &lt; 0.002\nset.seed(1)\ncount &lt;- 0\nn &lt;- 0\nse &lt;- Inf\n\nwhile (se &gt; 0.002) {\n  ## simulate in small batches for responsiveness\n  z &lt;- rnorm(1000)\n  n &lt;- n + length(z)\n  count &lt;- count + sum(z &gt; 1.96)\n  p_hat &lt;- count / n\n  se &lt;- sqrt(p_hat * (1 - p_hat) / n)\n}\n\ncat(\"p_hat:\", p_hat, \"n:\", n, \"se:\", se, \"\\n\")\n\n\np_hat: 0.0285 n: 8000 se: 0.001860368 \n\n\nExercise. Write a loop that, for each numeric column in a frame, replaces -999 with NA, then reports the fraction of missing values.\n\n\n\n\n\n\nWarning\n\n\n\nLoops are fine for clarity. Later you will see vectorized and apply‑family solutions that are faster and shorter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#workflow-basics",
    "href": "03-R.html#workflow-basics",
    "title": "3  Jump Start with R",
    "section": "3.7 Workflow Basics",
    "text": "3.7 Workflow Basics\n\n\nCode\n## Working directory\ngetwd()                  ## where am I\n\n\n[1] \"/Users/junyan/work/teaching/1010-f25/1010f25\"\n\n\nCode\n## setwd(\"path/to/folder\")   ## set if necessary\n\n\n\nIn Positron, confirm the directory in the Files pane.\nUse the console for quick tests; save work in scripts or .qmd.\nRun highlighted code with Ctrl/Cmd-Enter.\n\n\n\n\n\n\n\nTip\n\n\n\nUse project‑relative paths and file.path() to build paths. This keeps code portable across operating systems.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#importing-data",
    "href": "03-R.html#importing-data",
    "title": "3  Jump Start with R",
    "section": "3.8 Importing Data",
    "text": "3.8 Importing Data\nR can load data from text files and many other formats.\n\n3.8.1 Base R functions\n\n\nCode\n## Read a CSV file (comma-separated)\ncars &lt;- read.csv(\"data/india.csv\")\n\n## Read a general table with custom separators\nsurvey &lt;- read.table(\"data/survey.txt\", header = TRUE, sep = \" \")\n\n\nArguments to know: - header = TRUE tells R the first row has column names. - sep controls the separator (“,” for CSV, ” ” for tab‑delimited).\n\n\n\n\n\n\nTip\n\n\n\nCheck the imported object with str() or head() immediately to ensure it loaded as expected.\n\n\n\n\n3.8.2 Other formats\nThe foreign package imports legacy statistical software formats (SAS, SPSS, Stata):\n\n\nCode\nlibrary(foreign)\ndata_spss &lt;- read.spss(\"data/study.sav\", to.data.frame = TRUE)\ndata_stata &lt;- read.dta(\"data/study.dta\")\n\n\nMore modern workflows often use the haven package (part of the tidyverse) for these formats, but foreign is available in base R distributions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#good-style",
    "href": "03-R.html#good-style",
    "title": "3  Jump Start with R",
    "section": "3.9 Good Style",
    "text": "3.9 Good Style\nAdopt consistent style early. Follow the tidyverse guide: https://style.tidyverse.org/\n\nUse &lt;- for assignment.\nPlace spaces around operators and after commas.\nChoose meaningful names; avoid one‑letter names for data.\nBegin scripts with a header block.\n\n\n\nCode\n## Your Name\n## 2025-09-02\n## Purpose: demonstrate basic R style\nx &lt;- 1  # inline note uses a single \n\n\n\n\n\n\n\n\nNote\n\n\n\nComment convention. Start‑of‑line comments use at least two hashes (##). Reserve a single # for end‑of‑line notes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#tips-and-pitfalls",
    "href": "03-R.html#tips-and-pitfalls",
    "title": "3  Jump Start with R",
    "section": "3.10 Tips and Pitfalls",
    "text": "3.10 Tips and Pitfalls\n\nCase sensitivity: x and X are different.\nPaths: forward slashes / work on all platforms in R.\n\n\n\nCode\n## Portable path building\nfile.path(\"data\", \"mtcars.csv\")\n\n\n[1] \"data/mtcars.csv\"\n\n\n\nNumerical precision:\n\n\n\nCode\n## Floating‑point comparison\n0.1 == 0.3 / 3\n\n\n[1] FALSE\n\n\nCode\nall.equal(0.1, 0.3 / 3)\n\n\n[1] TRUE\n\n\nCode\n## Reveal stored value with extra digits\nprint(0.1, digits = 20)\n\n\n[1] 0.10000000000000000555\n\n\nCode\nsprintf(\"%.17f\", 0.1)\n\n\n[1] \"0.10000000000000001\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse all.equal() (or an absolute/relative tolerance) rather than == for real‑number comparisons.\n\n\n\nSave code in scripts, not the workspace.\nUse simple file names: letters, numbers, underscores.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#wrapup-checklist",
    "href": "03-R.html#wrapup-checklist",
    "title": "3  Jump Start with R",
    "section": "3.11 Wrap‑Up Checklist",
    "text": "3.11 Wrap‑Up Checklist\nYou should now be able to:\n\nStart and quit R in Positron.\nGet help with functions.\nRecognize and inspect core objects with class() and str().\nSubset vectors, matrices, and data frames.\nUse if, for, and while in useful contexts.\nManage your working directory and paths.\nWrite clean, consistent code and comments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "04-encounter.html",
    "href": "04-encounter.html",
    "title": "4  First Impression with Data",
    "section": "",
    "text": "4.1 R Packages and Data\nR is an open-source language, widely used in data science. One of its greatest strengths is the ecosystem of packages developed by the community. These packages make it easier to perform tasks such as importing data, cleaning it, and creating visualizations.\nTwo sets of packages will be central for us. Package tidyverse provides a coherent framework for data wrangling and visualization. The gapminder package offers a dataset on life expectancy, GDP per capita, and population across countries and years, which will serve as a running example in our practice.\nIf packages are not already installed, we can add them to our system with install.packages(). Installation is needed only once, but packages must be loaded every time we start a new R session.\nCode\ninstall.packages(\"tidyverse\")\ninstall.packages(\"gapminder\")\nOnce installed, packages are made available in a session by loading them with library().\nCode\nlibrary(tidyverse)\nlibrary(gapminder)\nAfter loading a dataset, it is good practice to examine its structure. Functions such as str() and summary() provide a quick overview of variable types, sample values, and ranges.\nCode\nstr(gapminder)\n\n\ntibble [1,704 × 6] (S3: tbl_df/tbl/data.frame)\n $ country  : Factor w/ 142 levels \"Afghanistan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ year     : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n $ lifeExp  : num [1:1704] 28.8 30.3 32 34 36.1 ...\n $ pop      : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...\n $ gdpPercap: num [1:1704] 779 821 853 836 740 ...\n\n\nCode\nsummary(gapminder)\n\n\n        country        continent        year         lifeExp     \n Afghanistan:  12   Africa  :624   Min.   :1952   Min.   :23.60  \n Albania    :  12   Americas:300   1st Qu.:1966   1st Qu.:48.20  \n Algeria    :  12   Asia    :396   Median :1980   Median :60.71  \n Angola     :  12   Europe  :360   Mean   :1980   Mean   :59.47  \n Argentina  :  12   Oceania : 24   3rd Qu.:1993   3rd Qu.:70.85  \n Australia  :  12                  Max.   :2007   Max.   :82.60  \n (Other)    :1632                                                \n      pop              gdpPercap       \n Min.   :6.001e+04   Min.   :   241.2  \n 1st Qu.:2.794e+06   1st Qu.:  1202.1  \n Median :7.024e+06   Median :  3531.8  \n Mean   :2.960e+07   Mean   :  7215.3  \n 3rd Qu.:1.959e+07   3rd Qu.:  9325.5  \n Max.   :1.319e+09   Max.   :113523.1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First Impression with Data</span>"
    ]
  },
  {
    "objectID": "04-encounter.html#exploring-data-frames-in-r",
    "href": "04-encounter.html#exploring-data-frames-in-r",
    "title": "4  First Impression with Data",
    "section": "4.2 Exploring Data Frames in R",
    "text": "4.2 Exploring Data Frames in R\nOnce data are loaded, the next step is to explore and understand the dataset. A data frame (or tibble, in tidyverse) is the standard format for rectangular data. R provides many built-in functions to examine, summarize, and manipulate data frames.\n\n4.2.1 Structure and dimensions\n\ndim(df) – number of rows and columns\n\nnrow(df), ncol(df) – number of rows or columns separately\n\nstr(df) – internal structure (types, first few values)\n\nglimpse(df) (from dplyr) – a cleaner version of str\n\n\n\n4.2.2 Column names and metadata\n\nnames(df) or colnames(df) – list column names\n\nrownames(df) – list row names (rarely used in tidy data)\n\n\n\n4.2.3 First and last rows\n\nhead(df) – first six rows\n\ntail(df) – last six rows\n\n\n\n4.2.4 Summaries\n\nsummary(df) – variable-by-variable summaries\n\nsapply(df, class) – variable types\n\nsapply(df, function) – apply any function to each column (e.g., mean, min, max)\n\n\n\n4.2.5 Accessing columns and rows\n\ndf$var – access a column by name\n\ndf[ , \"var\"] – same as above, but more general\n\ndf[1:5, ] – first five rows\n\ndf[ , 1:3] – first three columns\n\n\n\n4.2.6 Subsetting and filtering\n\nsubset(df, condition) – filter rows by condition\n\nWith tidyverse: filter(df, condition), select(df, cols)\n\n\n\n4.2.7 Checking contents\n\nunique(df$var) – unique values in a column\n\ntable(df$var) – frequency counts\n\nis.na(df) – identify missing values\n\nTogether, these functions give a toolkit for becoming familiar with any new dataset.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First Impression with Data</span>"
    ]
  },
  {
    "objectID": "04-encounter.html#variable-types",
    "href": "04-encounter.html#variable-types",
    "title": "4  First Impression with Data",
    "section": "4.3 Variable Types",
    "text": "4.3 Variable Types\nA crucial step in working with data is recognizing the types of variables. Variable types determine how we visualize, summarize, and analyze data.\nVariables are broadly divided into numerical and categorical. Numerical variables can be continuous, such as income or life expectancy, or discrete, such as the number of siblings or a graduation year. Categorical variables can be nominal, with no inherent order (for example, country or gender), or ordinal, with an order that matters (such as education levels or rankings). R also provides support for logical variables, representing true/false values, and date variables, with built-in functions for handling time information.\n\n\n\nVariable types\n\n\n\n\nCode\n# Example: variable types in gapminder\nglimpse(gapminder)\n\n\nRows: 1,704\nColumns: 6\n$ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n$ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n$ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n$ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…\n$ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n$ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …\n\n\nUnderstanding variable types is not just theoretical. The type guides decisions about visualization, statistical summaries, and models. For instance, the mean is meaningful for a numerical variable but not for a nominal one.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First Impression with Data</span>"
    ]
  },
  {
    "objectID": "04-encounter.html#messy-data",
    "href": "04-encounter.html#messy-data",
    "title": "4  First Impression with Data",
    "section": "4.4 Messy Data",
    "text": "4.4 Messy Data\nReal-world datasets are rarely clean. Messiness can arise from missing values, inconsistent formats, poorly named variables, or categories coded in multiple ways. Dates might appear in different styles, proper nouns might be inconsistently capitalized, and numeric values might be stored as text.\nCleaning data involves identifying and fixing these problems. R provides many tools for this work. Missing values can be detected with is.na() and handled using functions such as na.omit(). Variable names can be adjusted with rename(). The mutate() function can change types or create new variables, and joins such as left_join() allow information from multiple tables to be combined.\n\n\nCode\n# Example: identify missing values in gapminder\nsum(is.na(gapminder))\n\n\n[1] 0\n\n\nThe tidyverse philosophy emphasizes keeping data in a “tidy” format, where each variable is a column, each observation is a row, and each type of observation forms its own table. Working toward tidy data makes later analysis and visualization much easier.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First Impression with Data</span>"
    ]
  },
  {
    "objectID": "04-encounter.html#putting-it-together",
    "href": "04-encounter.html#putting-it-together",
    "title": "4  First Impression with Data",
    "section": "4.5 Putting It Together",
    "text": "4.5 Putting It Together\nTo see these ideas in practice, consider analyzing life expectancy in African countries. We might start by filtering the data to include only Africa, checking for missing values, and confirming variable types. Once the data are tidy, we can compute summaries and produce visualizations that reveal patterns over time.\n\n\nCode\nafrica &lt;- gapminder %&gt;% filter(continent == \"Africa\")\nhead(africa)\n\n\n\n  \n\n\n\nThis example illustrates the general workflow: install and load packages, import data, understand variable types, clean messy data, and prepare the dataset for analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First Impression with Data</span>"
    ]
  },
  {
    "objectID": "05-visual.html",
    "href": "05-visual.html",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.1 Base R Graphics: plot()\nData visualization is one of the most powerful tools in the data scientist’s toolbox. Visuals allow us to quickly summarize complex data, spot trends and outliers, and communicate results to both technical and non-technical audiences. A good visualization can illuminate patterns that might remain hidden in tables or numerical summaries, while a poor visualization can obscure the truth or even mislead. In this chapter, we explore both base R graphics and the ggplot2 package, emphasizing good practices and illustrating common pitfalls. We will also critique real world examples of misleading charts and learn how to improve them.\nR has a built-in graphics system that allows us to create plots quickly. The plot() function is versatile: depending on the type of data it is given, it can produce scatterplots, line plots, or even factor-based displays. This makes plot() an excellent starting point for beginners.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visual.html#base-r-graphics-plot",
    "href": "05-visual.html#base-r-graphics-plot",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.1.1 Scatterplot\nScatterplots display the relationship between two continuous variables. In the example below, we investigate how car weight relates to fuel efficiency using the built-in mtcars dataset.\n\n\nCode\n### simple scatter using built-in `mtcars`\nplot(mtcars$wt, mtcars$mpg,\n     main = \"Fuel efficiency vs. weight\",\n     xlab = \"Weight (1000 lbs)\", ylab = \"MPG\",\n     pch = 19, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 Line plot\nWhen data are ordered, such as time series or physical measurements, line plots are appropriate. The following plot shows how pressure changes with temperature.\n\n\nCode\n## line plot via type='l'\nplot(pressure$temperature, pressure$pressure,\n     type = \"l\", lwd = 2,\n     main = \"Pressure vs. Temperature\",\n     xlab = \"Temperature\", ylab = \"Pressure\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTip. Use options like pch, col, cex, and type to control appearance in base graphics. These adjustments can make exploratory plots more readable and more informative.\n\n\nBase R graphics are quick and convenient, but they can be inconsistent and limited when creating complex or publication-quality graphics. This motivates the use of a more systematic framework.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visual.html#ggplot2-basics",
    "href": "05-visual.html#ggplot2-basics",
    "title": "5  Data Visualization",
    "section": "5.2 ggplot2 Basics",
    "text": "5.2 ggplot2 Basics\nAlthough base R allows us to make plots quickly, its commands are not always consistent, and combining multiple layers can be challenging. This is where the Grammar of Graphics comes in (Wilkinson, 1999). The the strongest reason is that grammar teaches you to think systematically about how graphics are constructed, not just how to make a specific chart.\nThat means instead of thinking “I want a scatterplot” or “I want a bar chart,” you think in terms of layers of grammar:\n\nData: what dataset to use.\nAesthetics: how variables map to visual properties (x, y, color, size, etc.).\nGeoms: what geometric objects to draw (points, lines, bars).\nStats: statistical transformations (counts, smoothing, regression fits).\nScales: how data values are translated into colors, axes, and sizes.\nFacets: how to split data into subplots for comparison.\nThemes: the non-data ink (fonts, grid lines, background).\n\nThe ggplot2package (Wickham, 2016) implements this grammar, making it possible to build complex visualizations piece by piece. Because of this structure, ggplot2 is:\n\nConsistent: once you know the grammar, you can build almost any plot without learning a new function each time.\nExtensible: the same framework supports extensions (e.g., gganimate, ggrepel, patchwork).\nReproducible: code expresses the intent clearly, which is especially useful for teaching and communication.\n\nThe syntax involves calling ggplot() with a dataset and aesthetic mappings (via aes()), then adding layers such as geom_point() or geom_line() with the + operator. Additional layers for smoothing, faceting, and themes give us rich control over the appearance of plots.\nFor more syntax, see ggplot Cheatsheet\n\n5.2.1 Exploring the mpg dataset\nWe begin by loading the tidyverse, which includes ggplot2, and looking at the mpg dataset.\n\n\nCode\nlibrary(tidyverse)\n\nmpg\n\n\n\n  \n\n\n\nHere, mpg is a data frame containing information on car models, including engine displacement, highway mileage, and class.\n\n\nCode\nglimpse(mpg)\nView(mpg)\n?mpg\n\n\nThe functions glimpse() and View() allow us to quickly inspect the structure of the dataset, while ?mpg shows documentation.\n\n\n5.2.2 First ggplot calls\nBefore plotting, we might check available geoms:\n\n\nCode\n?ggplot\n?geom_point\n?geom_line\n\n\n\n\n5.2.3 Using pipes\nPipes make code easier to read by passing the result of one expression into the next. Instead of nesting functions, we can write a sequence of operations in the order we think about them. There are two main pipes in R: the base R pipe |&gt; (available since R 4.1) and the magrittr pipe %&gt;% (commonly used in the tidyverse).\nBoth pipes take the left-hand side and feed it into the first argument of the right-hand side.\n\n\nCode\n# Base R pipe\nmpg |&gt; head()\n\n\n\n  \n\n\n\nCode\n# Magrittr pipe (needs library(magrittr) or tidyverse)\nmpg %&gt;% head()\n\n\n\n  \n\n\n\nWe can now create a basic scatterplot of engine displacement vs highway mileage.\n\n\nCode\nmpg |&gt;\n  ggplot() +\n  geom_point(aes(displ, hwy))\n\n\nThis produces a scatterplot with displ on the x-axis and hwy on the y-axis. Swapping the variables simply flips the axes:\n\n\nCode\nmpg |&gt;\n  ggplot() +\n  geom_point(aes(hwy, displ))\n\n\n\n\n5.2.4 Building layers\nWe can add additional layers. For example, combining points with a line layer:\n\n\nCode\nmpg %&gt;%\n  ggplot() + \n  geom_point(aes(displ, hwy)) +\n  geom_line(aes(displ, hwy), color = \"tomato\")\n\n\n\n\n5.2.5 Adding aesthetics\nColor can highlight categories such as car class:\n\n\nCode\nmpg %&gt;%\n  ggplot() + \n  geom_point(aes(displ, hwy, color = class))\n\n\n\n\n5.2.6 Adding smoothers\nA smoothing curve helps reveal overall trends.\n\n\nCode\nmpg %&gt;%\n  ggplot() +\n  geom_point(aes(displ, hwy, color = class)) +\n  geom_smooth(aes(displ, hwy))\n\n\nThemes can alter the look of the plot:\n\n\nCode\nmpg %&gt;%\n  ggplot() +\n  geom_point(aes(displ, hwy, color = class)) +\n  geom_smooth(aes(displ, hwy)) +\n  theme_bw()\n\n\n\n\n5.2.7 Customization\nWe can set fixed aesthetics outside aes():\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(displ, hwy)) + \n  geom_point(color = \"steelblue\", size = 3)\n\n\nTransparency can improve clarity:\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(displ, hwy, color = class)) +\n  geom_point(size = 2, alpha = 0.8)\n\n\n\n\n5.2.8 Adding regression lines\nWe can fit smoothers with different methods:\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(displ, hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE)\n\n\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(displ, hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n5.2.9 Titles, labels, and themes\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(displ, hwy, color = class)) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Fuel efficiency vs. engine displacement\",\n    x = \"Engine displacement (liters)\",\n    y = \"Highway MPG\"\n  ) +\n  theme_minimal()\n\n\n\n\n5.2.10 Faceting\nWe can split data into subplots by categories.\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(displ, hwy, color = class)) +\n  geom_point(size = 2) +\n  facet_wrap(~ class)\n\n\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(displ, hwy)) +\n  geom_point() +\n  facet_grid(drv ~ cyl)\n\n\n\n\n5.2.11 Other plot types\nBar charts summarize categorical data:\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(class)) +\n  geom_bar()\n\n\nHistograms show distributions:\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(hwy)) +\n  geom_histogram(bins = 20)\n\n\nDensity plots are another way to display distributions:\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(hwy)) +\n  geom_density()\n\n\n\n\n5.2.12 Other ggplot functions\n\ncoord_flip flips the x and y axis to improve the readability of plots\nscales change the formatting of x and y axes\nplotly makes plots interactive; you can hover over points/lines for more information\nlabs allows you to add/edit a title, subtitle, a caption, and change the x and y axis labels\ngganimate allows you to animate plots into gifs",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visual.html#misleading-charts",
    "href": "05-visual.html#misleading-charts",
    "title": "5  Data Visualization",
    "section": "5.3 Misleading Charts",
    "text": "5.3 Misleading Charts\nVisualizations can be abused to mislead. It is important to learn how to critically assess charts we see in the media. The following real-world examples show common problems and better alternatives.\n\n5.3.1 Example 1: Truncated Bar Chart\n\n\nCode\napproval &lt;- data.frame(year = c(2000, 2005), percent = c(77, 65))\nbarplot(approval$percent, names.arg = approval$year,\n        ylim = c(60, 80), col = \"tomato\",\n        main = \"Approval Ratings (misleading)\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nbarplot(approval$percent, names.arg = approval$year,\n        ylim = c(0, 100), col = \"steelblue\",\n        main = \"Approval Ratings (truthful)\")\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 Example 2: Straw Poll Graphic\n\n\nCode\npoll &lt;- data.frame(candidate = c(\"A\", \"B\", \"C\", \"D\"),\n                   support = c(22, 18, 15, 10))\nbarplot(poll$support, names.arg = poll$candidate,\n        col = c(\"red\", \"blue\", \"green\", \"purple\"),\n        main = \"Poll Results (misleading)\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\npoll |&gt; ggplot(aes(x = reorder(candidate, support), y = support)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Poll Results (truthful)\",\n       x = \"Candidate\", y = \"Support (%)\")\n\n\n\n\n\n\n\n\n\n\n\n5.3.3 Example 3: Donut vs Bar Chart\n\n\nCode\nshares &lt;- data.frame(group = c(\"X\", \"Y\", \"Z\"), value = c(30, 50, 20))\nshares |&gt; ggplot(aes(x = 2, y = value, fill = group)) +\n  geom_col(width = 1, color = \"white\") +\n  coord_polar(theta = \"y\") +\n  xlim(0.5, 2.5) +\n  theme_void() +\n  labs(title = \"Shares (donut, misleading)\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nshares |&gt; ggplot(aes(x = group, y = value, fill = group)) +\n  geom_col() +\n  labs(title = \"Shares (bar chart)\",\n       x = NULL, y = \"Value\")\n\n\n\n\n\n\n\n\n\nPie charts (and donut charts, which are essentially pies with a hole in the middle) are widely criticized because humans are not good at accurately comparing angles or areas. Judgments based on angles are much less precise than those based on position or length. This makes pie charts poor at conveying quantitative comparisons, especially when slices are similar in size. Donut charts exacerbate the problem by removing the center, which eliminates a natural visual baseline (the full radius), making angle judgments even harder. For these reasons, most visualization experts recommend bar charts instead, where lengths aligned to a common baseline support more accurate comparisons.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visual.html#good-practice",
    "href": "05-visual.html#good-practice",
    "title": "5  Data Visualization",
    "section": "5.4 Good Practice",
    "text": "5.4 Good Practice\n\n5.4.1 Bad Plots\n\nTruncated axes exaggerate differences.\n\n3D effects distort perception.\n\nPie/donut charts hinder comparisons.\n\nOverplotting or excessive colors obscure patterns.\n\nInconsistent scales or ordering confuse the audience.\n\n\n\n5.4.2 Good Plots\n\nStart bar charts at zero to preserve proportion.\n\nUse simple, clear chart types.\n\nProvide informative labels and titles.\n\nKeep scales and colors consistent.\n\nAvoid unnecessary clutter.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visual.html#in-class-example",
    "href": "05-visual.html#in-class-example",
    "title": "5  Data Visualization",
    "section": "5.5 In-Class Example",
    "text": "5.5 In-Class Example\nConsider the data of Chetty et al. (2014).\n\nVisualize the relationship between social capital and absolute mobility. Do you see a correlation? Is it what you expected from the Chetty et al. (2014) study executive summary?\nAdd an aesthetic to your graph to represent whether the CZ is urban or not.\nSeparate urban and non-urban CZ’s into two separate plots.\nAdd a smooth fit to each of your plots above. Experiment with adding the option method=\"lm\" in your geom_smooth. What does this option do?\nWhich variables in the chetty data frame are appropriate x variables for a bar graph?\nMake two separate bar graphs for two different x variables.\nMake two more bar graphs that display proportions rather than counts of your selected variables.\nMake a bar graph that lets you compare the number of urban and rural CZ’s in each of the four regions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visual.html#summary",
    "href": "05-visual.html#summary",
    "title": "5  Data Visualization",
    "section": "5.6 Summary",
    "text": "5.6 Summary\nWe have explored both base R plotting and the ggplot2 grammar of graphics. Base R offers quick and simple plotting functions, but lacks consistency for more advanced tasks. ggplot2 provides a flexible and layered system, allowing us to build complex visualizations step by step. By studying both good and bad visualizations, we learn not only how to make effective charts but also how to critically evaluate visuals we encounter in practice.\n\n\n\n\nChetty, R., Hendren, N., Kline, P., & Saez, E. (2014). Where is the land of opportunity? The geography of intergenerational mobility in the United States. Quarterly Journal of Economics, 129(4), 1553–1623. https://doi.org/10.1093/qje/qju022\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWilkinson, L. (1999). The grammar of graphics. Springer-Verlag New York, Inc.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html",
    "href": "06-manipulation.html",
    "title": "6  Data Manipulation",
    "section": "",
    "text": "6.1 Introduction\nRaw data are rarely ready for direct analysis. We often need to reshape, filter, or summarize before we can create meaningful plots or fit statistical models. The tidyverse provides a consistent grammar for these operations, with dplyr as its central package.\nIn this chapter, we will learn the most important data manipulation verbs. Each verb is a function that takes a data frame (or tibble) as the first argument, applies some manipulation, and returns a new data frame.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#introduction",
    "href": "06-manipulation.html#introduction",
    "title": "6  Data Manipulation",
    "section": "",
    "text": "Backward Compatibility in the Tidyverse\n\n\n\nThe tidyverse strives to minimize disruption, but backward compatibility is not guaranteed. Breaking changes sometimes occur—especially in major releases—to improve consistency or fix design issues. Functions are usually deprecated with warnings before removal, giving time to update code. For long-term stability, pin package versions with tools like renv and always review release notes when upgrading.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#core-dplyr-verbs",
    "href": "06-manipulation.html#core-dplyr-verbs",
    "title": "6  Data Manipulation",
    "section": "6.2 Core dplyr Verbs",
    "text": "6.2 Core dplyr Verbs\nThe six most commonly used verbs are:\n\nfilter() — select rows based on conditions\n\narrange() — reorder rows\n\nselect() — choose columns\n\nmutate() — add or modify columns\n\ngroup_by() — define groups for analysis\n\nsummarise() — collapse groups into summaries\n\nAll verbs follow the same pattern: the first argument is a data frame, and subsequent arguments describe manipulations using column names.\nWe will illustrate these verbs using the nycflights13::flights dataset.\n\n\nCode\nlibrary(nycflights13)\nflights &lt;- nycflights13::flights\nstr(flights)\n\n\ntibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n\n\n\n6.2.1 Filtering Rows with filter()\nThe filter() function selects rows that satisfy logical conditions. Logical operators like & (and), | (or), and ! (not) are often used, along with comparisons such as ==, !=, &lt;, and &gt;=.\n\n== equals\n!= not equal\n&lt; less than, &lt;= less than or equal\n&gt; greater than, &gt;= greater than or equal\n& logical AND (both conditions must be true)\n| logical OR (at least one condition must be true)\n! logical NOT (negates a condition)\n\nExamples with the flights data:\n\n\nCode\n# Flights on January 1\nflights |&gt;\nfilter(month == 1 & day == 1)\n\n\n\n  \n\n\n\nCode\n# Flights in January or February\nflights |&gt;\nfilter(month == 1 | month == 2)\n\n\n\n  \n\n\n\nCode\n# Flights not in January\nflights |&gt;\nfilter(!(month == 1))\n\n\n\n  \n\n\n\nCode\n# Flights with arrival delay over 2 hours and from JFK\nflights |&gt;\nfilter(arr_delay &gt; 120 & origin == \"JFK\")\n\n\n\n  \n\n\n\nCode\n# Flights that were either very early (dep_delay &lt; -15) or very late (dep_delay &gt; 120)\nflights |&gt;\nfilter(dep_delay &lt; -15 | dep_delay &gt; 120)\n\n\n\n  \n\n\n\nCode\n# Flights in summer months AND either from JFK or LGA\nflights |&gt;\nfilter((month %in% c(6,7,8)) & (origin == \"JFK\" | origin == \"LGA\"))\n\n\n\n  \n\n\n\nCode\n# Flights in December with departure delay over 2 hours OR (in January with arrival delay over 2 hours)\nflights |&gt;\nfilter((month == 12 & dep_delay &gt; 120) | (month == 1 & arr_delay &gt; 120))\n\n\n\n  \n\n\n\nCode\n# Flights from EWR where either (dep_delay &gt; 60 AND arr_delay &gt; 60) OR (dep_delay &lt; -30 AND arr_delay &lt; -30)\nflights |&gt;\nfilter(origin == \"EWR\" & ((dep_delay &gt; 60 & arr_delay &gt; 60) |\n(dep_delay &lt; -30 & arr_delay &lt; -30)))\n\n\n\n  \n\n\n\n\n6.2.1.1 Handling Missing Values\nMissing values (NA) require care. For example:\n\n\nCode\nstocks &lt;- data.frame(\n  year   = c(2015, 2015, 2016, 2016),\n  qtr    = c(1, 2, 2, NA),\n  return = c(1.1, NA, 0.9, 2.0)\n)\n\nstocks |&gt; filter(is.na(qtr) | is.na(return))\n\n\n\n  \n\n\n\nCode\nstocks |&gt; filter(!is.na(qtr) & !is.na(return))\n\n\n\n  \n\n\n\n\n\n\n6.2.2 Reordering Rows with arrange()\narrange() sorts rows by column values. Missing values are sorted last.\n\n\nCode\nflights |&gt; arrange(year, month, day) |&gt; str()\n\n\ntibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n\n\nDescending order is done with desc():\n\n\nCode\nflights |&gt; arrange(desc(dep_delay)) |&gt; str()\n\n\ntibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 6 1 9 7 4 3 6 7 12 ...\n $ day           : int [1:336776] 9 15 10 20 22 10 17 27 22 5 ...\n $ dep_time      : int [1:336776] 641 1432 1121 1139 845 1100 2321 959 2257 756 ...\n $ sched_dep_time: int [1:336776] 900 1935 1635 1845 1600 1900 810 1900 759 1700 ...\n $ dep_delay     : num [1:336776] 1301 1137 1126 1014 1005 ...\n $ arr_time      : int [1:336776] 1242 1607 1239 1457 1044 1342 135 1236 121 1058 ...\n $ sched_arr_time: int [1:336776] 1530 2120 1810 2210 1815 2211 1020 2226 1026 2020 ...\n $ arr_delay     : num [1:336776] 1272 1127 1109 1007 989 ...\n $ carrier       : chr [1:336776] \"HA\" \"MQ\" \"MQ\" \"AA\" ...\n $ flight        : int [1:336776] 51 3535 3695 177 3075 2391 2119 2007 2047 172 ...\n $ tailnum       : chr [1:336776] \"N384HA\" \"N504MQ\" \"N517MQ\" \"N338AA\" ...\n $ origin        : chr [1:336776] \"JFK\" \"JFK\" \"EWR\" \"JFK\" ...\n $ dest          : chr [1:336776] \"HNL\" \"CMH\" \"ORD\" \"SFO\" ...\n $ air_time      : num [1:336776] 640 74 111 354 96 139 167 313 109 149 ...\n $ distance      : num [1:336776] 4983 483 719 2586 589 ...\n $ hour          : num [1:336776] 9 19 16 18 16 19 8 19 7 17 ...\n $ minute        : num [1:336776] 0 35 35 45 0 0 10 0 59 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-09 09:00:00\" \"2013-06-15 19:00:00\" ...\n\n\n\n\n6.2.3 Selecting Columns with select()\nselect() picks out specific columns. Useful helpers include starts_with() and ends_with().\n\n\nCode\nflights |&gt; select(year, month, day) |&gt; str()\n\n\ntibble [336,776 × 3] (S3: tbl_df/tbl/data.frame)\n $ year : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month: int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day  : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n\n\nCode\nflights |&gt; select(starts_with(\"dep\")) |&gt; str()\n\n\ntibble [336,776 × 2] (S3: tbl_df/tbl/data.frame)\n $ dep_time : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ dep_delay: num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n\n\nColumns can also be renamed inline:\n\n\nCode\nflights |&gt; select(tail = tailnum, everything()) |&gt; str()\n\n\ntibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n $ tail          : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n\n\n\n\n6.2.4 Creating New Columns with mutate()\nmutate() adds new variables or modifies existing ones.\n\n\nCode\nflights |&gt; mutate(speed = distance / (air_time/60)) |&gt; str()\n\n\ntibble [336,776 × 20] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n $ speed         : num [1:336776] 370 374 408 517 394 ...\n\n\nIt is common to create multiple new variables at once:\n\n\nCode\nflights |&gt; mutate(\n  gain = dep_delay - arr_delay,\n  hours = air_time / 60\n) |&gt; str()\n\n\ntibble [336,776 × 21] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n $ gain          : num [1:336776] -9 -16 -31 17 19 -16 -24 11 5 -10 ...\n $ hours         : num [1:336776] 3.78 3.78 2.67 3.05 1.93 ...\n\n\n\n\n6.2.5 Grouping and Summarizing\nThe group_by() function defines groups of rows, and summarise() reduces each group to summary statistics.\n\n\nCode\nflights |&gt;\n  group_by(month) |&gt;\n  summarise(delay = mean(dep_delay, na.rm = TRUE))\n\n\n\n  \n\n\n\nMultiple grouping variables are possible:\n\n\nCode\nflights |&gt;\n  group_by(carrier, month) |&gt;\n  summarise(\n    delay = mean(dep_delay, na.rm = TRUE),\n    n = n()\n  ) |&gt; str()\n\n\ngropd_df [185 × 4] (S3: grouped_df/tbl_df/tbl/data.frame)\n $ carrier: chr [1:185] \"9E\" \"9E\" \"9E\" \"9E\" ...\n $ month  : int [1:185] 1 2 3 4 5 6 7 8 9 10 ...\n $ delay  : num [1:185] 16.9 16.5 13.4 13.6 22.7 ...\n $ n      : int [1:185] 1573 1459 1627 1511 1462 1437 1494 1456 1540 1673 ...\n - attr(*, \"groups\")= tibble [16 × 2] (S3: tbl_df/tbl/data.frame)\n  ..$ carrier: chr [1:16] \"9E\" \"AA\" \"AS\" \"B6\" ...\n  ..$ .rows  : list&lt;int&gt; [1:16] \n  .. ..$ : int [1:12] 1 2 3 4 5 6 7 8 9 10 ...\n  .. ..$ : int [1:12] 13 14 15 16 17 18 19 20 21 22 ...\n  .. ..$ : int [1:12] 25 26 27 28 29 30 31 32 33 34 ...\n  .. ..$ : int [1:12] 37 38 39 40 41 42 43 44 45 46 ...\n  .. ..$ : int [1:12] 49 50 51 52 53 54 55 56 57 58 ...\n  .. ..$ : int [1:12] 61 62 63 64 65 66 67 68 69 70 ...\n  .. ..$ : int [1:12] 73 74 75 76 77 78 79 80 81 82 ...\n  .. ..$ : int [1:12] 85 86 87 88 89 90 91 92 93 94 ...\n  .. ..$ : int [1:12] 97 98 99 100 101 102 103 104 105 106 ...\n  .. ..$ : int [1:12] 109 110 111 112 113 114 115 116 117 118 ...\n  .. ..$ : int [1:5] 121 122 123 124 125\n  .. ..$ : int [1:12] 126 127 128 129 130 131 132 133 134 135 ...\n  .. ..$ : int [1:12] 138 139 140 141 142 143 144 145 146 147 ...\n  .. ..$ : int [1:12] 150 151 152 153 154 155 156 157 158 159 ...\n  .. ..$ : int [1:12] 162 163 164 165 166 167 168 169 170 171 ...\n  .. ..$ : int [1:12] 174 175 176 177 178 179 180 181 182 183 ...\n  .. ..@ ptype: int(0) \n  ..- attr(*, \".drop\")= logi TRUE\n\n\nSometimes, the keeping the group structured after summarising can be handy, which can be achieved with .groups = \"keep\". In this case, groups persist unless they are explicitly removed with ungroup(). With the default summarise(), a single grouping variable is dropped, producing an ungrouped result; so ungroup() after that is redundant.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#reshaping-data",
    "href": "06-manipulation.html#reshaping-data",
    "title": "6  Data Manipulation",
    "section": "6.3 Reshaping Data",
    "text": "6.3 Reshaping Data\nReal-world data often need reshaping. Tidy data prefers one observation per row, one variable per column.\n\n6.3.1 Wide to Long: pivot_longer()\n\n\nCode\ntable4a &lt;- tibble(\n  country = c(\"A\", \"B\"),\n  `1999` = c(745, 377),\n  `2000` = c(377, 345)\n)\n\ntable4a |&gt; pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n\n\n  \n\n\n\n\n\n6.3.2 Long to Wide: pivot_wider()\n\n\nCode\ntable2 &lt;- tibble(\n  country = c(\"A\", \"A\", \"B\", \"B\"),\n  year = c(1999, 2000, 1999, 2000),\n  type = c(\"cases\", \"cases\", \"cases\", \"cases\"),\n  count = c(745, 377, 377, 345)\n)\n\ntable2 |&gt; pivot_wider(names_from = type, values_from = count)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#combining-data-from-multiple-tables",
    "href": "06-manipulation.html#combining-data-from-multiple-tables",
    "title": "6  Data Manipulation",
    "section": "6.4 Combining Data from Multiple Tables",
    "text": "6.4 Combining Data from Multiple Tables\nJoins combine data from two tables based on a common key column. The type of join determines which rows are kept:\n\nInner Join: Keeps only rows with matching keys in both tables.\nLeft Join: Keeps all rows from the left table, adding matches from the right table where available (missing values filled with NA).\nRight Join: Keeps all rows from the right table, adding matches from the left table where available.\nFull Join: Keeps all rows from both tables, filling unmatched values with NA.\n\nConsider the planes data.\n\n\nCode\nplanes &lt;- nycflights13::planes\nstr(planes)\n\n\ntibble [3,322 × 9] (S3: tbl_df/tbl/data.frame)\n $ tailnum     : chr [1:3322] \"N10156\" \"N102UW\" \"N103US\" \"N104UW\" ...\n $ year        : int [1:3322] 2004 1998 1999 1999 2002 1999 1999 1999 1999 1999 ...\n $ type        : chr [1:3322] \"Fixed wing multi engine\" \"Fixed wing multi engine\" \"Fixed wing multi engine\" \"Fixed wing multi engine\" ...\n $ manufacturer: chr [1:3322] \"EMBRAER\" \"AIRBUS INDUSTRIE\" \"AIRBUS INDUSTRIE\" \"AIRBUS INDUSTRIE\" ...\n $ model       : chr [1:3322] \"EMB-145XR\" \"A320-214\" \"A320-214\" \"A320-214\" ...\n $ engines     : int [1:3322] 2 2 2 2 2 2 2 2 2 2 ...\n $ seats       : int [1:3322] 55 182 182 182 55 182 182 182 182 182 ...\n $ speed       : int [1:3322] NA NA NA NA NA NA NA NA NA NA ...\n $ engine      : chr [1:3322] \"Turbo-fan\" \"Turbo-fan\" \"Turbo-fan\" \"Turbo-fan\" ...\n\n\nJoin the flights data and the planes data:\n\n\nCode\nflights2 &lt;- flights |&gt; select(year, month, day, carrier, tailnum)\nplanes2 &lt;- planes |&gt; select(tailnum, type, manufacturer)\n\nflights2 |&gt; left_join(planes2, by = \"tailnum\") |&gt; str()\n\n\ntibble [336,776 × 7] (S3: tbl_df/tbl/data.frame)\n $ year        : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month       : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ carrier     : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ tailnum     : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ type        : chr [1:336776] \"Fixed wing multi engine\" \"Fixed wing multi engine\" \"Fixed wing multi engine\" \"Fixed wing multi engine\" ...\n $ manufacturer: chr [1:336776] \"BOEING\" \"BOEING\" \"BOEING\" \"AIRBUS\" ...\n\n\nCode\n# Try inner, right, and full join too\n\n\nOther join types include inner_join(), right_join(), and full_join().",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#assigning-results-of-manipulations",
    "href": "06-manipulation.html#assigning-results-of-manipulations",
    "title": "6  Data Manipulation",
    "section": "6.5 Assigning Results of Manipulations",
    "text": "6.5 Assigning Results of Manipulations\nWhen working with dplyr, it is often useful to save the results of a manipulation into a new object. This allows you to reuse the processed data without repeating all of the steps.\n\n\nCode\n# Filter flights on January 1st and arrange by departure delay\nflights_jan1 &lt;- flights |&gt;\nfilter(month == 1, day == 1) |&gt;\narrange(dep_delay)\n\n# Print the first few rows\nhead(flights_jan1)\n\n\n\n  \n\n\n\nYou can now work with flights_jan1 in later code chunks without re-running the entire manipulation. This practice is especially helpful for long workflows where the same processed data will be used multiple times.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#in-class-example",
    "href": "06-manipulation.html#in-class-example",
    "title": "6  Data Manipulation",
    "section": "6.6 In-Class Example",
    "text": "6.6 In-Class Example\nConsider the data of Chetty et al. (2014).\n\nCreate a data frame with CZs in CT, MA, and NY.\nCreate a data frame with CZ’s with absolute mobility of at least 40\nCreate a data frame with CZ’s in any state other than CT, MA, or NY, with absolute mobility at least 40\nCreate a data frame with CZ’s that are in CT, MA, NY and have absolute mobility less than 40.\nCreate a data frame with CZ’s that are in CT, MA, NY, sorting the CZ’s in decreasing order of absolute mobility, and keeping just the CZ name, state, and absolute mobility variables in the resulting data frames\nCreate a new data set with only the following variables: cz_name, state, pop_2000, abs_mobility, hhi_percap, and any variable that starts with frac.\nMake new variables for each of these quantities:\n\nThe number of people in each CZ who consider themselves to be religious.\nThe log base 2 of the per capita household income (hint: log2() ).\nThe proportion of people who are not married.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#summary-and-best-practices",
    "href": "06-manipulation.html#summary-and-best-practices",
    "title": "6  Data Manipulation",
    "section": "6.7 Summary and Best Practices",
    "text": "6.7 Summary and Best Practices\n\nBegin with a clear idea of what manipulation you need.\n\nChain verbs together with the pipe operator for readability.\n\nUse group_by() and summarise() to move from raw detail to aggregated insights.\n\nReshape and join data as needed to bring it into tidy form.\n\nThese manipulations prepare your data for visualization and modeling, ensuring clarity and reproducibility in analysis.\n\n\n\n\nChetty, R., Hendren, N., Kline, P., & Saez, E. (2014). Where is the land of opportunity? The geography of intergenerational mobility in the United States. Quarterly Journal of Economics, 129(4), 1553–1623. https://doi.org/10.1093/qje/qju022",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "07-eda.html",
    "href": "07-eda.html",
    "title": "7  Exploratory Data Analysis",
    "section": "",
    "text": "7.1 Questions to consider\nExploratory Data Analysis (EDA) is the practice of using graphs and numerical summaries to become familiar with a dataset before formal modeling. The term was popularized by John Tukey in his influential book Exploratory Data Analysis (Tukey, 1977), and it remains a cornerstone of modern data science. The purpose of EDA is to reveal the main characteristics of the data, detect unusual cases, check the quality of measurements, and generate hypotheses that can guide later modeling. Unlike confirmatory analysis, which tests specific hypotheses with formal statistical procedures, EDA is open ended and iterative. Analysts move back and forth between plotting, summarizing, and transforming data, refining their understanding at each step.\nFigure 7.1 shows the data science workflow in a compact layout, generated directly in R. Compared with embedding static images, this approach keeps your notes reproducible and editable. The flowchart emphasizes that exploratory data analysis sits between data transformation and modeling. It also highlights feedback loops: visualization can suggest new transformations, and communication can lead back to data collection.\nWe will use ggplot2::diamonds as a running example. It contains about fifty four thousand round‑cut diamonds with carat, cut, color, clarity, depth, table, price, and x/y/z dimensions. The dataset is big enough to be realistic while still rendering quickly in class.\nWhen beginning an EDA it is useful to ask two broad questions that frame all of the work. These questions remind us that analysis is about patterns, not isolated numbers.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-eda.html#questions-to-consider",
    "href": "07-eda.html#questions-to-consider",
    "title": "7  Exploratory Data Analysis",
    "section": "",
    "text": "What type of variation occurs within variables?\nVariation refers to the tendency of a variable to take different values from one observation to another. Even for simple variables like eye color or carat weight, no two individuals are exactly the same. Understanding the spread and shape of this variation is the first task of EDA.\nWhat type of covariation occurs between variables?\nCovariation describes the way two or more variables move together in a related fashion. For instance, diamond carat and price tend to increase together. Examining such relationships gives insight into possible explanations and later modeling choices.\n\n\n7.1.1 Useful terms\nClear vocabulary helps organize thinking during EDA.\n\nVariable. A measurable characteristic such as carat, cut, or price. Variables may be quantitative or qualitative.\nValue. The state of a variable when it is measured. Values can differ across observations; for example, two diamonds can have different carat weights.\nObservation. A set of measurements recorded on the same unit at the same time. In diamonds, each row is one diamond with values for all variables.\nTabular data. A collection of values arranged with variables in columns and observations in rows. Data is called tidy when each value occupies its own cell, making manipulation and visualization straightforward.\n\n\n\nCode\nlibrary(tidyverse)\nglimpse(diamonds)\n\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\n\n\n7.1.2 Variation\nEvery variable has its own pattern of variation, and these patterns often reveal important information. The best way to learn about the variation of a variable is to visualize its distribution.\n\nFor categorical variables, variation shows up in the relative frequencies of categories.\nFor quantitative variables, variation appears in the spread and shape of the distribution.\n\n\n\nCode\n# variation in cut (categorical)\ndiamonds |&gt;\n  count(cut) |&gt;\n  mutate(prop = n / sum(n))\n\n\n\n  \n\n\n\n\n\nCode\n# variation in carat (numeric)\nggplot(diamonds, aes(carat)) +\n  geom_histogram(binwidth = 0.1, boundary = 0, closed = \"left\") +\n  labs(title = \"Variation in carat\", x = \"Carat\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\n7.1.3 Visualizing distributions\nHow to visualize a distribution depends on the type of variable.\nCategorical variables take one of a small set of levels and are usually stored as factors or character vectors in R. Bar charts are a natural choice for displaying their distributions.\n\n\nCode\n    ggplot(diamonds, aes(cut)) +\n      geom_bar() +\n      labs(title = \"Distribution of cut\", x = NULL, y = \"Count\")\n\n\n\n\n\n\n\n\n\nQuantitative variables take on a wide range of values. Their distributions can be displayed in several ways:\n\nHistograms divide the range of the variable into bins and count how many observations fall into each bin. They are useful for seeing the overall shape of a distribution.\n\n\n\nCode\n    ggplot(diamonds, aes(price)) +\n      geom_histogram(binwidth = 500, boundary = 0, closed = \"left\") +\n      labs(title = \"Distribution of price (histogram)\",\n           x = \"Price (USD)\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\nBoxplots summarize the distribution using the median and quartiles.\n\nThe box spans the interquartile range (IQR), from the first quartile (Q1, 25th percentile) to the third quartile (Q3, 75th percentile).\nThe whiskers usually extend from the box out to the most extreme data points that are not considered outliers. A common rule is that the whiskers reach to the smallest and largest data points within 1.5 × IQR of Q1 and Q3.\nThe dots are outliers, data points beyond the whiskers (i.e., more than 1.5 × IQR away from Q1 or Q3).\n\n\n\n\nCode\n    ggplot(diamonds, aes(x = cut, y = price)) +\n      geom_boxplot() +\n      labs(title = \"Price by cut (boxplot)\", x = \"Cut\", y = \"Price (USD)\")\n\n\n\n\n\n\n\n\n\n\nViolin plots combine a boxplot with a rotated density curve, showing both summary statistics and the shape of the distribution.\n\n\n\nCode\n    ggplot(diamonds, aes(x = cut, y = price)) +\n      geom_violin(trim = FALSE) +\n      labs(title = \"Price by cut (violin plot)\", x = \"Cut\", y = \"Price (USD)\")\n\n\n\n\n\n\n\n\n\nThese three methods are closely related. A histogram provides a binned view of the data, while a density curve offers a smoothed version of the histogram. A violin plot takes that density curve, mirrors it to make the violin shape, and overlays summary statistics similar to a boxplot. Together they form a family of tools that balance detail, smoothing, and summary depending on the purpose of the visualization.\n\n\n7.1.4 Outliers\nOutliers are observations that fall far outside the general pattern of a distribution. They may represent data entry errors, unusual cases, or rare but valid observations. Outliers are important to detect because they can strongly influence summary statistics and models.\nThe diamonds dataset has some striking examples.\n\nHistogram of y (length)\n\n\n\nCode\nggplot(diamonds) +\ngeom_histogram(mapping = aes(x = y), binwidth = 0.5) +\nlabs(title = \"Distribution of diamond length (y)\",\nx = \"Length (mm)\", y = \"Count\")\n\n\n\n\n\n\n\n\n\nMost diamonds have lengths between 3 and 10 mm. However, there are odd spikes at 0, 30, 50, and 60 mm, which are not physically plausible. These are likely data recording errors.\n\n\nCode\nunusual &lt;- diamonds %&gt;% \n  filter(y &lt; 3 | y &gt; 20) %&gt;% \n  select(price, x, y, z) %&gt;%\n  arrange(y)\n\nunusual\n\n\n\n  \n\n\n\n\nThe y variable measures one of the three dimensions of these diamonds in mm. Diamonds can’t have a width of 0mm.\nMeasurements of 32mm and 59mm are implausible. Those diamonds are over an inch long, but don’t cost hundreds of thousands of dollars\n\nConfirmed invalid values could be replaced with NA, using the ifelse() function.\n\n\nCode\ndiamonds2 &lt;- diamonds %&gt;% \n  mutate(y = ifelse(y &lt; 3 | y &gt; 20, NA, y))\n\ndiamonds2 |&gt; filter(is.na(y))\n\n\n\n  \n\n\n\n\nHistogram of x (width)\n\n\n\nCode\nggplot(diamonds) +\ngeom_histogram(mapping = aes(x = x), binwidth = 0.5) +\nlabs(title = \"Distribution of diamond width (x)\",\nx = \"Width (mm)\", y = \"Count\")\n\n\n\n\n\n\n\n\n\nAgain, most values are in a reasonable range, but there are suspicious zeros and unusually large values.\n\nBoxplot of price by cut\n\n\n\nCode\nggplot(diamonds, aes(x = cut, y = price)) +\ngeom_boxplot() +\nlabs(title = \"Price by cut (boxplot)\", x = \"Cut\", y = \"Price (USD)\")\n\n\n\n\n\n\n\n\n\nThe boxplot reveals outliers as individual points beyond the whiskers. These highlight diamonds with unusually high or low prices relative to others of the same cut.\n\nScatterplot of carat vs. price\n\n\n\nCode\nggplot(diamonds, aes(x = carat, y = price)) +\ngeom_point(alpha = 0.3) +\nlabs(title = \"Price vs. Carat\", x = \"Carat\", y = \"Price (USD)\")\n\n\n\n\n\n\n\n\n\nMost diamonds follow a clear increasing trend. Outliers appear as points far above or below the main cloud — either unusually expensive small diamonds or unusually cheap large ones.\n\nBoxplot of z (depth) by cut\n\n\n\nCode\nggplot(diamonds, aes(x = cut, y = z)) +\ngeom_boxplot() +\nlabs(title = \"Depth (z) by cut\", x = \"Cut\", y = \"Depth (mm)\")\n\n\n\n\n\n\n\n\n\nHere too, implausible depths like 0 or 30+ mm appear as obvious outliers.\nThese examples illustrate different ways to detect outliers: - Histograms reveal spikes or extreme bars. - Boxplots show individual points beyond whiskers. - Scatterplots highlight deviations from a trend.\nTogether, they demonstrate that careful visualization is essential for spotting unusual data values before moving on to modeling.\n\n\n\n\n\n\nNote\n\n\n\nNote. Not every outlier should be removed. Some may hold the key to a new insight, while others may be errors worth correcting. Document any decisions you make.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-eda.html#conditional-statements",
    "href": "07-eda.html#conditional-statements",
    "title": "7  Exploratory Data Analysis",
    "section": "7.2 Conditional statements",
    "text": "7.2 Conditional statements\nSometimes, creating new variable relies on a complex combination of existing variables. Utilize the ifelse() or case_when() command.\n\n\nCode\ndiamonds %&gt;%\n  mutate(size_category = case_when(\n    carat &lt; 0.25 ~ \"tiny\",\n    carat &lt; 0.5  ~ \"small\",\n    carat &lt; 1    ~ \"medium\",\n    carat &lt; 1.5  ~ \"large\",\n    TRUE         ~ \"huge\"\n  )) %&gt;%\n  select(size_category, carat) %&gt;%\n  head()\n\n\n\n  \n\n\n\nMany insights arise from studying how a variable behaves given the level of another variable. These conditional relationships are the basis for understanding covariation.\n\nNumeric given categorical. Compare distributions of a numeric variable across categories.\nNumeric given numeric. Study scatterplots, possibly with facets, to see how the relationship changes across groups.\n\n\n\nCode\n# price conditional on cut\nggplot(diamonds, aes(cut, price)) +\n  geom_boxplot(outlier.alpha = 0.2) +\n  labs(title = \"Price distribution by cut\", x = NULL, y = \"Price (USD)\")\n\n\n\n\n\n\n\n\n\nCode\n# carat vs. price conditional on clarity\nset.seed(1)\nd_small &lt;- diamonds[sample(nrow(diamonds), 8000), ]\nggplot(d_small, aes(carat, price)) +\n  geom_point(alpha = 0.35) +\n  facet_wrap(~ clarity) +\n  labs(title = \"Price vs. carat by clarity\",\n       x = \"Carat\", y = \"Price (USD)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTip. Faceting is usually clearer than mapping too many aesthetics at once. Use small multiples to keep comparisons interpretable.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-eda.html#in-class-exercises",
    "href": "07-eda.html#in-class-exercises",
    "title": "7  Exploratory Data Analysis",
    "section": "7.3 In-Class Exercises",
    "text": "7.3 In-Class Exercises\nThe data come from the NYC Open Data portal: Motor Vehicle Collisions–Crashes. This dataset contains information reported to the NYPD for motor vehicle crashes, available as data/nyc_crashes_lbdwk_2025.csv. For this exercise, a subset covering Labor Day week 2025 (from 11:59:59 PM August 30 to 12:00:00 AM September 7) was downloaded. It includes 1,487 crash records with 29 variables.\nKey variables include:\n\nCRASH DATE, CRASH TIME: when the crash occurred.\nBOROUGH, ZIP CODE, LATITUDE, LONGITUDE, LOCATION: where the crash occurred.\nON STREET NAME, CROSS STREET NAME, OFF STREET NAME: streets involved.\nNUMBER OF PERSONS INJURED/KILLED and subcategories for pedestrians, cyclists, and motorists.\nCONTRIBUTING FACTOR VEHICLE 1–5: recorded causes of crashes.\nVEHICLE TYPE CODE 1–5: vehicle categories involved.\n\nThe raw data contain missing values in several fields (e.g., borough, ZIP code, coordinates, street names, and contributing factors). Detecting and interpreting missingness is part of the data exploration process.\n\nRead the data into R.\nPlot the number of crashes by day and by hour. Do weekends/holidays or evening show distinct patterns compared to weekdays?\nWhich borough recorded the most crashes? Are some boroughs missing altogether?\nHow many records have missing ZIP codes? Are the missing ones concentrated in certain boroughs?\nHow many crashes are missing latitude/longitude? What fraction of crashes with missing borough also lack coordinates?\nCompare counts of missing values in ON STREET NAME, CROSS STREET    NAME, and OFF STREET NAME. What does this suggest about where data collection is weaker?\nHow many crashes involved at least one person injured? At least one fatality? Show the distribution of injury counts.\nWhat are the most common contributing factors for VEHICLE 1? Are there many “Unspecified” entries?\nWhat are the most frequent vehicle categories in VEHICLE TYPE CODE 1? Compare them with secondary vehicles (VEHICLE TYPE CODE 2).\nAre there crashes with very high numbers of persons injured? How rare are these cases, and do they cluster in any borough?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-eda.html#good-practices",
    "href": "07-eda.html#good-practices",
    "title": "7  Exploratory Data Analysis",
    "section": "7.4 Good Practices",
    "text": "7.4 Good Practices\n\nPrefer simple, readable graphics over clever ones.\nKeep code blocks short and name intermediate objects clearly.\nMix visual and tabular summaries; each catches different issues.\nRecord every data change. EDA often uncovers fixes that matter later.\nRevisit EDA after modeling; residuals are just another dataset.\n\n\n\n\n\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-tidy.html",
    "href": "08-tidy.html",
    "title": "8  Tidy Data",
    "section": "",
    "text": "8.1 Introduction\nWorking effectively with data in R is greatly simplified by the tidyverse, a collection of packages designed for data science. The tidyverse provides a consistent framework for data manipulation, visualization, and modeling, which helps students learn generalizable skills rather than package-specific tricks.\nA central concept in the tidyverse is thetibble, a modern re-imagining of the traditional R data frame. Tibbles keep the familiar two-dimensional table structure but introduce improvements such as preserving variable types, supporting list columns, and displaying data more cleanly in the console. These features make them easier to use in practice, especially with large datasets.\nFinally, the idea of tidy data lies at the heart of the tidyverse. According to Hadley Wickham’s definition, tidy data means each variable forms a column, each observation forms a row, and each type of observational unit forms a table. Tidy data creates a standardized structure that enables smooth use of functions across the tidyverse, reducing the need for ad hoc data reshaping and making analyses more transparent and reproducible.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "08-tidy.html#tibbles",
    "href": "08-tidy.html#tibbles",
    "title": "8  Tidy Data",
    "section": "8.2 Tibbles",
    "text": "8.2 Tibbles\n\nWhat is a tibble?\n\nTibbles are data frames\nBut they try and enhance the regular “old” data frame from base R\n\nTo learn more\n\nvignette(\"tibble\")\n\n\nTibbles are data frames that enhance the regular “old” data frame from base R. They keep the same two-dimensional tabular structure but are designed to be more consistent, predictable, and user-friendly.\n\n8.2.1 Creating tibbles\nThere are several ways to create tibbles depending on the source of the data.\n\nFrom individual vectors\n\nThe simplest way is to build a tibble directly from vectors using tibble(). Inputs of length 1 are automatically recycled, and you can refer to variables you just created:\n\n\nCode\ntibble(\n  x = 1:5,\n  y = 1,\n  z = x ^ 2 + y\n)\n\n\n\n  \n\n\n\n\nConverting existing objects\n\nYou can convert existing data structures into tibbles with as_tibble():\n\n\nCode\n# From a data frame\nhead(as_tibble(iris))\n\n\n\n  \n\n\n\nCode\n# From a list\nas_tibble(list(x = 1:3, y = letters[1:3]))\n\n\n\n  \n\n\n\n\nReading from external files\n\nPackages in the tidyverse ecosystem return tibbles when reading data from files:\n\n\nCode\n# From CSV, TSV, or delimited text file\nhead(readr::read_csv(\"data/Chetty_2014.csv\"))\n\n\n\n  \n\n\n\nCode\n# From Excel files\n# readxl::read_excel(\"data.xlsx\")\n\n\n\nReading from databases\n\nYou can also obtain tibbles when working with databases using packages such as DBI and dbplyr:\nlibrary(DBI)\ncon &lt;- dbConnect(RSQLite::SQLite(), \"mydb.sqlite\")\ntbl(con, \"tablename\")   # returns a tibble-like object\nA tribble is a transposed tibble, designed for small data entry in code. Column headings are defined by formulas that start with ~:\n\n\nCode\ntribble(\n  ~x, ~y, ~z,\n  #--|--|----\n  \"a\", 2, 3.6,\n  \"b\", 1, 8.5\n)\n\n\n\n  \n\n\n\nThe tibble and readr packages are part of the core tidyverse, so they load automatically with library(tidyverse). Other packages such as readxl and dbplyr belong to the tidyverse ecosystem. They follow the same principles and return tibbles, but you need to load them explicitly.\n\n\n8.2.2 Features of tibbles\nTibbles make fewer automatic changes than base R data frames:\n\nThey never change the type of inputs (strings are not converted to factors).\nThey never change variable names.\nThey never create row names.\n\nFor example:\n\n\nCode\ntb &lt;- tibble(\n  `:)` = \"smile\",\n  ` ` = \"space\",\n  `2000` = \"number\"\n)\n\n\nThese column names would not be valid in base R, but are allowed in a tibble.\nThere are two main differences between tibbles and base R data frames:\nPrinting Tibbles have a refined print method that shows only the first 10 rows and only as many columns as fit on the screen:\n\n\nCode\ntibble(\n  a = lubridate::now() + runif(1e3) * 86400,\n  b = lubridate::today() + runif(1e3) * 30,\n  c = 1:1e3,\n  d = runif(1e3),\n  e = sample(letters, 1e3, replace = TRUE)\n) |&gt; head()\n\n\n\n  \n\n\n\nThis design avoids overwhelming the console when printing large data frames.\nIf you need more output, you can adjust options:\n\nprint(n = , width = ) controls number of rows and columns.\nGlobal options can be set with:\n\noptions(tibble.print_max = n, tibble.print_min = m)\noptions(tibble.print_min = Inf)     # always show all rows\noptions(tibble.width = Inf)         # always print all columns\nSubsetting Most of the subsetting tools we have used so far generally subset the entire data frame. To pull out just a single variable or value, we can use $ and [[: - [[ extracts by name or position - $ extracts by name with less typing\n\n\nCode\ndf &lt;- tibble(\n  x = runif(5),\n  y = rnorm(5)\n)\n\n# Extract by name\ndf$x\n\n\n[1] 0.3431398 0.5596790 0.3083230 0.2437374 0.2244139\n\n\nCode\ndf[[\"x\"]]\n\n\n[1] 0.3431398 0.5596790 0.3083230 0.2437374 0.2244139\n\n\nCode\n# Extract by position\ndf[[1]]\n\n\n[1] 0.3431398 0.5596790 0.3083230 0.2437374 0.2244139\n\n\nCode\ndf[[1,1]]\n\n\n[1] 0.3431398",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "08-tidy.html#tidy-up-data",
    "href": "08-tidy.html#tidy-up-data",
    "title": "8  Tidy Data",
    "section": "8.3 Tidy up data",
    "text": "8.3 Tidy up data\nStructuring datasets to facilitate analysis is at the core of the principles of tidy data, as described by Hadley Wickham.\n\n8.3.1 Tydy data\nTidy data follows three basic rules:\n\nEach variable forms a column\nEach observation forms a row\nEach type of observational unit forms a table\n\nWhen these rules are not followed, the dataset is considered untidy. Common signs of untidy data include:\n\nColumn headers are values instead of variable names\nMultiple variables are stored in one column (for example, City_State)\nVariables are stored in both rows and columns\nMultiple types of observational units are stored in the same table\nA single observational unit is stored in multiple tables\nThe dataset is either too long or too wide\n\n\n\n8.3.2 Pivoting\nMost data encountered in practice will be untidy. This is because most people are not familiar with the principles of tidy data, and data is often organised to facilitate uses other than analysis, such as making entry easier.\nTwo common problems to look for are:\n\nOne variable might be spread across multiple columns\nOne observation might be scattered across multiple rows\n\nUsually, a dataset will only suffer from one of these problems.\nTo resolve them, the tidyr package provides two key functions:\n\npivot_longer()\npivot_wider()\n\nThese functions are illustrated with example datasets included in the tidyr package. The tables (table2, table4a) contain data on the number of tuberculosis (TB) cases recorded in different countries for the years 1999 and 2000. The variable cases represents the number of TB cases reported for a given country, year, and type of measure.\n\n8.3.2.1 Pivot longer\nA common problem is a dataset where some of the column names are not variable names, but values of a variable:\n\n\nCode\ntable4a\n\n\n\n  \n\n\n\nTo tidy a dataset like this, pivot the offending columns into a new pair of variables.\nSteps:\n\nSelect the columns whose names are values, not variables. In this example, those are 1999 and 2000.\nChoose the variable to move the column names to (here, year).\nChoose the variable to move the column values to (here, cases).\n\n\n\nCode\ntable4a |&gt; \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n\n\n  \n\n\n\nIn the final result, the pivoted columns are dropped, and new year and cases columns are created. Other variables, such as country, are preserved. The cases column now explicitly records the number of TB cases for each year and country.\n\n\n\n8.3.3 Pivot wider\nFunction pivot_wider() is the opposite of pivot_longer(). You use it when an observation is scattered across multiple rows.\n\n\nCode\ntable2\n\n\n\n  \n\n\n\nTo tidy this up, analyze the representation in a similar way to pivot_longer():\n\nThe column to take variable names from (here, type).\nThe column to take values from (here, count).\n\n\n\nCode\ntable2 |&gt;\n  pivot_wider(names_from = type, values_from = count)\n\n\n\n  \n\n\n\nIn this result, values of type (cases and population) become separate columns, and their associated numbers from count fill in the values. This produces a clearer dataset where each row corresponds to a country and year with distinct variables for cases and population.\n\n\n8.3.4 Separating\nThe separate() function is used to pull apart one column into multiple columns by splitting wherever a separator character appears. This is useful when a single column actually contains more than one variable.\nConsider the dataset table3 included in the tidyr package:\n\n\nCode\ntable3\n\n\n\n  \n\n\n\nNotice the rate column. It contains two variables combined into a single column: the number of cases and the population size, separated by a forward slash. To make the dataset tidy, these should be split into separate variables.\nThe separate() function takes the name of the column to split and the names of the new columns to create:\n\n\nCode\ntable3 %&gt;%\nseparate(rate, into = c(\"cases\", \"population\"))\n\n\n\n  \n\n\n\nThis produces two new columns, cases and population, replacing the original rate column. The new columns now contain integer values for the reported tuberculosis cases and the population in each country and year.\nBy default, separate() splits values wherever it sees a non-alphanumeric character, meaning any character that is not a number or letter. In the example above, it automatically detected and split at the forward slash.\nIf you want to be explicit, you can specify the character to split on with the sep argument:\n\n\nCode\ntable3 %&gt;%\nseparate(rate, into = c(\"cases\", \"population\"), sep = \"/\")\n\n\n\n  \n\n\n\nThis ensures that the column is split exactly where expected, giving you clearer control over the separation process.\n\n\n8.3.5 Unite\nThe unite() function is the inverse of separate(). It combines multiple columns into a single column. This can be useful when two or more variables are stored in separate columns but logically belong together.\nConsider the dataset table5:\n\n\nCode\ntable5\n\n\n\n  \n\n\n\nIn this table, the year of observation is split into two columns, century and year. To make the dataset easier to work with, we can combine these into a single column.\n\n\nCode\ntable5 %&gt;% \n  unite(new, century, year)\n\n\n\n  \n\n\n\nBy default, unite() places an underscore (_) between the values from different columns. In this case, that would produce values like 19_99.\nIf we want the numbers to run together without any separator, we can control this with the sep argument:\n\n\nCode\ntable5 %&gt;% \n  unite(new, century, year, sep = \"\")\n\n\n\n  \n\n\n\nThis produces a single column new with values such as 1999 and 2000, giving a cleaner representation of the year variable.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "9  Exercises",
    "section": "",
    "text": "Setting up the computing environment Using the right computing tools and environment is a prerequisite of data science projects. Set up your computer for this course with the following steps. For each step, document what you did, the obstacles you encountered, and how you overcame them. If you used AI, document your prompts. Note that the steps you take may depend on your computer’s operating system.Think of this as a user manual for students who are new to this. Use the command line interface.\n\nInstall R.\nInstall Positron or RStudio.\nInstall Quarto.\nSet up SSH authentication between your computer and your GitHub account.\nRender your homework into an HTML file.\nPrint the HTML file into a pdf file and put it into the release of this homework assignment.\n\nGetting familiar with command line interface The command line interface (CLI) is widely used among computing professionals, even though graphical user interfaces (GUI) are more common for everyday users. Be clear and concise in your explanations. Provide short examples if you think they will help illustrate your points.\n\nResearch and explain why many professionals prefer using the CLI over a GUI. Consider aspects such as efficiency, automation, reproducibility, or remote access.\nIdentify your top five favorite commands in the Unix/Linux shell. For each command, explain what it does. If there is an option/flag you found particularly useful, describe it.\nIdentify your top five favorite Git commands. For each command, explain what it does. If there is an option/flag you found particularly useful, describe it.\n\nDo women promote different policies than men? Chattopadhyay & Duflo (2004) studied whether female policymakers make different choices than male policymakers by exploiting a unique natural experiment in India. India’s 1993 constitutional amendment required that one-third of village council head positions (pradhans) be randomly reserved for women, creating a setting where the assignment of female leaders was exogenous. In the data here, villages were randomly assigned to have a female council head. The dataset is in the source of the class notes “data/india.csv”. As shown in Table 9.1, the dataset contains four variables.\n\n\n\nTable 9.1: Variables in india.csv\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nvillage\nvillage identifier (“Gram Panchayat number _ village number”)\n\n\nfemale\nwhether village was assigned a female politician: 1 = yes, 0 = no\n\n\nwater\nnumber of new (or repaired) drinking water facilities in the village\n\n\n\nsince random assignment\n\n\nirrigation\nnumber of new (or repaired) irrigation facilities in the village\n\n\n\nsince random assignment\n\n\n\n\n\n\n\nUsing the correct R code, set your working directory.\nLoad the tidyverse package.\nLoad the data as a data.frame and assign the name india to it.\nUtilizing either head() or glimpse() view the first few rows of the dataset. Substantively describe what these functions do.\nWhat does each observation in this dataset represent?\nSubstantively interpret the first observation in the dataset.\nFor each variable in the dataset, identify the type of variable (character vs. numeric binary vs. numeric non-binary).\nHow many observations are in the dataset? In other words, how many villages were part of this experiment? Additionally, provide a substantive answer.\n\nLand of opportunity in the US Chetty et al. (2014) show that children’s chances of rising out of poverty in the U.S. vary sharply across commuting zones, with higher mobility in areas with less segregation, less inequality, stronger schools, more two-parent families, and greater social capital. The data and its variable dictionary are available as data/Chetty_2014.csv and data/Chetty_2014_dict.csv. Here we look into a subset of the data, exploring the relationship between economic mobility and CZ characteristics: household income per capita (hhi_percap). The mobility measure that you will use in this analysis captures the probability that a child born to a parent in quintile 1 moves to income quintile 5 as an adult (prob_q1q5).\n\nRead and filter data to get data only for the 100 largest commuting zones (CZ). You will work with filtered data, chetty_top100, throughout this lab.\nMake a scatterplot with household income per capita (hhi_percap) on the x-axis, and mobility (abs_mobility) on the y-axis. (A) Describe the graph: what is the approximate range of the x-axis? (B) What is the approximate range of the y-axis? (C) Do you think there is a relationship between these two variables?\nUse color to represent the geographic region (region) to your scatterplot. (A) What patterns does this reveal? (B) Describe the distribution of the data, by region.\nRepresent geographic region (region) on your scatterplot using shape instead of color. Compare the use of color vs shape to represent the region: what are the benefits and drawbacks of each?\nGoing back to the graph you just made, which uses color to represent the geographic region, add another aesthetic to represent the size of the population (pop_2000, population from the 2000 Census). Describe any relationships between size and region.\nSplit your plot into facets to display scatterplots of your data by region. (A) Compare this split plot to the combined plot earlier. Are there aspects of the relationship between hhi_percap and mobility that are easier to detect in the faceted plot than in the combined plot? (B) Which regions appear to have a relatively stronger relationship between hhi_percap and mobility?\nAdd information on the census division (division) to your graph using the color aesthetic. (A) What does this reveal about divisional differences in the West?\nCreate a plot of the relationship between hhi_percap and abs_mobility with two layers: (1) A scatterplot colored by region, and (2) a smooth fit chart with no standard error also colored by region. (A) What patterns does this illustrate in the data?\nCreate a bar graph that displays the count of CZs by region and fill each bar using information on census division. What do you learn from this graph? (A) Make new bar graphs with position dodge. (B) Make new bar graphs with position fill.\n\n\nWhat is the relative advantage of each of the three bar graphs?\n\nConnecticut schools Data on schools from the Common Core of Data (CCD) are collected by the National Center for Education Statistics. Information about the CCD can be found here. We are working with a subset of the 2013-14 dataset, data/ct_schools in the classnotes repo. A variable codebook for the CCD, which will explain what each column represents in the data file is located in the flat file in the record layout column and 2013-14 row.\n\nLoad the appropriate packages and load the data.\nData manipulation.\n\nMake a new variable sch_type that has the value Charter, Magnet or TPS, to specify if the school is a charter, magnet or traditional public school.\nHow many schools of each of these three types are in our area?\nMake a table that show the number of schools of each of the three school types that are missing data for free lunch.\n\nDescriptives.\n\nHow many schools are elementary schools? Middle schools? High schools? Other? Missing?\nHow many schools are eligible for Title I status? Summarize the count of schools in each category\n\n\nRacial composition.\n\nCreate new variables that compute the percentage of students who are Black, White, Hispanic, Asian or another race (call this variable “other”), and the percentage of students receiving free OR reduced price lunch.\nVisualize these data to see the variability of each of these variables (Hint: Use geom_histogram).\n\nVisualize the variation in the percent of students receiving free lunch (frelch) for magnet schools, TPS, and charter schools (Hint: Use geom_boxplot).\nVisualize the variation in the percent of students of each race and ethnicity in the data file (Black, White, Hispanic, Asian and Other) for each of the type of schools (magnet, TPS and charter).\nWith more R skills what types of questions could you answer using this dataset?\nWhat are some questions you have about this dataset? What information would you add to the dataset, if you could?\n\nEarly care and education, NC, 2007–2014 The data on early care and education were carefully collected by Scott Latham and colleagues at the Stanford Center for Educational Policy Analysis, in support of a recent publication, available here. Additional background on North Carolina’s Quality Rating and Improvement System (QRIS) can be found here. We have a copy in the classnotes repo: data/NC_ECE_2007-2014.csv. You can download it into your homework repo, but please do not add it to your repo; it’s over 11MB.\nInstead of a formal online codebook, Professor Latham provided the following description:\n\n“We have a panel of all licensed child care providers in North Carolina from 2007 to 2014. This includes both center-based providers and family child care homes. For all providers, we have county identifiers and zip codes. We also know facility types (e.g., independent, Head Start, local public school), enrollment, capacity, and some zip-code-level demographics (e.g., percentage below poverty, percentage Black, percentage Hispanic). For most providers, we have information on quality as measured by North Carolina’s Quality Rating and Improvement System (QRIS). However, many of these indicators are not readily interpretable because they are tied to the QRIS rubric (e.g., a 1–7 measure of teacher/staff education and credentials). The one quality measure that is relatively straightforward to interpret is the ERS rating—a widely used measure of observed classroom quality. These are elective, so we only have them for a subset of providers.”\n\n\nLoad the needed packages and the data. Take a glimpse of the data.\nMake a new data frame called nc14 that contains all NC facilities in the year 2014, with the variables fname, zip, ftype, p_pov, med_income, QRIS_ERS, and a new variable p_cap = enroll/capacity. You will use this data frame in the remaining questions.\nMake a new data frame that shows the number of facilities evaluated in each zip code in 2014, and the value of p_pov for each zip code.\n\nSort by decreasing order in the number of facilities.\nMake a plot from the data frame you made to demonstrate variation in percent poverty across zip codes in NC.\nExplain why your plot in is different from a plot that demonstrates variation in p_pov in the original data frame.\n\nAdd a new variable to nc14 that lumps together various facility types into five groups, based on the value of ftype, as follows: Independent; Franchise; Religious sponsored; Federal, Head Start or Local public school; All others.\n\nMake a plot to show the variation in p_cap across these five groups.\n\nMake a boxplot to visualize covariation between QRIS_ERS and med_income, binning med_income to treat it as a categorical variable, and representing the number of observations in each category by the width of the corresponding box.\nVisualize the relationship between med_income and QRIS_ERS using a scatter plot.\n\nVisualize the same relationship using geom_bin2d.\nDiscuss the pros and cons of each of these visualizations.\n\n\nQuality of life, NC This project was created by the UNCC Urban Institute in collaboration with the city of Charlotte, Mecklenburg County, and nearby townships. This dataset describes how various aspects of Charlotte residents’ “quality of life” varies according to neighborhood. You can explore and learn more about this project here. The data is data/qol_PS3.csv. We explore how adolescent birth rate and income are related at the neighborhood level in Mecklenburg county.\n\nLoad the appropriate packages and load the data.\nVisualize the covariation between the rate of adolescent births and median household income. What do you observe about the relationship between these variables?\nCreate a boxplot of the rate of adolescent birth by household income quartile. Use the features of the boxplot (i.e, the bar, box, whiskers and dots) to interpret your findings and discuss the pros and cons of this visualization, including breaking income up by quartile.\nUse the “cut_width” strategy for binning the household income variable. Discuss the pros and cons of this visualization.\nCreate a visualization that compares the distributions of the rate of adolescent birth for low-income neighborhoods (with a median HHI of 24257), and high-income neighborhoods (with a median HHI of 48514).\nVisualize the combined distribution of adolescent births, household income, and access to adequate prenatal care. Note that you may want to bin one or more of these variables to visualize these relationships, and you will need to find a way to represent a third variable.\nDescribe why you decided to assign variables to the asthetics you chose. How did these choices help you tell a\ncompelling story with this visualization?\n\nQuality of life, NC (continued) Conduct your own exploratory data analysis (EDA). Your EDA will be graded on your choice of an appropriate research question, the clarity of your visualizations, and how well your visualizations allow you to recognize patterns in your data and provide insight into the question you have posed.\n\nPose and answer at minimum four iterative, related EDA questions, using a minimum of 3 different variables (as in the last exercise). Write as though you were guiding future students through an exploration of the data, along with an answer key. Be sure to clearly state your EDA question, explain why you chose the visualizations that you did, and describe your findings.\nWhat were the easiest and hardest parts of this assignment?\nWith more R skills what types of questions could you answer using this dataset?\nWhat are some questions you have about this dataset? What information would you add to the dataset, if you could?\n\nRole of colleges in economic mobility This exercise is based on a data set from Chetty et al., the same author team as the economic mobility data we have worked with, but a different paper and different data that examines the role of colleges in economic mobility. If you wish, you can read about this study here.\n\nLoad the appropriate packages and the data and take a glimpse of the data.\nGet to know the data.\n\nWhat does each row in this data set represent?\nMake a table of the number of colleges in each state, sorted by number.\nExplain why there are more than 50 rows in the table you just produced even though there are only 50 states, and write code to show what are in these extra rows.\nFor the four CZs in New York with the most colleges, output a table of the CZ name, number of colleges in the CZ, and the average value of par_median in the CZ, sorted by the number of colleges in the CZ.\n\nCompare parent and child median income.\n\nMake a new variable called pk_ratio that is the ratio of parent median income to child median income, and another new variable to indicate if this ratio is low (&lt;=2), medium (between 2 and 3), or high (&gt;= 3).\nGraphically display the number of colleges with low, medium and high parent to child median income by CZ in New York\n\nVisualize income mobility.\n\nGraphically display the number of colleges in each state.\nGraphically display the distribution of mr_kq5_pq1.\n\nVisualize the variability in mr_kq5_pq1 by CZ name in New York and describe your visualization.\nVisualize the relationship between pk_ratio, mr_kq5_pq1, and trend_parq1 using geom_boxplot Describe your visualization.\n\n\n\n\n\n\nChattopadhyay, R., & Duflo, E. (2004). Women as policy makers: Evidence from a randomized policy experiment in india. Econometrica, 72(5), 1409–1443. https://doi.org/10.1111/j.1468-0262.2004.00539.x\n\n\nChetty, R., Hendren, N., Kline, P., & Saez, E. (2014). Where is the land of opportunity? The geography of intergenerational mobility in the United States. Quarterly Journal of Economics, 129(4), 1553–1623. https://doi.org/10.1093/qje/qju022",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Chattopadhyay, R., & Duflo, E. (2004). Women as policy makers:\nEvidence from a randomized policy experiment in india.\nEconometrica, 72(5), 1409–1443. https://doi.org/10.1111/j.1468-0262.2004.00539.x\n\n\nChetty, R., Hendren, N., Kline, P., & Saez, E. (2014). Where is the\nland of opportunity? The geography of intergenerational mobility in the\nUnited States. Quarterly Journal of\nEconomics, 129(4), 1553–1623. https://doi.org/10.1093/qje/qju022\n\n\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley.\n\n\nVanderPlas, J. (2016). Python data science handbook:\nEssential tools for working with data. O’Reilly Media,\nInc.\n\n\nWickham, H. (2016). ggplot2: Elegant\ngraphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWilkinson, L. (1999). The grammar of graphics. Springer-Verlag\nNew York, Inc.",
    "crumbs": [
      "References"
    ]
  }
]