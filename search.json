[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science and Society with R",
    "section": "",
    "text": "Preliminaries\nThe notes were developed with Quarto; for details about Quarto, visit https://quarto.org/docs/books.\nThis book is free and is licensed under a Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#sources-at-github",
    "href": "index.html#sources-at-github",
    "title": "Data Science and Society with R",
    "section": "Sources at GitHub",
    "text": "Sources at GitHub\nThese lecture notes for STAT/DSDA 1010 in Fall 2025 are developed by Professor Jun Yan, with help from generative AI and the students enrolled in the course. This cooperative approach to education was facilitated through the use of GitHub, a platform that encourages collaborative coding and content development. To view these contributions and the lecture notes in their entirety, please visit our GitHub repository at https://github.com/statds/1010f25.\nStudents are welcome to contribute to the lecture notes by submitting pull requests to our GitHub repository. This method not only enriched the course material but also provided students with practical experience in collaborative software development and version control.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#adapting-to-rapid-skill-acquisition",
    "href": "index.html#adapting-to-rapid-skill-acquisition",
    "title": "Data Science and Society with R",
    "section": "Adapting to Rapid Skill Acquisition",
    "text": "Adapting to Rapid Skill Acquisition\nIn this course, students are expected to rapidly acquire new skills, a critical aspect of data science. To emphasize this, consider this insightful quote from VanderPlas (2016):\n\nWhen a technologically-minded person is asked to help a friend, family member, or colleague with a computer problem, most of the time it’s less a matter of knowing the answer as much as knowing how to quickly find an unknown answer. In data science it’s the same: searchable web resources such as online documentation, mailing-list threads, and StackOverflow answers contain a wealth of information, even (especially?) if it is a topic you’ve found yourself searching before. Being an effective practitioner of data science is less about memorizing the tool or command you should use for every possible situation, and more about learning to effectively find the information you don’t know, whether through a web search engine or another means.\n\nThis quote captures the essence of what we aim to develop in our students: the ability to swiftly navigate and utilize the vast resources available to solve complex problems in data science. Examples tasks are: install needed software (or even hardware); search and find solutions to encountered problems.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#course-tools",
    "href": "index.html#course-tools",
    "title": "Data Science and Society with R",
    "section": "Course Tools",
    "text": "Course Tools\n\nR & RStudio for analysis\nQuarto for reproducible documents and dashboards\nGit & GitHub for version control and project management\nCommand line for automation and efficiency",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#policies-syllabus",
    "href": "index.html#policies-syllabus",
    "title": "Data Science and Society with R",
    "section": "Policies & Syllabus",
    "text": "Policies & Syllabus\nSee the course syllabus on HuskyCT.\nKey reminders: academic integrity, no AI-generated text in graded submissions, and professional email etiquette.\n\nGrading Rubrics\nBaseline (C level work)\n\nYour .qmd file knits to HTML without errors.\nYou answer questions correctly but do not use complete sentences.\nThere are typos and ‘junk code’ throughout the document.\nYou do not put much thought or effort into the reflection answers.\nYou do not follow the good styles in using R, Quarto, and Git.\n\nAverage (B level work)\n\nYou use complete sentences to answer questions.\nYou attempt every exercise/question.\n\nAdvanced (A level work)\n\nYour code is simple and concise.\nUnnecessary messages from R are hidden from being displayed in the HTML.\nYour document is typo-free.\nYou practice all the good styles of using R, Quarto, and Git.\nAt the discretion of the instructor, you give exceptionally thoughtful or insightful responses.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#homework-logistics",
    "href": "index.html#homework-logistics",
    "title": "Data Science and Society with R",
    "section": "Homework Logistics",
    "text": "Homework Logistics\n\nWorkflow of Submitting Homework Assignment\n\nClick the GitHub classroom assignment link in HuskyCT announcement.\nAccept the assignment and follow the instructions to an empty repository.\nMake a clone of the repo at an appropriate folder on your own computer with git clone.\nGo to this folder, add your qmd source, work on it, and group your changes to different, meaningful commits.\nPush your work to your GitHub repo with git push.\nCreate a new release and put the generated pdf file in it for ease of grading.\n\n\n\nHomework Requirements\n\nUse the repo from Git Classroom to submit your work. See Section 2  Project Management with Git.\n\nKeep the repo clean (no tracking generated files).\nNever “Upload” your files; use the git command lines.\nMake commit message informative (think about the readers).\nMake at least 10 commits and form a style of frequent small commits.\n\nUse quarto source only. See Install R, Positron (or RStudio), and Quarto.\nFor the convenience of grading, add your standalone pdf output to a release in your repo.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#final-project-logistics",
    "href": "index.html#final-project-logistics",
    "title": "Data Science and Society with R",
    "section": "Final Project Logistics",
    "text": "Final Project Logistics\nMany data science professionals work independently while collaborating across teams. This final project will give you experience completing a full data analysis cycle on your own, following the Tidy Data Workflow to transform questions into a well-designed product that communicates clear findings.\nFor your final project, you will work individually to identify a data set and complete a data analysis that answers one or more questions. Your project should generally follow the process of the Tidy Data Workflow. As this charge can be interpreted broadly, more information is outlined below.\nThis project is designed as a capstone experience, bringing together most of what you have learned this semester. You will complete all aspects of the project yourself—planning, analysis, visualization, and communication—to simulate the end-to-end workflow of a professional data scientist. The goal is to illuminate the issue through visualization of data and encourage exploration by providing a user interface in an R Shiny application.\n\n\n\n\n\n\nTip\n\n\n\nStart identifying your data source early. A good data set is rich, clean enough to work with, and aligned with your interests.\n\n\n\nGoals\n\nProduce an end-to-end data science project\n\nTake full ownership of your analysis and communication\n\nIdentify interesting data sets, develop questions that can be answered with available data, use data science techniques to draw insights, and present those insights clearly and convincingly\n\n\n\nDeliverables\n\nProject Proposal\nExploratory Data Analysis\nFinal Presentation\nFinal Report\n\n\n\nRationale for the Individual Project\n\nPractices independent data science work from start to finish.\nEncourages personal accountability and self-management.\nProvides flexibility to pursue topics reflecting individual interests and strengths.\nGives practice communicating insights in multiple formats.\nEnsures that each student demonstrates mastery of all phases of the data science workflow.\n\n\n\nProject Grading\n\nTotal project grade out of 40 points.\n\nProject Proposal = 5 points.\nExploratory Data Analysis / Proof of Concept = 5 points.\nFinal Presentation = 20 points.\nFinal User Report = 10 points.\n\n\nTo eliminate subjectivity from grading:\n- Follow instructions. - Meet deadlines. - Address all rubric items. - Produce a cohesive, reproducible workflow.\nAreas of subjectivity include verbal and written communication, graphics grammar, product design, and text mechanics.\n\n\n\n\n\n\nTip\n\n\n\nFollow each rubric item carefully and proofread your materials before submission.\n\n\n\n\nProject Expectations\nYou are responsible for managing your own timeline, ensuring that each deliverable is completed on time, and seeking feedback from the instructor when necessary. You may informally discuss ideas with peers, but all coding, analysis, and writing must be your own work.\n\nPart 1: Proposal\n\nSelect and use one publicly available data sources.\nSubmit two initial questions you intend to answer.\n\nInnovative thought\nNon-trivial\nData can be used to answer your questions\n\nSchedule a 15-minute meeting with me Nov. 14 to present your proposal.\n\nDescribe chosen data sources\nOutline variables and types\nPresent initial research questions and variables of interest\nKeep it concise\n\n\nHere is a template for your project proposal.\n\n\n\n\n\n\nMilestone\n\n\n\nProposal meeting by Nov. 14. Bring a short document and be prepared to explain your data and questions.\n\n\n\n\nPart 2: Exploratory Data Analysis\n\nChoose two of your initial questions to explore in depth\nProvide an overview of the data, describe “things to explore,” and interpret results\n\nDisplay a map with at least one UI control in your dashboard\n\nDisplay at least three different types of plots (geoms) responding to one or more UI controls\n\nWritten summary (edited, proofread HTML document)\n\nAt least one page\n\nDescribe your questions and data\n\nDepict and discuss plots and relationships\n\nProvide conclusions drawn from the analysis\n\n\n\n\nPart 3: Presentation and Report\n\nPresent your interactive R Shiny dashboard.\nPresentation length: 15 minutes.\nExplain the data used, show compelling visuals, discuss methods, and summarize your accompanying report.\n\nHere is a template for presentation slides with Quarto.\n\n\n\nAdvice\n\nChoose data that are interesting and rich enough for meaningful analysis\n\nFormulate questions that can lead to clear, data-driven answers\n\nPlan your work schedule early and stick to it\n\nEdit, proofread, and polish all deliverables carefully\n\nReach out to me early if challenges arise\n\n\n\n\n\n\n\nTip\n\n\n\nReserve the last week for polishing your Shiny app and final report.\n\n\n\n\nFinding (non-trivial) Data\n\nKaggle Public Datasets\n\nUN Humanitarian Data Exchange\n\nNYC Open Data\n\nCT Open Data\nHartford Open Data\nCSAS 2026 Data Challenge\nGoogle Dataset Search\n\nUConn Library Research Data Services\nUConn Energy Data",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "index.html#schedule-and-readings",
    "href": "index.html#schedule-and-readings",
    "title": "Data Science and Society with R",
    "section": "Schedule and Readings",
    "text": "Schedule and Readings\n\nComputing environment\n\nR4DS Ch 28-29\nHGR Ch 20-23\n\nJump start with R\n\nR4DS Ch 4-8; Ch 20-24\n\nVisualization\n\nR4DS Ch 1; Ch 9; Ch 11\nData visualization in R\n\nData manipulation\n\nR4DS Ch 3\n\nExploring data\n\nR4DS Ch 10; Ch 12-13\n\nTidy data\n\nR4DS Ch 5; Ch 19.1-19.2\n\nRelational Data\n\nR4DS Ch 14; Ch 16; Ch 17; Ch 19.3-19.6\n\nGeospatial Data\n\nUSDR Ch 1; Ch 3.1-3.4\nGCR Ch1; Ch 8.1-8.2\n\n\n\n\n\n\nVanderPlas, J. (2016). Python data science handbook: Essential tools for working with data. O’Reilly Media, Inc.",
    "crumbs": [
      "Preliminaries"
    ]
  },
  {
    "objectID": "01-computing.html",
    "href": "01-computing.html",
    "title": "1  Setting up Computing Environment",
    "section": "",
    "text": "1.1 Operating Systems\nThis chapter reviews operating systems, files, folders, paths, terminals; installing R, Positron/RStudio, and Quarto.\nMost students are familiar with Windows, but data science workflows often run on macOS or Linux. To keep everyone on the same page for command-line work, we will treat macOS and Linux as a single “Unix-like” family and help Windows users bridge to the same environment via WSL (Windows Subsystem for Linux).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Computing Environment</span>"
    ]
  },
  {
    "objectID": "01-computing.html#operating-systems",
    "href": "01-computing.html#operating-systems",
    "title": "1  Setting up Computing Environment",
    "section": "",
    "text": "Note\n\n\n\nWhat is an Operating System?\nAn operating system (OS) manages your computer’s resources (CPU, memory, storage, files, and processes) and provides interfaces (GUI and terminal) for people and programs to interact with hardware.\n\n\n\n1.1.1 The big three\n\nWindows — Ubiquitous on personal laptops; historically less aligned with Unix tooling, but excellent with WSL.\nmacOS — Unix-based; ships with a terminal and many developer tools out of the box.\nLinux — Open-source family used on servers, clusters, and cloud VMs; many distributions (Ubuntu, Fedora, Debian) share common command-line tools.\n\n\n\n\n\n\n\nTip\n\n\n\nWhy this matters in data science\nReproducibility and collaboration require knowing your OS, versions, and paths. Most research servers and HPC clusters run Linux. Learning a command-line interface (CLI) gives you a common language across systems.\n\n\n\n\n1.1.2 Quick checks: what am I running?\nOpen a terminal and run one of the following:\n# macOS / Linux / WSL\nuname -a\n# Windows (PowerShell)\nver\nIf you see Linux details on a Windows laptop, you are inside WSL.\n\n\n1.1.3 File systems and paths\n\nUnix-like path style: /home/alex/project/data.csv\nWindows path style: C:\\\\Users\\\\Alex\\\\project\\\\data.csv\n\nOn macOS/Linux, your home is typically /Users/&lt;name&gt; (macOS) or /home/&lt;name&gt; (Linux). On Windows, it is usually C:\\\\Users\\\\&lt;name&gt;.\n\n\n\n\n\n\nImportant\n\n\n\nNaming habits that save you pain\nAvoid spaces in file and folder names. Prefer kebab-case or snake-case. Keep a project’s scripts, data, and reports together.\n\n\n\n\n1.1.4 Windows Subsystem for Linux (WSL)\nWSL lets you run a real Linux environment (e.g., Ubuntu) inside Windows, so your terminal commands match those of macOS/Linux users. This is the recommended setup for Windows in this course.\n\n1.1.4.1 Install WSL (Windows 11 or Windows 10 ≥ 2004)\n\nOpen PowerShell as Administrator.\n\nRun:\n\nwsl --install\n\nWhen prompted, choose Ubuntu and set a Linux username and password.\n\n(Optional) Ensure WSL2 is the default:\n\nwsl --set-default-version 2\n\nVerify:\n\nwsl --status\nwsl -l -v\nYou should see Ubuntu listed and version 2.\n\n\n1.1.4.2 Start using WSL\n\nLaunch the Ubuntu app (or run wsl in PowerShell).\n\nYou are now at a Linux shell (bash). Try:\n\npwd         # current directory in the Linux filesystem\nls          # list files\nwhoami      # your Linux username\n\n\n1.1.4.3 Sharing files between Windows and WSL\n\nWindows drives are mounted inside Linux at /mnt/c, /mnt/d, …\nExample: C:\\\\Users\\\\Alex\\\\project appears at /mnt/c/Users/Alex/project inside WSL.\n\nYour Linux home is separate (e.g., /home/alex).\n\n\n\n\n\n\n\nTip\n\n\n\nBest practice\nKeep course projects inside your Linux home (e.g., /home/&lt;name&gt;/dsda1010) to avoid path and permissions surprises.\n\n\n\n\n1.1.4.4 Line endings and Git on Windows/WSL\nConfigure Git once to avoid CRLF/LF confusion:\ngit config --global core.autocrlf input   # recommended in WSL/macOS/Linux\ngit config --global init.defaultBranch main\nAdd a .gitattributes to normalize endings:\n* text=auto\n*.qmd text eol=lf\n*.R   text eol=lf\n*.md  text eol=lf\n*.yml text eol=lf\n\n\n\n1.1.5 Terminal quickstart by OS\n\nmacOS: Open Terminal (or iTerm2). You are in a Unix shell. Commands from the book work as-is.\n\nLinux: Open your terminal (GNOME Terminal, Konsole, etc.). You are already in a Unix shell.\n\nWindows (WSL): Open the Ubuntu app (WSL). You are now in a Linux shell that matches macOS/Linux.\n\nHere are several commonly used shell commands:\n\ncd: change directory; .. means parent directory.\npwd: present working directory.\nls: list the content of a folder; -l long version; -a show hidden files; -t ordered by modification time.\nmkdir: create a new directory.\ncp: copy file/folder from a source to a target.\nmv: move file/folder from a source to a target.\nrm: remove a file a folder.\n\n\n\n1.1.6 Common pitfalls and fixes\n\n“Command not found” — The program is not installed, or your PATH does not include it.\nPermission denied — You may be in a protected folder; work in your home directory.\nStrange characters in filenames — Avoid spaces and punctuation; stick to letters, numbers, dashes.\nMixing Windows and WSL paths — Prefer working inside your WSL home. If you must access Windows files, use /mnt/c/....\n\n\n\n1.1.7 Hands-on check (to do now)\n\nOpen your terminal (macOS/Linux/WSL).\n\nConfirm your OS with uname -a (or ver in PowerShell).\n\nCreate a course folder and a notes file:\n\nmkdir -p ~/dsda1010/week1\ncd ~/dsda1010/week1\necho \"Week 1 notes\" &gt; notes.txt\nls -l",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Computing Environment</span>"
    ]
  },
  {
    "objectID": "01-computing.html#files-folders-and-paths",
    "href": "01-computing.html#files-folders-and-paths",
    "title": "1  Setting up Computing Environment",
    "section": "1.2 Files, folders, and paths",
    "text": "1.2 Files, folders, and paths\n\nHome folder: Your personal workspace.\n\nAbsolute vs relative paths:\n\n# absolute (macOS/Linux)\n/Users/alex/projects/dsda1010\n\n# absolute (Windows)\nC:\\Users\\alex\\projects\\dsda1010\n\n# relative (from a project root)\n../data/nyc311.csv\n\nGood habits\n\nAvoid spaces in file/folder names (use-kebab-case).\nKeep project files together (R scripts, data, and reports).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Computing Environment</span>"
    ]
  },
  {
    "objectID": "01-computing.html#extensions-and-line-endings",
    "href": "01-computing.html#extensions-and-line-endings",
    "title": "1  Setting up Computing Environment",
    "section": "1.3 Extensions and line endings",
    "text": "1.3 Extensions and line endings\n\nCommon types: .qmd, .R, .csv, .tsv, .parquet, .md.\nText vs binary: keep data in text formats when possible.\nLine endings: Git normalizes these; we set .gitattributes to help.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Computing Environment</span>"
    ]
  },
  {
    "objectID": "01-computing.html#terminals-and-shells",
    "href": "01-computing.html#terminals-and-shells",
    "title": "1  Setting up Computing Environment",
    "section": "1.4 Terminals and shells",
    "text": "1.4 Terminals and shells\nOpen a terminal and try:\npwd        # print working directory\nls         # list files\ncd ..      # move up one directory\nmkdir lab  # make a folder\ncd lab       # go into the newly created folder\n\n\n\n\n\n\nTip\n\n\n\nUse Tab to autocomplete file and folder names.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Computing Environment</span>"
    ]
  },
  {
    "objectID": "01-computing.html#sec-quarto",
    "href": "01-computing.html#sec-quarto",
    "title": "1  Setting up Computing Environment",
    "section": "1.5 Install R, Positron (or RStudio), and Quarto",
    "text": "1.5 Install R, Positron (or RStudio), and Quarto\n\nInstall R from CRAN.\nInstall Positron or RStudio Desktop.\nInstall Quarto\nVerify:\n\nR --version\nquarto --version\nIn RStudio, create a new Quarto document and render it.\n\n1.5.1 Your first Quarto project\nFrom a terminal:\nmkdir dsda1010\ncd dsda1010\n# Edit hw-template.qmd\nquarto render hw-template.qmd\nOpen the folder in RStudio and inspect the generated files.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Computing Environment</span>"
    ]
  },
  {
    "objectID": "01-computing.html#troubleshooting-checklist",
    "href": "01-computing.html#troubleshooting-checklist",
    "title": "1  Setting up Computing Environment",
    "section": "1.6 Troubleshooting checklist",
    "text": "1.6 Troubleshooting checklist\n\nPATH issues: can the terminal find R and quarto?\n\nPermissions: can you write to your project folder?\n\nAntivirus or VPN blocking downloads?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Setting up Computing Environment</span>"
    ]
  },
  {
    "objectID": "02-git.html",
    "href": "02-git.html",
    "title": "2  Project Management with Git",
    "section": "",
    "text": "2.1 Install and configure Git",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "02-git.html#install-and-configure-git",
    "href": "02-git.html#install-and-configure-git",
    "title": "2  Project Management with Git",
    "section": "",
    "text": "2.1.1 Install Git\n\nmacOS: Install the Command Line Tools or use Homebrew.\n\n# Option A: Trigger Apple CLT install when you first run git\nxcode-select --install\n\n# Option B: Homebrew (preferred if you use brew)\nbrew install git\n\nWindows: Use Git for Windows (includes Git Bash) or enable WSL and install Git inside Ubuntu.\n\n# WSL (Ubuntu) inside Windows\nsudo apt update && sudo apt install -y git\n\nLinux:\n\nsudo apt update && sudo apt install -y git   # Debian/Ubuntu\n# or\nsudo dnf install -y git                       # Fedora\n\n\n2.1.2 Identify yourself to Git\nSet your name and email (must match the email used on GitHub for a clean history):\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"netid@uconn.edu\"\nOptional but recommended:\n# Show colored output and a friendlier log\ngit config --global color.ui auto\ngit config --global init.defaultBranch main\n\n# Better default editor (choose one you actually use)\n# git config --global core.editor \"code --wait\"   # VS Code\n\n\n2.1.3 Connect to GitHub: HTTPS vs SSH\n\nHTTPS + Personal Access Token (PAT): simplest to start; you paste a token when Git asks for a password.\n\nSSH keys: more convenient long-term (no token prompts). Recommended if you frequently push/pull.\n\n\n2.1.3.1 Create and add an SSH key\n# Generate a modern Ed25519 key\nssh-keygen -t ed25519 -C \"netid@uconn.edu\"\n# Press Enter to accept default path; set a passphrase when prompted\n\n# Start the agent and add your key\neval \"$(ssh-agent -s)\"\nssh-add ~/.ssh/id_ed25519\n\n# Print the public key and copy it\ncat ~/.ssh/id_ed25519.pub\nIn GitHub: Settings → SSH and GPG keys → New SSH key → paste the public key.\nTest the connection:\nssh -T git@github.com\n# Expect: \"Hi &lt;username&gt;! You've successfully authenticated...\"",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "02-git.html#github-classroom-homework-workflow",
    "href": "02-git.html#github-classroom-homework-workflow",
    "title": "2  Project Management with Git",
    "section": "2.2 GitHub Classroom: homework workflow",
    "text": "2.2 GitHub Classroom: homework workflow\nYour instructor will share a GitHub Classroom invitation link. The link automatically creates a private repository for you.\n\n2.2.1 Accept and clone the repository\n\nOpen the invitation link and accept the assignment.\n\nAfter a minute, click into your created repo (e.g., dsda1010-hw01-&lt;username&gt;).\n\nClone it once to your computer:\n\n# Using SSH (recommended)\ngit clone git@github.com:course-org/dsda1010-hw01-&lt;username&gt;.git\n\n# or using HTTPS (you will use a PAT when prompted)\n# git clone https://github.com/course-org/dsda1010-hw01-&lt;username&gt;.git\nEnter the folder, inspect starter files:\ncd dsda1010-hw01-&lt;username&gt;\nls -la\nCopy the homework template to this folder and start working on it.\nMake commits at appropriate stops.\n\n\n\n\n\n\nTip\n\n\n\nPro tip: If the repo includes a Quarto project, you can render it locally with quarto render before committing.\n\n\n\n\n2.2.2 Make changes, commit, and push\n# Check the current status\ngit status\n\n# Stage a specific file, or use `.` to stage all changes\ngit add README.md\n\n# Write a clear, imperative commit message\ngit commit -m \"Complete Q1 and add explanation\"\n\n# Push your work to GitHub\ngit push origin main\nRepeat the edit → add → commit → push loop as you progress. Your latest push before the deadline is your submission.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "02-git.html#core-git-operations-you-will-use-often",
    "href": "02-git.html#core-git-operations-you-will-use-often",
    "title": "2  Project Management with Git",
    "section": "2.3 Core Git operations you will use often",
    "text": "2.3 Core Git operations you will use often\n\n2.3.1 Create or initialize a repository\n# Start a new repo in an existing folder\ngit init\n\n# Connect it to a new remote repository on GitHub\ngit remote add origin git@github.com:&lt;user&gt;/&lt;repo&gt;.git\n\n# First commit and push\ngit add .\ngit commit -m \"Initial commit\"\ngit push -u origin main\n\n\n2.3.2 Inspect history and changes\ngit status\n\n# What changed since the last commit?\ngit diff\n\n# What changed in staged files?\ngit diff --staged\n\n# View history (pretty)\ngit log --oneline --graph --decorate --all\n\n\n2.3.3 Branching\n# Create and switch to a new branch\ngit switch -c feature/q2-solution\n\n# List branches\ngit branch -vv\n\n# Switch back\ngit switch main\n\n\n2.3.4 Merging and fast-forwards\n# On main, merge your feature branch\ngit switch main\ngit merge feature/q2-solution\n\n# Delete the merged branch\ngit branch -d feature/q2-solution\n\n\n2.3.5 Rebasing (optional, but good to know)\n# Rebase your work on top of updated main\ngit fetch origin\ngit rebase origin/main\n\n\n2.3.6 Fixing mistakes\n# Unstage a file you just added\ngit restore --staged path/to/file\n\n# Discard local changes in a file (careful: destructive)\ngit restore path/to/file\n\n# Amend the last commit message (if not yet pushed)\ngit commit --amend -m \"Better message\"\n\n# Revert a bad commit by creating a new inverse commit\ngit revert &lt;commit-sha&gt;\n\n\n2.3.7 Handling merge conflicts (quick recipe)\n# After a merge or rebase reports conflicts\ngit status            # see which files conflict\n\n# Open conflicted files, look for &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;\n# Edit to the desired final content, then:\ngit add path/to/conflicted-file\n\ngit commit            # completes a merge\n# or if rebasing:\ngit rebase --continue\n\n\n2.3.8 .gitignore essentials\nCreate a .gitignore in the project root:\n# Editors & OS\n.DS_Store\n.vscode/\n.Rproj.user/\n\n# Build outputs\n*_cache/\n*.html\n*.pdf\n\n# Dependencies\n.Rhistory\n.venv/\n__pycache__/\n.ipynb_checkpoints/",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "02-git.html#command-line-operations-you-should-know",
    "href": "02-git.html#command-line-operations-you-should-know",
    "title": "2  Project Management with Git",
    "section": "2.4 Command-line operations you should know",
    "text": "2.4 Command-line operations you should know\n\nUse these across macOS, Linux, and WSL. On Windows Git Bash, equivalents mostly work too.\n\n\n2.4.1 Navigation & inspection\npwd            # print working directory\nls -la         # list all files with details\ncd path/dir    # change directory\ncat file.txt   # print file contents\nhead -n 20 f   # first 20 lines\ntail -n 20 f   # last 20 lines\n\n\n2.4.2 Files & folders\nmkdir data figures scripts\nmv oldname.txt newname.txt\ncp src.txt backup/src.txt\nrm -i unwanted.tmp      # -i asks before deleting\n\n# Create a new file quickly\necho \"Title\" &gt; README.md\n\n# Edit with a CLI editor (choose one you like)\nnano README.md\n# or\nvim README.md\n\n\n2.4.3 Search & find\ngrep -n \"pattern\" file.txt        # search within a file\nrg -n \"pattern\" .                  # ripgrep (if installed) across project\nfind . -name \"*.qmd\"              # find matching files\n\n\n2.4.4 Environment & tooling\n# Check versions\npython --version\nR --version\ngit --version\nquarto --version\n\n# (Optional) create a Python virtual environment\npython -m venv .venv\nsource .venv/bin/activate     # Windows: .venv\\Scripts\\activate\npip install -r requirements.txt",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "02-git.html#end-to-end-demo-script-copypaste",
    "href": "02-git.html#end-to-end-demo-script-copypaste",
    "title": "2  Project Management with Git",
    "section": "2.5 End-to-end demo script (copy/paste)",
    "text": "2.5 End-to-end demo script (copy/paste)\n# 0) Accept GitHub Classroom invite, then clone your repo\ncd ~/courses/dsda1010\ngit clone git@github.com:course-org/dsda1010-hw01-&lt;user&gt;.git\ncd dsda1010-hw01-&lt;user&gt;\n\n# 1) Create a working branch\ngit switch -c work/q1\n\n# 2) Edit files (use your editor), then stage & commit\necho \"My answer to Q1\" &gt; answers/q1.md\ngit add answers/q1.md\ngit commit -m \"Answer Q1\"\n\n# 3) Merge into main and push\ngit switch main\ngit merge work/q1\ngit push -u origin main\n\n# 4) Pull in any upstream updates (if configured)\ngit fetch upstream\n# Merge or rebase as instructed\ngit merge upstream/main\n\n# 5) Verify on GitHub: files, commits, and CI checks (if any)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "02-git.html#troubleshooting-faq",
    "href": "02-git.html#troubleshooting-faq",
    "title": "2  Project Management with Git",
    "section": "2.6 Troubleshooting FAQ",
    "text": "2.6 Troubleshooting FAQ\nGit asks for a password on HTTPS and rejects it\nCreate a Personal Access Token on GitHub and use that instead of a password, or switch to SSH.\n“Permission denied (publickey)” when using SSH\nYour key is not added or not uploaded. Run ssh-add ~/.ssh/id_ed25519 then add the public key to GitHub Settings.\n“fatal: not a git repository”\nRun commands inside a folder that contains a .git directory, or run git init to create one.\nLine endings (Windows vs Unix)\nSet git config --global core.autocrlf input (macOS/Linux) or true (Windows) to avoid noisy diffs.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "02-git.html#quick-reference-cheat-sheet",
    "href": "02-git.html#quick-reference-cheat-sheet",
    "title": "2  Project Management with Git",
    "section": "2.7 Quick reference (cheat sheet)",
    "text": "2.7 Quick reference (cheat sheet)\nstatus   → what changed\nadd      → stage changes\ncommit   → record staged snapshot\npush     → upload to remote\npull     → fetch + merge\nfetch    → download without merging\nswitch   → move between branches\nmerge    → combine histories\nrebase   → replay commits on a new base\nlog      → show history\nrestore  → discard or unstage changes\nrevert   → make an inverse commit\n\n\n\n\n\n\nImportant\n\n\n\nSubmission rule of thumb: If it is not pushed to GitHub by the deadline, it is not submitted.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Project Management with Git</span>"
    ]
  },
  {
    "objectID": "03-R.html",
    "href": "03-R.html",
    "title": "3  Jump Start with R",
    "section": "",
    "text": "3.1 Starting and Quitting R\nThis chapter gives you the minimum essentials to start using R comfortably. It assumes no prior knowledge and emphasizes good habits from the very beginning. We cover how to start and quit R, get help, understand core object types, subset objects, use basic control structures, manage your working directory, and write clean code.\nCode\n## End your R session programmatically\nq()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#starting-and-quitting-r",
    "href": "03-R.html#starting-and-quitting-r",
    "title": "3  Jump Start with R",
    "section": "",
    "text": "Start Positron, open a folder as a project, and create a new script (.R) or Quarto document (.qmd).\nRun code by highlighting lines in the editor and pressing Ctrl-Enter (Win/Linux) or Cmd-Enter (Mac). The console runs one complete line at a time.\nQuit with:\n\n\n\nWhen asked to save the workspace, choose No. Rely on scripts for reproducibility.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#positron-interface",
    "href": "03-R.html#positron-interface",
    "title": "3  Jump Start with R",
    "section": "3.2 Positron Interface",
    "text": "3.2 Positron Interface\nPositron is organized into panes and a sidebar.\n\nEditor pane: main area for .R and .qmd files; supports tabs.\nConsole: interactive R prompt for quick tests.\nTerminal: a shell for system commands (e.g., git, Rscript).\nFiles: browse, create, rename, and delete items.\nEnvironment: lists objects in memory; clear with care.\nSource control: stage, commit, and view diffs in git repos.\nCommand palette: Ctrl-Shift-P or Cmd-Shift-P to search commands.\nStatus bar: shows project folder and basic status.\n\nWorking in a project\n\nOpen a folder as the project root. Use relative paths from this root.\nKeep data in data/ and scripts in R/ or src/.\n\nRunning code\n\nRun the current line or selection with Ctrl/Cmd-Enter.\nExecute a full cell in a .qmd with the Run Cell button.\n\n\n\n\n\n\n\nTip\n\n\n\nKeep the Files and Console visible. Beginners benefit from constant feedback on where they are and what ran.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#getting-help",
    "href": "03-R.html#getting-help",
    "title": "3  Jump Start with R",
    "section": "3.3 Getting Help",
    "text": "3.3 Getting Help\nR has built‑in help for every function. Every call or command you type is calling a function.\nSearch the help system on a topic:\nhelp.search(\"linear model\")\nGet the documentation of a function with known name:\n?mean\nhelp(mean)\nInspect arguments quickly for a function\n\n\nCode\nargs(mean)\n\n\nfunction (x, ...) \nNULL\n\n\nRun examples in the documentation (man page)\nexample(mean)\nPractice: find how sd() handles missing values.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#objects-in-r",
    "href": "03-R.html#objects-in-r",
    "title": "3  Jump Start with R",
    "section": "3.4 Objects in R",
    "text": "3.4 Objects in R\nEverything you store is a vector or built from vectors. Length‑one values are still vectors.\nAtomic vector types (all of fixed type):\n\n\nCode\n## Atomic vectors (length one shown; still vectors)\nnum &lt;- 3.14      ## double (numeric)\nint &lt;- 2L        ## integer\nchr &lt;- \"Ann\"     ## character\nlgc &lt;- TRUE      ## logical\n\n## A longer vector (same type throughout)\nv &lt;- c(1, 2, 3)\n\n\nHigher‑level structures built from vectors:\n\n\nCode\n## Matrix/array: same type, 2D or more\nm &lt;- matrix(1:6, nrow = 2)\n\n## List: heterogenous elements\nlst &lt;- list(name = \"Bob\", age = 25, scores = c(90, 88))\n\n## Data frame: list of equal‑length columns\n## (columns can be different atomic types)\ndf &lt;- data.frame(name = c(\"Ann\", \"Bob\"), age = c(20, 25))\n\n## Function: also an object\nsq &lt;- function(x) x^2\n\n\nInspect objects:\n\n\nCode\n## Class and structure\nclass(df)\n\n\n[1] \"data.frame\"\n\n\nCode\nstr(df)\n\n\n'data.frame':   2 obs. of  2 variables:\n $ name: chr  \"Ann\" \"Bob\"\n $ age : num  20 25\n\n\n\n\n\n\n\n\nTip\n\n\n\nPrefer str(x) for a compact view of what an object contains, its type, and its sizes.\n\n\nExercise. Create one example of each object above and check with class() and str().",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#subsetting",
    "href": "03-R.html#subsetting",
    "title": "3  Jump Start with R",
    "section": "3.5 Subsetting",
    "text": "3.5 Subsetting\nUse bracket notation consistently.\n\n\nCode\n## Vectors\nx &lt;- c(2, 4, 6, 8)\nx[2]             ## second element\n\n\n[1] 4\n\n\nCode\nx[1:3]           ## slice\n\n\n[1] 2 4 6\n\n\nCode\nx[x &gt; 5]         ## logical filter\n\n\n[1] 6 8\n\n\nCode\n## Matrices\nm &lt;- matrix(1:9, nrow = 3)\nm[2, 3]          ## row 2, col 3\n\n\n[1] 8\n\n\nCode\nm[, 1]           ## first column\n\n\n[1] 1 2 3\n\n\nCode\n## Data frames\npeople &lt;- data.frame(name = c(\"Ann\", \"Bob\"), age = c(20, 25))\npeople$age       ## column by name\n\n\n[1] 20 25\n\n\nCode\npeople[1, ]      ## first row\n\n\n\n  \n\n\n\nCode\npeople[, \"name\"] ## column by string\n\n\n[1] \"Ann\" \"Bob\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#control-structures",
    "href": "03-R.html#control-structures",
    "title": "3  Jump Start with R",
    "section": "3.6 Control Structures",
    "text": "3.6 Control Structures\n\n3.6.1 If statement (missing‑value cleaning)\n\n\nCode\n## Replace sentinel values with NA\nx &lt;- -999\nif (x == -999) {\n  x &lt;- NA\n}\nprint(x)\n\n\n[1] NA\n\n\n\n\n3.6.2 For loop (column‑wise cleaning and summary)\nUseful when applying a simple rule across columns.\n\n\nCode\n## Make a toy data frame with a sentinel value\nscores &lt;- data.frame(\n  math = c(95, -999, 88, 91),\n  eng  = c(87, 90, -999, 85),\n  sci  = c(92, 88, 94, -999)\n)\n\n## Replace -999 with NA, then compute column means\nfor (col in names(scores)) {\n  ## clean\n  bad &lt;- scores[[col]] == -999\n  scores[[col]][bad] &lt;- NA\n  ## summarize\n  m &lt;- mean(scores[[col]], na.rm = TRUE)\n  cat(col, \"mean:\", m, \"\\n\")\n}\n\n\nmath mean: 91.33333 \neng mean: 87.33333 \nsci mean: 91.33333 \n\n\n\n\n3.6.3 While loop (simulation until tolerance met)\nStop when an estimate is precise enough.\n\n\nCode\n## Estimate P(X &gt; 1.96) for N(0,1) via Monte Carlo\n## Stop when stderr &lt; 0.002\nset.seed(1)\ncount &lt;- 0\nn &lt;- 0\nse &lt;- Inf\n\nwhile (se &gt; 0.002) {\n  ## simulate in small batches for responsiveness\n  z &lt;- rnorm(1000)\n  n &lt;- n + length(z)\n  count &lt;- count + sum(z &gt; 1.96)\n  p_hat &lt;- count / n\n  se &lt;- sqrt(p_hat * (1 - p_hat) / n)\n}\n\ncat(\"p_hat:\", p_hat, \"n:\", n, \"se:\", se, \"\\n\")\n\n\np_hat: 0.0285 n: 8000 se: 0.001860368 \n\n\nExercise. Write a loop that, for each numeric column in a frame, replaces -999 with NA, then reports the fraction of missing values.\n\n\n\n\n\n\nWarning\n\n\n\nLoops are fine for clarity. Later you will see vectorized and apply‑family solutions that are faster and shorter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#workflow-basics",
    "href": "03-R.html#workflow-basics",
    "title": "3  Jump Start with R",
    "section": "3.7 Workflow Basics",
    "text": "3.7 Workflow Basics\n\n\nCode\n## Working directory\ngetwd()                  ## where am I\n\n\n[1] \"/Users/junyan/work/teaching/1010-f25/1010f25\"\n\n\nCode\n## setwd(\"path/to/folder\")   ## set if necessary\n\n\n\nIn Positron, confirm the directory in the Files pane.\nUse the console for quick tests; save work in scripts or .qmd.\nRun highlighted code with Ctrl/Cmd-Enter.\n\n\n\n\n\n\n\nTip\n\n\n\nUse project‑relative paths and file.path() to build paths. This keeps code portable across operating systems.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#importing-data",
    "href": "03-R.html#importing-data",
    "title": "3  Jump Start with R",
    "section": "3.8 Importing Data",
    "text": "3.8 Importing Data\nR can load data from text files and many other formats.\n\n3.8.1 Base R functions\n\n\nCode\n## Read a CSV file (comma-separated)\ncars &lt;- read.csv(\"data/india.csv\")\n\n## Read a general table with custom separators\nsurvey &lt;- read.table(\"data/survey.txt\", header = TRUE, sep = \" \")\n\n\nArguments to know: - header = TRUE tells R the first row has column names. - sep controls the separator (“,” for CSV, ” ” for tab‑delimited).\n\n\n\n\n\n\nTip\n\n\n\nCheck the imported object with str() or head() immediately to ensure it loaded as expected.\n\n\n\n\n3.8.2 Other formats\nThe foreign package imports legacy statistical software formats (SAS, SPSS, Stata):\n\n\nCode\nlibrary(foreign)\ndata_spss &lt;- read.spss(\"data/study.sav\", to.data.frame = TRUE)\ndata_stata &lt;- read.dta(\"data/study.dta\")\n\n\nMore modern workflows often use the haven package (part of the tidyverse) for these formats, but foreign is available in base R distributions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#good-style",
    "href": "03-R.html#good-style",
    "title": "3  Jump Start with R",
    "section": "3.9 Good Style",
    "text": "3.9 Good Style\nAdopt consistent style early. Follow the tidyverse guide: https://style.tidyverse.org/\n\nUse &lt;- for assignment.\nPlace spaces around operators and after commas.\nChoose meaningful names; avoid one‑letter names for data.\nBegin scripts with a header block.\n\n\n\nCode\n## Your Name\n## 2025-09-02\n## Purpose: demonstrate basic R style\nx &lt;- 1  # inline note uses a single \n\n\n\n\n\n\n\n\nNote\n\n\n\nComment convention. Start‑of‑line comments use at least two hashes (##). Reserve a single # for end‑of‑line notes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#tips-and-pitfalls",
    "href": "03-R.html#tips-and-pitfalls",
    "title": "3  Jump Start with R",
    "section": "3.10 Tips and Pitfalls",
    "text": "3.10 Tips and Pitfalls\n\nCase sensitivity: x and X are different.\nPaths: forward slashes / work on all platforms in R.\n\n\n\nCode\n## Portable path building\nfile.path(\"data\", \"mtcars.csv\")\n\n\n[1] \"data/mtcars.csv\"\n\n\n\nNumerical precision:\n\n\n\nCode\n## Floating‑point comparison\n0.1 == 0.3 / 3\n\n\n[1] FALSE\n\n\nCode\nall.equal(0.1, 0.3 / 3)\n\n\n[1] TRUE\n\n\nCode\n## Reveal stored value with extra digits\nprint(0.1, digits = 20)\n\n\n[1] 0.10000000000000000555\n\n\nCode\nsprintf(\"%.17f\", 0.1)\n\n\n[1] \"0.10000000000000001\"\n\n\n\n\n\n\n\n\nTip\n\n\n\nUse all.equal() (or an absolute/relative tolerance) rather than == for real‑number comparisons.\n\n\n\nSave code in scripts, not the workspace.\nUse simple file names: letters, numbers, underscores.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "03-R.html#wrapup-checklist",
    "href": "03-R.html#wrapup-checklist",
    "title": "3  Jump Start with R",
    "section": "3.11 Wrap‑Up Checklist",
    "text": "3.11 Wrap‑Up Checklist\nYou should now be able to:\n\nStart and quit R in Positron.\nGet help with functions.\nRecognize and inspect core objects with class() and str().\nSubset vectors, matrices, and data frames.\nUse if, for, and while in useful contexts.\nManage your working directory and paths.\nWrite clean, consistent code and comments.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Jump Start with R</span>"
    ]
  },
  {
    "objectID": "04-encounter.html",
    "href": "04-encounter.html",
    "title": "4  First Impression with Data",
    "section": "",
    "text": "4.1 R Packages and Data\nR is an open-source language, widely used in data science. One of its greatest strengths is the ecosystem of packages developed by the community. These packages make it easier to perform tasks such as importing data, cleaning it, and creating visualizations.\nTwo sets of packages will be central for us. Package tidyverse provides a coherent framework for data wrangling and visualization. The gapminder package offers a dataset on life expectancy, GDP per capita, and population across countries and years, which will serve as a running example in our practice.\nIf packages are not already installed, we can add them to our system with install.packages(). Installation is needed only once, but packages must be loaded every time we start a new R session.\nCode\ninstall.packages(\"tidyverse\")\ninstall.packages(\"gapminder\")\nOnce installed, packages are made available in a session by loading them with library().\nCode\nlibrary(tidyverse)\nlibrary(gapminder)\nAfter loading a dataset, it is good practice to examine its structure. Functions such as str() and summary() provide a quick overview of variable types, sample values, and ranges.\nCode\nstr(gapminder)\n\n\ntibble [1,704 × 6] (S3: tbl_df/tbl/data.frame)\n $ country  : Factor w/ 142 levels \"Afghanistan\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ continent: Factor w/ 5 levels \"Africa\",\"Americas\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ year     : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n $ lifeExp  : num [1:1704] 28.8 30.3 32 34 36.1 ...\n $ pop      : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ...\n $ gdpPercap: num [1:1704] 779 821 853 836 740 ...\n\n\nCode\nsummary(gapminder)\n\n\n        country        continent        year         lifeExp     \n Afghanistan:  12   Africa  :624   Min.   :1952   Min.   :23.60  \n Albania    :  12   Americas:300   1st Qu.:1966   1st Qu.:48.20  \n Algeria    :  12   Asia    :396   Median :1980   Median :60.71  \n Angola     :  12   Europe  :360   Mean   :1980   Mean   :59.47  \n Argentina  :  12   Oceania : 24   3rd Qu.:1993   3rd Qu.:70.85  \n Australia  :  12                  Max.   :2007   Max.   :82.60  \n (Other)    :1632                                                \n      pop              gdpPercap       \n Min.   :6.001e+04   Min.   :   241.2  \n 1st Qu.:2.794e+06   1st Qu.:  1202.1  \n Median :7.024e+06   Median :  3531.8  \n Mean   :2.960e+07   Mean   :  7215.3  \n 3rd Qu.:1.959e+07   3rd Qu.:  9325.5  \n Max.   :1.319e+09   Max.   :113523.1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First Impression with Data</span>"
    ]
  },
  {
    "objectID": "04-encounter.html#exploring-data-frames-in-r",
    "href": "04-encounter.html#exploring-data-frames-in-r",
    "title": "4  First Impression with Data",
    "section": "4.2 Exploring Data Frames in R",
    "text": "4.2 Exploring Data Frames in R\nOnce data are loaded, the next step is to explore and understand the dataset. A data frame (or tibble, in tidyverse) is the standard format for rectangular data. R provides many built-in functions to examine, summarize, and manipulate data frames.\n\n4.2.1 Structure and dimensions\n\ndim(df) – number of rows and columns\n\nnrow(df), ncol(df) – number of rows or columns separately\n\nstr(df) – internal structure (types, first few values)\n\nglimpse(df) (from dplyr) – a cleaner version of str\n\n\n\n4.2.2 Column names and metadata\n\nnames(df) or colnames(df) – list column names\n\nrownames(df) – list row names (rarely used in tidy data)\n\n\n\n4.2.3 First and last rows\n\nhead(df) – first six rows\n\ntail(df) – last six rows\n\n\n\n4.2.4 Summaries\n\nsummary(df) – variable-by-variable summaries\n\nsapply(df, class) – variable types\n\nsapply(df, function) – apply any function to each column (e.g., mean, min, max)\n\n\n\n4.2.5 Accessing columns and rows\n\ndf$var – access a column by name\n\ndf[ , \"var\"] – same as above, but more general\n\ndf[1:5, ] – first five rows\n\ndf[ , 1:3] – first three columns\n\n\n\n4.2.6 Subsetting and filtering\n\nsubset(df, condition) – filter rows by condition\n\nWith tidyverse: filter(df, condition), select(df, cols)\n\n\n\n4.2.7 Checking contents\n\nunique(df$var) – unique values in a column\n\ntable(df$var) – frequency counts\n\nis.na(df) – identify missing values\n\nTogether, these functions give a toolkit for becoming familiar with any new dataset.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First Impression with Data</span>"
    ]
  },
  {
    "objectID": "04-encounter.html#variable-types",
    "href": "04-encounter.html#variable-types",
    "title": "4  First Impression with Data",
    "section": "4.3 Variable Types",
    "text": "4.3 Variable Types\nA crucial step in working with data is recognizing the types of variables. Variable types determine how we visualize, summarize, and analyze data.\nVariables are broadly divided into numerical and categorical. Numerical variables can be continuous, such as income or life expectancy, or discrete, such as the number of siblings or a graduation year. Categorical variables can be nominal, with no inherent order (for example, country or gender), or ordinal, with an order that matters (such as education levels or rankings). R also provides support for logical variables, representing true/false values, and date variables, with built-in functions for handling time information.\n\n\n\nVariable types\n\n\n\n\nCode\n# Example: variable types in gapminder\nglimpse(gapminder)\n\n\nRows: 1,704\nColumns: 6\n$ country   &lt;fct&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", …\n$ continent &lt;fct&gt; Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, …\n$ year      &lt;int&gt; 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 1992, 1997, …\n$ lifeExp   &lt;dbl&gt; 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39.854, 40.8…\n$ pop       &lt;int&gt; 8425333, 9240934, 10267083, 11537966, 13079460, 14880372, 12…\n$ gdpPercap &lt;dbl&gt; 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, 786.1134, …\n\n\nUnderstanding variable types is not just theoretical. The type guides decisions about visualization, statistical summaries, and models. For instance, the mean is meaningful for a numerical variable but not for a nominal one.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First Impression with Data</span>"
    ]
  },
  {
    "objectID": "04-encounter.html#messy-data",
    "href": "04-encounter.html#messy-data",
    "title": "4  First Impression with Data",
    "section": "4.4 Messy Data",
    "text": "4.4 Messy Data\nReal-world datasets are rarely clean. Messiness can arise from missing values, inconsistent formats, poorly named variables, or categories coded in multiple ways. Dates might appear in different styles, proper nouns might be inconsistently capitalized, and numeric values might be stored as text.\nCleaning data involves identifying and fixing these problems. R provides many tools for this work. Missing values can be detected with is.na() and handled using functions such as na.omit(). Variable names can be adjusted with rename(). The mutate() function can change types or create new variables, and joins such as left_join() allow information from multiple tables to be combined.\n\n\nCode\n# Example: identify missing values in gapminder\nsum(is.na(gapminder))\n\n\n[1] 0\n\n\nThe tidyverse philosophy emphasizes keeping data in a “tidy” format, where each variable is a column, each observation is a row, and each type of observation forms its own table. Working toward tidy data makes later analysis and visualization much easier.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First Impression with Data</span>"
    ]
  },
  {
    "objectID": "04-encounter.html#putting-it-together",
    "href": "04-encounter.html#putting-it-together",
    "title": "4  First Impression with Data",
    "section": "4.5 Putting It Together",
    "text": "4.5 Putting It Together\nTo see these ideas in practice, consider analyzing life expectancy in African countries. We might start by filtering the data to include only Africa, checking for missing values, and confirming variable types. Once the data are tidy, we can compute summaries and produce visualizations that reveal patterns over time.\n\n\nCode\nafrica &lt;- gapminder %&gt;% filter(continent == \"Africa\")\nhead(africa)\n\n\n\n  \n\n\n\nThis example illustrates the general workflow: install and load packages, import data, understand variable types, clean messy data, and prepare the dataset for analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>First Impression with Data</span>"
    ]
  },
  {
    "objectID": "05-visual.html",
    "href": "05-visual.html",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.1 Base R Graphics: plot()\nData visualization is one of the most powerful tools in the data scientist’s toolbox. Visuals allow us to quickly summarize complex data, spot trends and outliers, and communicate results to both technical and non-technical audiences. A good visualization can illuminate patterns that might remain hidden in tables or numerical summaries, while a poor visualization can obscure the truth or even mislead. In this chapter, we explore both base R graphics and the ggplot2 package, emphasizing good practices and illustrating common pitfalls. We will also critique real world examples of misleading charts and learn how to improve them.\nR has a built-in graphics system that allows us to create plots quickly. The plot() function is versatile: depending on the type of data it is given, it can produce scatterplots, line plots, or even factor-based displays. This makes plot() an excellent starting point for beginners.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visual.html#base-r-graphics-plot",
    "href": "05-visual.html#base-r-graphics-plot",
    "title": "5  Data Visualization",
    "section": "",
    "text": "5.1.1 Scatterplot\nScatterplots display the relationship between two continuous variables. In the example below, we investigate how car weight relates to fuel efficiency using the built-in mtcars dataset.\n\n\nCode\n### simple scatter using built-in `mtcars`\nplot(mtcars$wt, mtcars$mpg,\n     main = \"Fuel efficiency vs. weight\",\n     xlab = \"Weight (1000 lbs)\", ylab = \"MPG\",\n     pch = 19, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\n\n\n5.1.2 Line plot\nWhen data are ordered, such as time series or physical measurements, line plots are appropriate. The following plot shows how pressure changes with temperature.\n\n\nCode\n## line plot via type='l'\nplot(pressure$temperature, pressure$pressure,\n     type = \"l\", lwd = 2,\n     main = \"Pressure vs. Temperature\",\n     xlab = \"Temperature\", ylab = \"Pressure\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTip. Use options like pch, col, cex, and type to control appearance in base graphics. These adjustments can make exploratory plots more readable and more informative.\n\n\nBase R graphics are quick and convenient, but they can be inconsistent and limited when creating complex or publication-quality graphics. This motivates the use of a more systematic framework.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visual.html#ggplot2-basics",
    "href": "05-visual.html#ggplot2-basics",
    "title": "5  Data Visualization",
    "section": "5.2 ggplot2 Basics",
    "text": "5.2 ggplot2 Basics\nAlthough base R allows us to make plots quickly, its commands are not always consistent, and combining multiple layers can be challenging. This is where the Grammar of Graphics comes in (Wilkinson, 1999). The the strongest reason is that grammar teaches you to think systematically about how graphics are constructed, not just how to make a specific chart.\nThat means instead of thinking “I want a scatterplot” or “I want a bar chart,” you think in terms of layers of grammar:\n\nData: what dataset to use.\nAesthetics: how variables map to visual properties (x, y, color, size, etc.).\nGeoms: what geometric objects to draw (points, lines, bars).\nStats: statistical transformations (counts, smoothing, regression fits).\nScales: how data values are translated into colors, axes, and sizes.\nFacets: how to split data into subplots for comparison.\nThemes: the non-data ink (fonts, grid lines, background).\n\nThe ggplot2package (Wickham, 2016) implements this grammar, making it possible to build complex visualizations piece by piece. Because of this structure, ggplot2 is:\n\nConsistent: once you know the grammar, you can build almost any plot without learning a new function each time.\nExtensible: the same framework supports extensions (e.g., gganimate, ggrepel, patchwork).\nReproducible: code expresses the intent clearly, which is especially useful for teaching and communication.\n\nThe syntax involves calling ggplot() with a dataset and aesthetic mappings (via aes()), then adding layers such as geom_point() or geom_line() with the + operator. Additional layers for smoothing, faceting, and themes give us rich control over the appearance of plots.\nFor more syntax, see ggplot Cheatsheet\n\n5.2.1 Exploring the mpg dataset\nWe begin by loading the tidyverse, which includes ggplot2, and looking at the mpg dataset.\n\n\nCode\nlibrary(tidyverse)\n\nmpg\n\n\n\n  \n\n\n\nHere, mpg is a data frame containing information on car models, including engine displacement, highway mileage, and class.\n\n\nCode\nglimpse(mpg)\nView(mpg)\n?mpg\n\n\nThe functions glimpse() and View() allow us to quickly inspect the structure of the dataset, while ?mpg shows documentation.\n\n\n5.2.2 First ggplot calls\nBefore plotting, we might check available geoms:\n\n\nCode\n?ggplot\n?geom_point\n?geom_line\n\n\n\n\n5.2.3 Using pipes\nPipes make code easier to read by passing the result of one expression into the next. Instead of nesting functions, we can write a sequence of operations in the order we think about them. There are two main pipes in R: the base R pipe |&gt; (available since R 4.1) and the magrittr pipe %&gt;% (commonly used in the tidyverse).\nBoth pipes take the left-hand side and feed it into the first argument of the right-hand side.\n\n\nCode\n# Base R pipe\nmpg |&gt; head()\n\n\n\n  \n\n\n\nCode\n# Magrittr pipe (needs library(magrittr) or tidyverse)\nmpg %&gt;% head()\n\n\n\n  \n\n\n\nWe can now create a basic scatterplot of engine displacement vs highway mileage.\n\n\nCode\nmpg |&gt;\n  ggplot() +\n  geom_point(aes(displ, hwy))\n\n\nThis produces a scatterplot with displ on the x-axis and hwy on the y-axis. Swapping the variables simply flips the axes:\n\n\nCode\nmpg |&gt;\n  ggplot() +\n  geom_point(aes(hwy, displ))\n\n\n\n\n5.2.4 Building layers\nWe can add additional layers. For example, combining points with a line layer:\n\n\nCode\nmpg %&gt;%\n  ggplot() + \n  geom_point(aes(displ, hwy)) +\n  geom_line(aes(displ, hwy), color = \"tomato\")\n\n\n\n\n5.2.5 Adding aesthetics\nColor can highlight categories such as car class:\n\n\nCode\nmpg %&gt;%\n  ggplot() + \n  geom_point(aes(displ, hwy, color = class))\n\n\n\n\n5.2.6 Adding smoothers\nA smoothing curve helps reveal overall trends.\n\n\nCode\nmpg %&gt;%\n  ggplot() +\n  geom_point(aes(displ, hwy, color = class)) +\n  geom_smooth(aes(displ, hwy))\n\n\nThemes can alter the look of the plot:\n\n\nCode\nmpg %&gt;%\n  ggplot() +\n  geom_point(aes(displ, hwy, color = class)) +\n  geom_smooth(aes(displ, hwy)) +\n  theme_bw()\n\n\n\n\n5.2.7 Customization\nWe can set fixed aesthetics outside aes():\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(displ, hwy)) + \n  geom_point(color = \"steelblue\", size = 3)\n\n\nTransparency can improve clarity:\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(displ, hwy, color = class)) +\n  geom_point(size = 2, alpha = 0.8)\n\n\n\n\n5.2.8 Adding regression lines\nWe can fit smoothers with different methods:\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(displ, hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE)\n\n\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(displ, hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n5.2.9 Titles, labels, and themes\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(displ, hwy, color = class)) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Fuel efficiency vs. engine displacement\",\n    x = \"Engine displacement (liters)\",\n    y = \"Highway MPG\"\n  ) +\n  theme_minimal()\n\n\n\n\n5.2.10 Faceting\nWe can split data into subplots by categories.\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(displ, hwy, color = class)) +\n  geom_point(size = 2) +\n  facet_wrap(~ class)\n\n\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(displ, hwy)) +\n  geom_point() +\n  facet_grid(drv ~ cyl)\n\n\n\n\n5.2.11 Other plot types\nBar charts summarize categorical data:\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(class)) +\n  geom_bar()\n\n\nHistograms show distributions:\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(hwy)) +\n  geom_histogram(bins = 20)\n\n\nDensity plots are another way to display distributions:\n\n\nCode\nmpg %&gt;%\n  ggplot(aes(hwy)) +\n  geom_density()\n\n\n\n\n5.2.12 Other ggplot functions\n\ncoord_flip flips the x and y axis to improve the readability of plots\nscales change the formatting of x and y axes\nplotly makes plots interactive; you can hover over points/lines for more information\nlabs allows you to add/edit a title, subtitle, a caption, and change the x and y axis labels\ngganimate allows you to animate plots into gifs",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visual.html#misleading-charts",
    "href": "05-visual.html#misleading-charts",
    "title": "5  Data Visualization",
    "section": "5.3 Misleading Charts",
    "text": "5.3 Misleading Charts\nVisualizations can be abused to mislead. It is important to learn how to critically assess charts we see in the media. The following real-world examples show common problems and better alternatives.\n\n5.3.1 Example 1: Truncated Bar Chart\n\n\nCode\napproval &lt;- data.frame(year = c(2000, 2005), percent = c(77, 65))\nbarplot(approval$percent, names.arg = approval$year,\n        ylim = c(60, 80), col = \"tomato\",\n        main = \"Approval Ratings (misleading)\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nbarplot(approval$percent, names.arg = approval$year,\n        ylim = c(0, 100), col = \"steelblue\",\n        main = \"Approval Ratings (truthful)\")\n\n\n\n\n\n\n\n\n\n\n\n5.3.2 Example 2: Straw Poll Graphic\n\n\nCode\npoll &lt;- data.frame(candidate = c(\"A\", \"B\", \"C\", \"D\"),\n                   support = c(22, 18, 15, 10))\nbarplot(poll$support, names.arg = poll$candidate,\n        col = c(\"red\", \"blue\", \"green\", \"purple\"),\n        main = \"Poll Results (misleading)\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nlibrary(ggplot2)\npoll |&gt; ggplot(aes(x = reorder(candidate, support), y = support)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip() +\n  labs(title = \"Poll Results (truthful)\",\n       x = \"Candidate\", y = \"Support (%)\")\n\n\n\n\n\n\n\n\n\n\n\n5.3.3 Example 3: Donut vs Bar Chart\n\n\nCode\nshares &lt;- data.frame(group = c(\"X\", \"Y\", \"Z\"), value = c(30, 50, 20))\nshares |&gt; ggplot(aes(x = 2, y = value, fill = group)) +\n  geom_col(width = 1, color = \"white\") +\n  coord_polar(theta = \"y\") +\n  xlim(0.5, 2.5) +\n  theme_void() +\n  labs(title = \"Shares (donut, misleading)\")\n\n\n\n\n\n\n\n\n\n\n\nCode\nshares |&gt; ggplot(aes(x = group, y = value, fill = group)) +\n  geom_col() +\n  labs(title = \"Shares (bar chart)\",\n       x = NULL, y = \"Value\")\n\n\n\n\n\n\n\n\n\nPie charts (and donut charts, which are essentially pies with a hole in the middle) are widely criticized because humans are not good at accurately comparing angles or areas. Judgments based on angles are much less precise than those based on position or length. This makes pie charts poor at conveying quantitative comparisons, especially when slices are similar in size. Donut charts exacerbate the problem by removing the center, which eliminates a natural visual baseline (the full radius), making angle judgments even harder. For these reasons, most visualization experts recommend bar charts instead, where lengths aligned to a common baseline support more accurate comparisons.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visual.html#good-practice",
    "href": "05-visual.html#good-practice",
    "title": "5  Data Visualization",
    "section": "5.4 Good Practice",
    "text": "5.4 Good Practice\n\n5.4.1 Bad Plots\n\nTruncated axes exaggerate differences.\n\n3D effects distort perception.\n\nPie/donut charts hinder comparisons.\n\nOverplotting or excessive colors obscure patterns.\n\nInconsistent scales or ordering confuse the audience.\n\n\n\n5.4.2 Good Plots\n\nStart bar charts at zero to preserve proportion.\n\nUse simple, clear chart types.\n\nProvide informative labels and titles.\n\nKeep scales and colors consistent.\n\nAvoid unnecessary clutter.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visual.html#in-class-example",
    "href": "05-visual.html#in-class-example",
    "title": "5  Data Visualization",
    "section": "5.5 In-Class Example",
    "text": "5.5 In-Class Example\nConsider the data of Chetty et al. (2014).\n\nVisualize the relationship between social capital and absolute mobility. Do you see a correlation? Is it what you expected from the Chetty et al. (2014) study executive summary?\nAdd an aesthetic to your graph to represent whether the CZ is urban or not.\nSeparate urban and non-urban CZ’s into two separate plots.\nAdd a smooth fit to each of your plots above. Experiment with adding the option method=\"lm\" in your geom_smooth. What does this option do?\nWhich variables in the chetty data frame are appropriate x variables for a bar graph?\nMake two separate bar graphs for two different x variables.\nMake two more bar graphs that display proportions rather than counts of your selected variables.\nMake a bar graph that lets you compare the number of urban and rural CZ’s in each of the four regions.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "05-visual.html#summary",
    "href": "05-visual.html#summary",
    "title": "5  Data Visualization",
    "section": "5.6 Summary",
    "text": "5.6 Summary\nWe have explored both base R plotting and the ggplot2 grammar of graphics. Base R offers quick and simple plotting functions, but lacks consistency for more advanced tasks. ggplot2 provides a flexible and layered system, allowing us to build complex visualizations step by step. By studying both good and bad visualizations, we learn not only how to make effective charts but also how to critically evaluate visuals we encounter in practice.\n\n\n\n\nChetty, R., Hendren, N., Kline, P., & Saez, E. (2014). Where is the land of opportunity? The geography of intergenerational mobility in the United States. Quarterly Journal of Economics, 129(4), 1553–1623. https://doi.org/10.1093/qje/qju022\n\n\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWilkinson, L. (1999). The grammar of graphics. Springer-Verlag New York, Inc.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html",
    "href": "06-manipulation.html",
    "title": "6  Data Manipulation",
    "section": "",
    "text": "6.1 Introduction\nRaw data are rarely ready for direct analysis. We often need to reshape, filter, or summarize before we can create meaningful plots or fit statistical models. The tidyverse provides a consistent grammar for these operations, with dplyr as its central package.\nIn this chapter, we will learn the most important data manipulation verbs. Each verb is a function that takes a data frame (or tibble) as the first argument, applies some manipulation, and returns a new data frame.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#introduction",
    "href": "06-manipulation.html#introduction",
    "title": "6  Data Manipulation",
    "section": "",
    "text": "Backward Compatibility in the Tidyverse\n\n\n\nThe tidyverse strives to minimize disruption, but backward compatibility is not guaranteed. Breaking changes sometimes occur—especially in major releases—to improve consistency or fix design issues. Functions are usually deprecated with warnings before removal, giving time to update code. For long-term stability, pin package versions with tools like renv and always review release notes when upgrading.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#core-dplyr-verbs",
    "href": "06-manipulation.html#core-dplyr-verbs",
    "title": "6  Data Manipulation",
    "section": "6.2 Core dplyr Verbs",
    "text": "6.2 Core dplyr Verbs\nThe six most commonly used verbs are:\n\nfilter() — select rows based on conditions\n\narrange() — reorder rows\n\nselect() — choose columns\n\nmutate() — add or modify columns\n\ngroup_by() — define groups for analysis\n\nsummarise() — collapse groups into summaries\n\nAll verbs follow the same pattern: the first argument is a data frame, and subsequent arguments describe manipulations using column names.\nWe will illustrate these verbs using the nycflights13::flights dataset.\n\n\nCode\nlibrary(nycflights13)\nflights &lt;- nycflights13::flights\nstr(flights)\n\n\ntibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n\n\n\n6.2.1 Filtering Rows with filter()\nThe filter() function selects rows that satisfy logical conditions. Logical operators like & (and), | (or), and ! (not) are often used, along with comparisons such as ==, !=, &lt;, and &gt;=.\n\n== equals\n!= not equal\n&lt; less than, &lt;= less than or equal\n&gt; greater than, &gt;= greater than or equal\n& logical AND (both conditions must be true)\n| logical OR (at least one condition must be true)\n! logical NOT (negates a condition)\n\nExamples with the flights data:\n\n\nCode\n# Flights on January 1\nflights |&gt;\nfilter(month == 1 & day == 1)\n\n\n\n  \n\n\n\nCode\n# Flights in January or February\nflights |&gt;\nfilter(month == 1 | month == 2)\n\n\n\n  \n\n\n\nCode\n# Flights not in January\nflights |&gt;\nfilter(!(month == 1))\n\n\n\n  \n\n\n\nCode\n# Flights with arrival delay over 2 hours and from JFK\nflights |&gt;\nfilter(arr_delay &gt; 120 & origin == \"JFK\")\n\n\n\n  \n\n\n\nCode\n# Flights that were either very early (dep_delay &lt; -15) or very late (dep_delay &gt; 120)\nflights |&gt;\nfilter(dep_delay &lt; -15 | dep_delay &gt; 120)\n\n\n\n  \n\n\n\nCode\n# Flights in summer months AND either from JFK or LGA\nflights |&gt;\nfilter((month %in% c(6,7,8)) & (origin == \"JFK\" | origin == \"LGA\"))\n\n\n\n  \n\n\n\nCode\n# Flights in December with departure delay over 2 hours OR (in January with arrival delay over 2 hours)\nflights |&gt;\nfilter((month == 12 & dep_delay &gt; 120) | (month == 1 & arr_delay &gt; 120))\n\n\n\n  \n\n\n\nCode\n# Flights from EWR where either (dep_delay &gt; 60 AND arr_delay &gt; 60) OR (dep_delay &lt; -30 AND arr_delay &lt; -30)\nflights |&gt;\nfilter(origin == \"EWR\" & ((dep_delay &gt; 60 & arr_delay &gt; 60) |\n(dep_delay &lt; -30 & arr_delay &lt; -30)))\n\n\n\n  \n\n\n\n\n6.2.1.1 Handling Missing Values\nMissing values (NA) require care. For example:\n\n\nCode\nstocks &lt;- data.frame(\n  year   = c(2015, 2015, 2016, 2016),\n  qtr    = c(1, 2, 2, NA),\n  return = c(1.1, NA, 0.9, 2.0)\n)\n\nstocks |&gt; filter(is.na(qtr) | is.na(return))\n\n\n\n  \n\n\n\nCode\nstocks |&gt; filter(!is.na(qtr) & !is.na(return))\n\n\n\n  \n\n\n\n\n\n\n6.2.2 Reordering Rows with arrange()\narrange() sorts rows by column values. Missing values are sorted last.\n\n\nCode\nflights |&gt; arrange(year, month, day) |&gt; str()\n\n\ntibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n\n\nDescending order is done with desc():\n\n\nCode\nflights |&gt; arrange(desc(dep_delay)) |&gt; str()\n\n\ntibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 6 1 9 7 4 3 6 7 12 ...\n $ day           : int [1:336776] 9 15 10 20 22 10 17 27 22 5 ...\n $ dep_time      : int [1:336776] 641 1432 1121 1139 845 1100 2321 959 2257 756 ...\n $ sched_dep_time: int [1:336776] 900 1935 1635 1845 1600 1900 810 1900 759 1700 ...\n $ dep_delay     : num [1:336776] 1301 1137 1126 1014 1005 ...\n $ arr_time      : int [1:336776] 1242 1607 1239 1457 1044 1342 135 1236 121 1058 ...\n $ sched_arr_time: int [1:336776] 1530 2120 1810 2210 1815 2211 1020 2226 1026 2020 ...\n $ arr_delay     : num [1:336776] 1272 1127 1109 1007 989 ...\n $ carrier       : chr [1:336776] \"HA\" \"MQ\" \"MQ\" \"AA\" ...\n $ flight        : int [1:336776] 51 3535 3695 177 3075 2391 2119 2007 2047 172 ...\n $ tailnum       : chr [1:336776] \"N384HA\" \"N504MQ\" \"N517MQ\" \"N338AA\" ...\n $ origin        : chr [1:336776] \"JFK\" \"JFK\" \"EWR\" \"JFK\" ...\n $ dest          : chr [1:336776] \"HNL\" \"CMH\" \"ORD\" \"SFO\" ...\n $ air_time      : num [1:336776] 640 74 111 354 96 139 167 313 109 149 ...\n $ distance      : num [1:336776] 4983 483 719 2586 589 ...\n $ hour          : num [1:336776] 9 19 16 18 16 19 8 19 7 17 ...\n $ minute        : num [1:336776] 0 35 35 45 0 0 10 0 59 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-09 09:00:00\" \"2013-06-15 19:00:00\" ...\n\n\n\n\n6.2.3 Selecting Columns with select()\nselect() picks out specific columns. Useful helpers include starts_with() and ends_with().\n\n\nCode\nflights |&gt; select(year, month, day) |&gt; str()\n\n\ntibble [336,776 × 3] (S3: tbl_df/tbl/data.frame)\n $ year : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month: int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day  : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n\n\nCode\nflights |&gt; select(starts_with(\"dep\")) |&gt; str()\n\n\ntibble [336,776 × 2] (S3: tbl_df/tbl/data.frame)\n $ dep_time : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ dep_delay: num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n\n\nColumns can also be renamed inline:\n\n\nCode\nflights |&gt; select(tail = tailnum, everything()) |&gt; str()\n\n\ntibble [336,776 × 19] (S3: tbl_df/tbl/data.frame)\n $ tail          : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n\n\n\n\n6.2.4 Creating New Columns with mutate()\nmutate() adds new variables or modifies existing ones.\n\n\nCode\nflights |&gt; mutate(speed = distance / (air_time/60)) |&gt; str()\n\n\ntibble [336,776 × 20] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n $ speed         : num [1:336776] 370 374 408 517 394 ...\n\n\nIt is common to create multiple new variables at once:\n\n\nCode\nflights |&gt; mutate(\n  gain = dep_delay - arr_delay,\n  hours = air_time / 60\n) |&gt; str()\n\n\ntibble [336,776 × 21] (S3: tbl_df/tbl/data.frame)\n $ year          : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day           : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ dep_time      : int [1:336776] 517 533 542 544 554 554 555 557 557 558 ...\n $ sched_dep_time: int [1:336776] 515 529 540 545 600 558 600 600 600 600 ...\n $ dep_delay     : num [1:336776] 2 4 2 -1 -6 -4 -5 -3 -3 -2 ...\n $ arr_time      : int [1:336776] 830 850 923 1004 812 740 913 709 838 753 ...\n $ sched_arr_time: int [1:336776] 819 830 850 1022 837 728 854 723 846 745 ...\n $ arr_delay     : num [1:336776] 11 20 33 -18 -25 12 19 -14 -8 8 ...\n $ carrier       : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ flight        : int [1:336776] 1545 1714 1141 725 461 1696 507 5708 79 301 ...\n $ tailnum       : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ origin        : chr [1:336776] \"EWR\" \"LGA\" \"JFK\" \"JFK\" ...\n $ dest          : chr [1:336776] \"IAH\" \"IAH\" \"MIA\" \"BQN\" ...\n $ air_time      : num [1:336776] 227 227 160 183 116 150 158 53 140 138 ...\n $ distance      : num [1:336776] 1400 1416 1089 1576 762 ...\n $ hour          : num [1:336776] 5 5 5 5 6 5 6 6 6 6 ...\n $ minute        : num [1:336776] 15 29 40 45 0 58 0 0 0 0 ...\n $ time_hour     : POSIXct[1:336776], format: \"2013-01-01 05:00:00\" \"2013-01-01 05:00:00\" ...\n $ gain          : num [1:336776] -9 -16 -31 17 19 -16 -24 11 5 -10 ...\n $ hours         : num [1:336776] 3.78 3.78 2.67 3.05 1.93 ...\n\n\n\n\n6.2.5 Grouping and Summarizing\nThe group_by() function defines groups of rows, and summarise() reduces each group to summary statistics.\n\n\nCode\nflights |&gt;\n  group_by(month) |&gt;\n  summarise(delay = mean(dep_delay, na.rm = TRUE))\n\n\n\n  \n\n\n\nMultiple grouping variables are possible:\n\n\nCode\nflights |&gt;\n  group_by(carrier, month) |&gt;\n  summarise(\n    delay = mean(dep_delay, na.rm = TRUE),\n    n = n()\n  ) |&gt; str()\n\n\ngropd_df [185 × 4] (S3: grouped_df/tbl_df/tbl/data.frame)\n $ carrier: chr [1:185] \"9E\" \"9E\" \"9E\" \"9E\" ...\n $ month  : int [1:185] 1 2 3 4 5 6 7 8 9 10 ...\n $ delay  : num [1:185] 16.9 16.5 13.4 13.6 22.7 ...\n $ n      : int [1:185] 1573 1459 1627 1511 1462 1437 1494 1456 1540 1673 ...\n - attr(*, \"groups\")= tibble [16 × 2] (S3: tbl_df/tbl/data.frame)\n  ..$ carrier: chr [1:16] \"9E\" \"AA\" \"AS\" \"B6\" ...\n  ..$ .rows  : list&lt;int&gt; [1:16] \n  .. ..$ : int [1:12] 1 2 3 4 5 6 7 8 9 10 ...\n  .. ..$ : int [1:12] 13 14 15 16 17 18 19 20 21 22 ...\n  .. ..$ : int [1:12] 25 26 27 28 29 30 31 32 33 34 ...\n  .. ..$ : int [1:12] 37 38 39 40 41 42 43 44 45 46 ...\n  .. ..$ : int [1:12] 49 50 51 52 53 54 55 56 57 58 ...\n  .. ..$ : int [1:12] 61 62 63 64 65 66 67 68 69 70 ...\n  .. ..$ : int [1:12] 73 74 75 76 77 78 79 80 81 82 ...\n  .. ..$ : int [1:12] 85 86 87 88 89 90 91 92 93 94 ...\n  .. ..$ : int [1:12] 97 98 99 100 101 102 103 104 105 106 ...\n  .. ..$ : int [1:12] 109 110 111 112 113 114 115 116 117 118 ...\n  .. ..$ : int [1:5] 121 122 123 124 125\n  .. ..$ : int [1:12] 126 127 128 129 130 131 132 133 134 135 ...\n  .. ..$ : int [1:12] 138 139 140 141 142 143 144 145 146 147 ...\n  .. ..$ : int [1:12] 150 151 152 153 154 155 156 157 158 159 ...\n  .. ..$ : int [1:12] 162 163 164 165 166 167 168 169 170 171 ...\n  .. ..$ : int [1:12] 174 175 176 177 178 179 180 181 182 183 ...\n  .. ..@ ptype: int(0) \n  ..- attr(*, \".drop\")= logi TRUE\n\n\nSometimes, the keeping the group structured after summarising can be handy, which can be achieved with .groups = \"keep\". In this case, groups persist unless they are explicitly removed with ungroup(). With the default summarise(), a single grouping variable is dropped, producing an ungrouped result; so ungroup() after that is redundant.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#reshaping-data",
    "href": "06-manipulation.html#reshaping-data",
    "title": "6  Data Manipulation",
    "section": "6.3 Reshaping Data",
    "text": "6.3 Reshaping Data\nReal-world data often need reshaping. Tidy data prefers one observation per row, one variable per column.\n\n6.3.1 Wide to Long: pivot_longer()\n\n\nCode\ntable4a &lt;- tibble(\n  country = c(\"A\", \"B\"),\n  `1999` = c(745, 377),\n  `2000` = c(377, 345)\n)\n\ntable4a |&gt; pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n\n\n  \n\n\n\n\n\n6.3.2 Long to Wide: pivot_wider()\n\n\nCode\ntable2 &lt;- tibble(\n  country = c(\"A\", \"A\", \"B\", \"B\"),\n  year = c(1999, 2000, 1999, 2000),\n  type = c(\"cases\", \"cases\", \"cases\", \"cases\"),\n  count = c(745, 377, 377, 345)\n)\n\ntable2 |&gt; pivot_wider(names_from = type, values_from = count)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#combining-data-from-multiple-tables",
    "href": "06-manipulation.html#combining-data-from-multiple-tables",
    "title": "6  Data Manipulation",
    "section": "6.4 Combining Data from Multiple Tables",
    "text": "6.4 Combining Data from Multiple Tables\nJoins combine data from two tables based on a common key column. The type of join determines which rows are kept:\n\nInner Join: Keeps only rows with matching keys in both tables.\nLeft Join: Keeps all rows from the left table, adding matches from the right table where available (missing values filled with NA).\nRight Join: Keeps all rows from the right table, adding matches from the left table where available.\nFull Join: Keeps all rows from both tables, filling unmatched values with NA.\n\nConsider the planes data.\n\n\nCode\nplanes &lt;- nycflights13::planes\nstr(planes)\n\n\ntibble [3,322 × 9] (S3: tbl_df/tbl/data.frame)\n $ tailnum     : chr [1:3322] \"N10156\" \"N102UW\" \"N103US\" \"N104UW\" ...\n $ year        : int [1:3322] 2004 1998 1999 1999 2002 1999 1999 1999 1999 1999 ...\n $ type        : chr [1:3322] \"Fixed wing multi engine\" \"Fixed wing multi engine\" \"Fixed wing multi engine\" \"Fixed wing multi engine\" ...\n $ manufacturer: chr [1:3322] \"EMBRAER\" \"AIRBUS INDUSTRIE\" \"AIRBUS INDUSTRIE\" \"AIRBUS INDUSTRIE\" ...\n $ model       : chr [1:3322] \"EMB-145XR\" \"A320-214\" \"A320-214\" \"A320-214\" ...\n $ engines     : int [1:3322] 2 2 2 2 2 2 2 2 2 2 ...\n $ seats       : int [1:3322] 55 182 182 182 55 182 182 182 182 182 ...\n $ speed       : int [1:3322] NA NA NA NA NA NA NA NA NA NA ...\n $ engine      : chr [1:3322] \"Turbo-fan\" \"Turbo-fan\" \"Turbo-fan\" \"Turbo-fan\" ...\n\n\nJoin the flights data and the planes data:\n\n\nCode\nflights2 &lt;- flights |&gt; select(year, month, day, carrier, tailnum)\nplanes2 &lt;- planes |&gt; select(tailnum, type, manufacturer)\n\nflights2 |&gt; left_join(planes2, by = \"tailnum\") |&gt; str()\n\n\ntibble [336,776 × 7] (S3: tbl_df/tbl/data.frame)\n $ year        : int [1:336776] 2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...\n $ month       : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ day         : int [1:336776] 1 1 1 1 1 1 1 1 1 1 ...\n $ carrier     : chr [1:336776] \"UA\" \"UA\" \"AA\" \"B6\" ...\n $ tailnum     : chr [1:336776] \"N14228\" \"N24211\" \"N619AA\" \"N804JB\" ...\n $ type        : chr [1:336776] \"Fixed wing multi engine\" \"Fixed wing multi engine\" \"Fixed wing multi engine\" \"Fixed wing multi engine\" ...\n $ manufacturer: chr [1:336776] \"BOEING\" \"BOEING\" \"BOEING\" \"AIRBUS\" ...\n\n\nCode\n# Try inner, right, and full join too\n\n\nOther join types include inner_join(), right_join(), and full_join().",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#assigning-results-of-manipulations",
    "href": "06-manipulation.html#assigning-results-of-manipulations",
    "title": "6  Data Manipulation",
    "section": "6.5 Assigning Results of Manipulations",
    "text": "6.5 Assigning Results of Manipulations\nWhen working with dplyr, it is often useful to save the results of a manipulation into a new object. This allows you to reuse the processed data without repeating all of the steps.\n\n\nCode\n# Filter flights on January 1st and arrange by departure delay\nflights_jan1 &lt;- flights |&gt;\nfilter(month == 1, day == 1) |&gt;\narrange(dep_delay)\n\n# Print the first few rows\nhead(flights_jan1)\n\n\n\n  \n\n\n\nYou can now work with flights_jan1 in later code chunks without re-running the entire manipulation. This practice is especially helpful for long workflows where the same processed data will be used multiple times.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#in-class-example",
    "href": "06-manipulation.html#in-class-example",
    "title": "6  Data Manipulation",
    "section": "6.6 In-Class Example",
    "text": "6.6 In-Class Example\nConsider the data of Chetty et al. (2014).\n\nCreate a data frame with CZs in CT, MA, and NY.\nCreate a data frame with CZ’s with absolute mobility of at least 40\nCreate a data frame with CZ’s in any state other than CT, MA, or NY, with absolute mobility at least 40\nCreate a data frame with CZ’s that are in CT, MA, NY and have absolute mobility less than 40.\nCreate a data frame with CZ’s that are in CT, MA, NY, sorting the CZ’s in decreasing order of absolute mobility, and keeping just the CZ name, state, and absolute mobility variables in the resulting data frames\nCreate a new data set with only the following variables: cz_name, state, pop_2000, abs_mobility, hhi_percap, and any variable that starts with frac.\nMake new variables for each of these quantities:\n\nThe number of people in each CZ who consider themselves to be religious.\nThe log base 2 of the per capita household income (hint: log2() ).\nThe proportion of people who are not married.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "06-manipulation.html#summary-and-best-practices",
    "href": "06-manipulation.html#summary-and-best-practices",
    "title": "6  Data Manipulation",
    "section": "6.7 Summary and Best Practices",
    "text": "6.7 Summary and Best Practices\n\nBegin with a clear idea of what manipulation you need.\n\nChain verbs together with the pipe operator for readability.\n\nUse group_by() and summarise() to move from raw detail to aggregated insights.\n\nReshape and join data as needed to bring it into tidy form.\n\nThese manipulations prepare your data for visualization and modeling, ensuring clarity and reproducibility in analysis.\n\n\n\n\nChetty, R., Hendren, N., Kline, P., & Saez, E. (2014). Where is the land of opportunity? The geography of intergenerational mobility in the United States. Quarterly Journal of Economics, 129(4), 1553–1623. https://doi.org/10.1093/qje/qju022",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data Manipulation</span>"
    ]
  },
  {
    "objectID": "07-eda.html",
    "href": "07-eda.html",
    "title": "7  Exploratory Data Analysis",
    "section": "",
    "text": "7.1 Questions to consider\nExploratory Data Analysis (EDA) is the practice of using graphs and numerical summaries to become familiar with a dataset before formal modeling. The term was popularized by John Tukey in his influential book Exploratory Data Analysis (Tukey, 1977), and it remains a cornerstone of modern data science. The purpose of EDA is to reveal the main characteristics of the data, detect unusual cases, check the quality of measurements, and generate hypotheses that can guide later modeling. Unlike confirmatory analysis, which tests specific hypotheses with formal statistical procedures, EDA is open ended and iterative. Analysts move back and forth between plotting, summarizing, and transforming data, refining their understanding at each step.\nFigure 7.1 shows the data science workflow in a compact layout, generated directly in R. Compared with embedding static images, this approach keeps your notes reproducible and editable. The flowchart emphasizes that exploratory data analysis sits between data transformation and modeling. It also highlights feedback loops: visualization can suggest new transformations, and communication can lead back to data collection.\nWe will use ggplot2::diamonds as a running example. It contains about fifty four thousand round‑cut diamonds with carat, cut, color, clarity, depth, table, price, and x/y/z dimensions. The dataset is big enough to be realistic while still rendering quickly in class.\nWhen beginning an EDA it is useful to ask two broad questions that frame all of the work. These questions remind us that analysis is about patterns, not isolated numbers.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-eda.html#questions-to-consider",
    "href": "07-eda.html#questions-to-consider",
    "title": "7  Exploratory Data Analysis",
    "section": "",
    "text": "What type of variation occurs within variables?\nVariation refers to the tendency of a variable to take different values from one observation to another. Even for simple variables like eye color or carat weight, no two individuals are exactly the same. Understanding the spread and shape of this variation is the first task of EDA.\nWhat type of covariation occurs between variables?\nCovariation describes the way two or more variables move together in a related fashion. For instance, diamond carat and price tend to increase together. Examining such relationships gives insight into possible explanations and later modeling choices.\n\n\n7.1.1 Useful terms\nClear vocabulary helps organize thinking during EDA.\n\nVariable. A measurable characteristic such as carat, cut, or price. Variables may be quantitative or qualitative.\nValue. The state of a variable when it is measured. Values can differ across observations; for example, two diamonds can have different carat weights.\nObservation. A set of measurements recorded on the same unit at the same time. In diamonds, each row is one diamond with values for all variables.\nTabular data. A collection of values arranged with variables in columns and observations in rows. Data is called tidy when each value occupies its own cell, making manipulation and visualization straightforward.\n\n\n\nCode\nlibrary(tidyverse)\nglimpse(diamonds)\n\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\n\n\n\n7.1.2 Variation\nEvery variable has its own pattern of variation, and these patterns often reveal important information. The best way to learn about the variation of a variable is to visualize its distribution.\n\nFor categorical variables, variation shows up in the relative frequencies of categories.\nFor quantitative variables, variation appears in the spread and shape of the distribution.\n\n\n\nCode\n# variation in cut (categorical)\ndiamonds |&gt;\n  count(cut) |&gt;\n  mutate(prop = n / sum(n))\n\n\n\n  \n\n\n\n\n\nCode\n# variation in carat (numeric)\nggplot(diamonds, aes(carat)) +\n  geom_histogram(binwidth = 0.1, boundary = 0, closed = \"left\") +\n  labs(title = \"Variation in carat\", x = \"Carat\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\n7.1.3 Visualizing distributions\nHow to visualize a distribution depends on the type of variable.\nCategorical variables take one of a small set of levels and are usually stored as factors or character vectors in R. Bar charts are a natural choice for displaying their distributions.\n\n\nCode\n    ggplot(diamonds, aes(cut)) +\n      geom_bar() +\n      labs(title = \"Distribution of cut\", x = NULL, y = \"Count\")\n\n\n\n\n\n\n\n\n\nQuantitative variables take on a wide range of values. Their distributions can be displayed in several ways:\n\nHistograms divide the range of the variable into bins and count how many observations fall into each bin. They are useful for seeing the overall shape of a distribution.\n\n\n\nCode\n    ggplot(diamonds, aes(price)) +\n      geom_histogram(binwidth = 500, boundary = 0, closed = \"left\") +\n      labs(title = \"Distribution of price (histogram)\",\n           x = \"Price (USD)\", y = \"Count\")\n\n\n\n\n\n\n\n\n\n\nBoxplots summarize the distribution using the median and quartiles.\n\nThe box spans the interquartile range (IQR), from the first quartile (Q1, 25th percentile) to the third quartile (Q3, 75th percentile).\nThe whiskers usually extend from the box out to the most extreme data points that are not considered outliers. A common rule is that the whiskers reach to the smallest and largest data points within 1.5 × IQR of Q1 and Q3.\nThe dots are outliers, data points beyond the whiskers (i.e., more than 1.5 × IQR away from Q1 or Q3).\n\n\n\n\nCode\n    ggplot(diamonds, aes(x = cut, y = price)) +\n      geom_boxplot() +\n      labs(title = \"Price by cut (boxplot)\", x = \"Cut\", y = \"Price (USD)\")\n\n\n\n\n\n\n\n\n\n\nViolin plots combine a boxplot with a rotated density curve, showing both summary statistics and the shape of the distribution.\n\n\n\nCode\n    ggplot(diamonds, aes(x = cut, y = price)) +\n      geom_violin(trim = FALSE) +\n      labs(title = \"Price by cut (violin plot)\", x = \"Cut\", y = \"Price (USD)\")\n\n\n\n\n\n\n\n\n\nThese three methods are closely related. A histogram provides a binned view of the data, while a density curve offers a smoothed version of the histogram. A violin plot takes that density curve, mirrors it to make the violin shape, and overlays summary statistics similar to a boxplot. Together they form a family of tools that balance detail, smoothing, and summary depending on the purpose of the visualization.\n\n\n7.1.4 Outliers\nOutliers are observations that fall far outside the general pattern of a distribution. They may represent data entry errors, unusual cases, or rare but valid observations. Outliers are important to detect because they can strongly influence summary statistics and models.\nThe diamonds dataset has some striking examples.\n\nHistogram of y (length)\n\n\n\nCode\nggplot(diamonds) +\ngeom_histogram(mapping = aes(x = y), binwidth = 0.5) +\nlabs(title = \"Distribution of diamond length (y)\",\nx = \"Length (mm)\", y = \"Count\")\n\n\n\n\n\n\n\n\n\nMost diamonds have lengths between 3 and 10 mm. However, there are odd spikes at 0, 30, 50, and 60 mm, which are not physically plausible. These are likely data recording errors.\n\n\nCode\nunusual &lt;- diamonds %&gt;% \n  filter(y &lt; 3 | y &gt; 20) %&gt;% \n  select(price, x, y, z) %&gt;%\n  arrange(y)\n\nunusual\n\n\n\n  \n\n\n\n\nThe y variable measures one of the three dimensions of these diamonds in mm. Diamonds can’t have a width of 0mm.\nMeasurements of 32mm and 59mm are implausible. Those diamonds are over an inch long, but don’t cost hundreds of thousands of dollars\n\nConfirmed invalid values could be replaced with NA, using the ifelse() function.\n\n\nCode\ndiamonds2 &lt;- diamonds %&gt;% \n  mutate(y = ifelse(y &lt; 3 | y &gt; 20, NA, y))\n\ndiamonds2 |&gt; filter(is.na(y))\n\n\n\n  \n\n\n\n\nHistogram of x (width)\n\n\n\nCode\nggplot(diamonds) +\ngeom_histogram(mapping = aes(x = x), binwidth = 0.5) +\nlabs(title = \"Distribution of diamond width (x)\",\nx = \"Width (mm)\", y = \"Count\")\n\n\n\n\n\n\n\n\n\nAgain, most values are in a reasonable range, but there are suspicious zeros and unusually large values.\n\nBoxplot of price by cut\n\n\n\nCode\nggplot(diamonds, aes(x = cut, y = price)) +\ngeom_boxplot() +\nlabs(title = \"Price by cut (boxplot)\", x = \"Cut\", y = \"Price (USD)\")\n\n\n\n\n\n\n\n\n\nThe boxplot reveals outliers as individual points beyond the whiskers. These highlight diamonds with unusually high or low prices relative to others of the same cut.\n\nScatterplot of carat vs. price\n\n\n\nCode\nggplot(diamonds, aes(x = carat, y = price)) +\ngeom_point(alpha = 0.3) +\nlabs(title = \"Price vs. Carat\", x = \"Carat\", y = \"Price (USD)\")\n\n\n\n\n\n\n\n\n\nMost diamonds follow a clear increasing trend. Outliers appear as points far above or below the main cloud — either unusually expensive small diamonds or unusually cheap large ones.\n\nBoxplot of z (depth) by cut\n\n\n\nCode\nggplot(diamonds, aes(x = cut, y = z)) +\ngeom_boxplot() +\nlabs(title = \"Depth (z) by cut\", x = \"Cut\", y = \"Depth (mm)\")\n\n\n\n\n\n\n\n\n\nHere too, implausible depths like 0 or 30+ mm appear as obvious outliers.\nThese examples illustrate different ways to detect outliers: - Histograms reveal spikes or extreme bars. - Boxplots show individual points beyond whiskers. - Scatterplots highlight deviations from a trend.\nTogether, they demonstrate that careful visualization is essential for spotting unusual data values before moving on to modeling.\n\n\n\n\n\n\nNote\n\n\n\nNote. Not every outlier should be removed. Some may hold the key to a new insight, while others may be errors worth correcting. Document any decisions you make.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-eda.html#conditional-statements",
    "href": "07-eda.html#conditional-statements",
    "title": "7  Exploratory Data Analysis",
    "section": "7.2 Conditional statements",
    "text": "7.2 Conditional statements\nSometimes, creating new variable relies on a complex combination of existing variables. Utilize the ifelse() or case_when() command.\n\n\nCode\ndiamonds %&gt;%\n  mutate(size_category = case_when(\n    carat &lt; 0.25 ~ \"tiny\",\n    carat &lt; 0.5  ~ \"small\",\n    carat &lt; 1    ~ \"medium\",\n    carat &lt; 1.5  ~ \"large\",\n    TRUE         ~ \"huge\"\n  )) %&gt;%\n  select(size_category, carat) %&gt;%\n  head()\n\n\n\n  \n\n\n\nMany insights arise from studying how a variable behaves given the level of another variable. These conditional relationships are the basis for understanding covariation.\n\nNumeric given categorical. Compare distributions of a numeric variable across categories.\nNumeric given numeric. Study scatterplots, possibly with facets, to see how the relationship changes across groups.\n\n\n\nCode\n# price conditional on cut\nggplot(diamonds, aes(cut, price)) +\n  geom_boxplot(outlier.alpha = 0.2) +\n  labs(title = \"Price distribution by cut\", x = NULL, y = \"Price (USD)\")\n\n\n\n\n\n\n\n\n\nCode\n# carat vs. price conditional on clarity\nset.seed(1)\nd_small &lt;- diamonds[sample(nrow(diamonds), 8000), ]\nggplot(d_small, aes(carat, price)) +\n  geom_point(alpha = 0.35) +\n  facet_wrap(~ clarity) +\n  labs(title = \"Price vs. carat by clarity\",\n       x = \"Carat\", y = \"Price (USD)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTip. Faceting is usually clearer than mapping too many aesthetics at once. Use small multiples to keep comparisons interpretable.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "07-eda.html#good-practices",
    "href": "07-eda.html#good-practices",
    "title": "7  Exploratory Data Analysis",
    "section": "7.3 Good Practices",
    "text": "7.3 Good Practices\n\nPrefer simple, readable graphics over clever ones.\nKeep code blocks short and name intermediate objects clearly.\nMix visual and tabular summaries; each catches different issues.\nRecord every data change. EDA often uncovers fixes that matter later.\nRevisit EDA after modeling; residuals are just another dataset.\n\n\n\n\n\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "08-tidy.html",
    "href": "08-tidy.html",
    "title": "8  Tidy Data",
    "section": "",
    "text": "8.1 Introduction\nWorking effectively with data in R is greatly simplified by the tidyverse, a collection of packages designed for data science. The tidyverse provides a consistent framework for data manipulation, visualization, and modeling, which helps students learn generalizable skills rather than package-specific tricks.\nA central concept in the tidyverse is thetibble, a modern re-imagining of the traditional R data frame. Tibbles keep the familiar two-dimensional table structure but introduce improvements such as preserving variable types, supporting list columns, and displaying data more cleanly in the console. These features make them easier to use in practice, especially with large datasets.\nFinally, the idea of tidy data lies at the heart of the tidyverse. According to Hadley Wickham’s definition, tidy data means each variable forms a column, each observation forms a row, and each type of observational unit forms a table. Tidy data creates a standardized structure that enables smooth use of functions across the tidyverse, reducing the need for ad hoc data reshaping and making analyses more transparent and reproducible.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "08-tidy.html#tibbles",
    "href": "08-tidy.html#tibbles",
    "title": "8  Tidy Data",
    "section": "8.2 Tibbles",
    "text": "8.2 Tibbles\n\nWhat is a tibble?\n\nTibbles are data frames\nBut they try and enhance the regular “old” data frame from base R\n\nTo learn more\n\nvignette(\"tibble\")\n\n\nTibbles are data frames that enhance the regular “old” data frame from base R. They keep the same two-dimensional tabular structure but are designed to be more consistent, predictable, and user-friendly.\n\n8.2.1 Creating tibbles\nThere are several ways to create tibbles depending on the source of the data.\n\nFrom individual vectors\n\nThe simplest way is to build a tibble directly from vectors using tibble(). Inputs of length 1 are automatically recycled, and you can refer to variables you just created:\n\n\nCode\ntibble(\n  x = 1:5,\n  y = 1,\n  z = x ^ 2 + y\n)\n\n\n\n  \n\n\n\n\nConverting existing objects\n\nYou can convert existing data structures into tibbles with as_tibble():\n\n\nCode\n# From a data frame\nhead(as_tibble(iris))\n\n\n\n  \n\n\n\nCode\n# From a list\nas_tibble(list(x = 1:3, y = letters[1:3]))\n\n\n\n  \n\n\n\n\nReading from external files\n\nPackages in the tidyverse ecosystem return tibbles when reading data from files:\n\n\nCode\n# From CSV, TSV, or delimited text file\nhead(readr::read_csv(\"data/Chetty_2014.csv\"))\n\n\n\n  \n\n\n\nCode\n# From Excel files\n# readxl::read_excel(\"data.xlsx\")\n\n\n\nReading from databases\n\nYou can also obtain tibbles when working with databases using packages such as DBI and dbplyr:\nlibrary(DBI)\ncon &lt;- dbConnect(RSQLite::SQLite(), \"mydb.sqlite\")\ntbl(con, \"tablename\")   # returns a tibble-like object\nA tribble is a transposed tibble, designed for small data entry in code. Column headings are defined by formulas that start with ~:\n\n\nCode\ntribble(\n  ~x, ~y, ~z,\n  #--|--|----\n  \"a\", 2, 3.6,\n  \"b\", 1, 8.5\n)\n\n\n\n  \n\n\n\nThe tibble and readr packages are part of the core tidyverse, so they load automatically with library(tidyverse). Other packages such as readxl and dbplyr belong to the tidyverse ecosystem. They follow the same principles and return tibbles, but you need to load them explicitly.\n\n\n8.2.2 Features of tibbles\nTibbles make fewer automatic changes than base R data frames:\n\nThey never change the type of inputs (strings are not converted to factors).\nThey never change variable names.\nThey never create row names.\n\nFor example:\n\n\nCode\ntb &lt;- tibble(\n  `:)` = \"smile\",\n  ` ` = \"space\",\n  `2000` = \"number\"\n)\n\n\nThese column names would not be valid in base R, but are allowed in a tibble.\nThere are two main differences between tibbles and base R data frames:\nPrinting Tibbles have a refined print method that shows only the first 10 rows and only as many columns as fit on the screen:\n\n\nCode\ntibble(\n  a = lubridate::now() + runif(1e3) * 86400,\n  b = lubridate::today() + runif(1e3) * 30,\n  c = 1:1e3,\n  d = runif(1e3),\n  e = sample(letters, 1e3, replace = TRUE)\n) |&gt; head()\n\n\n\n  \n\n\n\nThis design avoids overwhelming the console when printing large data frames.\nIf you need more output, you can adjust options:\n\nprint(n = , width = ) controls number of rows and columns.\nGlobal options can be set with:\n\noptions(tibble.print_max = n, tibble.print_min = m)\noptions(tibble.print_min = Inf)     # always show all rows\noptions(tibble.width = Inf)         # always print all columns\nSubsetting Most of the subsetting tools we have used so far generally subset the entire data frame. To pull out just a single variable or value, we can use $ and [[: - [[ extracts by name or position - $ extracts by name with less typing\n\n\nCode\ndf &lt;- tibble(\n  x = runif(5),\n  y = rnorm(5)\n)\n\n# Extract by name\ndf$x\n\n\n[1] 0.13537978 0.43237156 0.70282199 0.03161239 0.86934134\n\n\nCode\ndf[[\"x\"]]\n\n\n[1] 0.13537978 0.43237156 0.70282199 0.03161239 0.86934134\n\n\nCode\n# Extract by position\ndf[[1]]\n\n\n[1] 0.13537978 0.43237156 0.70282199 0.03161239 0.86934134\n\n\nCode\ndf[[1,1]]\n\n\n[1] 0.1353798",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "08-tidy.html#tidy-up-data",
    "href": "08-tidy.html#tidy-up-data",
    "title": "8  Tidy Data",
    "section": "8.3 Tidy up data",
    "text": "8.3 Tidy up data\nStructuring datasets to facilitate analysis is at the core of the principles of tidy data, as described by Hadley Wickham.\n\n8.3.1 Tydy data\nTidy data follows three basic rules:\n\nEach variable forms a column\nEach observation forms a row\nEach type of observational unit forms a table\n\nWhen these rules are not followed, the dataset is considered untidy. Common signs of untidy data include:\n\nColumn headers are values instead of variable names\nMultiple variables are stored in one column (for example, City_State)\nVariables are stored in both rows and columns\nMultiple types of observational units are stored in the same table\nA single observational unit is stored in multiple tables\nThe dataset is either too long or too wide\n\n\n\n8.3.2 Pivoting\nMost data encountered in practice will be untidy. This is because most people are not familiar with the principles of tidy data, and data is often organised to facilitate uses other than analysis, such as making entry easier.\nTwo common problems to look for are:\n\nOne variable might be spread across multiple columns\nOne observation might be scattered across multiple rows\n\nUsually, a dataset will only suffer from one of these problems.\nTo resolve them, the tidyr package provides two key functions:\n\npivot_longer()\npivot_wider()\n\nThese functions are illustrated with example datasets included in the tidyr package. The tables (table2, table4a) contain data on the number of tuberculosis (TB) cases recorded in different countries for the years 1999 and 2000. The variable cases represents the number of TB cases reported for a given country, year, and type of measure.\n\n8.3.2.1 Pivot longer\nA common problem is a dataset where some of the column names are not variable names, but values of a variable:\n\n\nCode\ntable4a\n\n\n\n  \n\n\n\nTo tidy a dataset like this, pivot the offending columns into a new pair of variables.\nSteps:\n\nSelect the columns whose names are values, not variables. In this example, those are 1999 and 2000.\nChoose the variable to move the column names to (here, year).\nChoose the variable to move the column values to (here, cases).\n\n\n\nCode\ntable4a |&gt; \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\n\n\n  \n\n\n\nIn the final result, the pivoted columns are dropped, and new year and cases columns are created. Other variables, such as country, are preserved. The cases column now explicitly records the number of TB cases for each year and country.\n\n\n\n8.3.3 Pivot wider\nFunction pivot_wider() is the opposite of pivot_longer(). You use it when an observation is scattered across multiple rows.\n\n\nCode\ntable2\n\n\n\n  \n\n\n\nTo tidy this up, analyze the representation in a similar way to pivot_longer():\n\nThe column to take variable names from (here, type).\nThe column to take values from (here, count).\n\n\n\nCode\ntable2 |&gt;\n  pivot_wider(names_from = type, values_from = count)\n\n\n\n  \n\n\n\nIn this result, values of type (cases and population) become separate columns, and their associated numbers from count fill in the values. This produces a clearer dataset where each row corresponds to a country and year with distinct variables for cases and population.\n\n\n8.3.4 Separating\nThe separate() function is used to pull apart one column into multiple columns by splitting wherever a separator character appears. This is useful when a single column actually contains more than one variable.\nConsider the dataset table3 included in the tidyr package:\n\n\nCode\ntable3\n\n\n\n  \n\n\n\nNotice the rate column. It contains two variables combined into a single column: the number of cases and the population size, separated by a forward slash. To make the dataset tidy, these should be split into separate variables.\nThe separate() function takes the name of the column to split and the names of the new columns to create:\n\n\nCode\ntable3 %&gt;%\nseparate(rate, into = c(\"cases\", \"population\"))\n\n\n\n  \n\n\n\nThis produces two new columns, cases and population, replacing the original rate column. The new columns now contain integer values for the reported tuberculosis cases and the population in each country and year.\nBy default, separate() splits values wherever it sees a non-alphanumeric character, meaning any character that is not a number or letter. In the example above, it automatically detected and split at the forward slash.\nIf you want to be explicit, you can specify the character to split on with the sep argument:\n\n\nCode\ntable3 %&gt;%\nseparate(rate, into = c(\"cases\", \"population\"), sep = \"/\")\n\n\n\n  \n\n\n\nThis ensures that the column is split exactly where expected, giving you clearer control over the separation process.\n\n\n8.3.5 Unite\nThe unite() function is the inverse of separate(). It combines multiple columns into a single column. This can be useful when two or more variables are stored in separate columns but logically belong together.\nConsider the dataset table5:\n\n\nCode\ntable5\n\n\n\n  \n\n\n\nIn this table, the year of observation is split into two columns, century and year. To make the dataset easier to work with, we can combine these into a single column.\n\n\nCode\ntable5 %&gt;% \n  unite(new, century, year)\n\n\n\n  \n\n\n\nBy default, unite() places an underscore (_) between the values from different columns. In this case, that would produce values like 19_99.\nIf we want the numbers to run together without any separator, we can control this with the sep argument:\n\n\nCode\ntable5 %&gt;% \n  unite(new, century, year, sep = \"\")\n\n\n\n  \n\n\n\nThis produces a single column new with values such as 1999 and 2000, giving a cleaner representation of the year variable.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "08-tidy.html#relational-data",
    "href": "08-tidy.html#relational-data",
    "title": "8  Tidy Data",
    "section": "8.4 Relational Data",
    "text": "8.4 Relational Data\nMany datasets consist of multiple tables that are connected in a meaningful way. Together, such collections are called relational data. Each table stores information about a particular type of observation, and the relationships among these tables allow us to draw richer conclusions.\nWhy not store everything in one giant table? Because different kinds of observations naturally belong in different tables. For example, aircraft information does not change across flights, and weather data apply to all flights departing at a specific time from the same airport.\n\n\n\n\n\n\nThink: Storage Efficiency and Consistency\n\n\n\nKeeping separate tables avoids duplication, reduces storage, and prevents inconsistencies when information changes. If the manufacturer name of a plane changes, updating one record in the planes table is enough, rather than updating thousands of flight records.\n\n\n\n8.4.1 Introduction\nWe use the nycflights13 package as an example again to illustrate. This package includes several tables describing all flights departing New York City in 2013.\n\n\nCode\nlibrary(nycflights13)\nflights %&gt;% select(year:day, hour, origin, dest, tailnum, carrier) %&gt;% head()\n\n\n\n  \n\n\n\nCode\nairlines %&gt;% head()\n\n\n\n  \n\n\n\nCode\nplanes %&gt;% select(tailnum, manufacturer, model, year) %&gt;% head()\n\n\n\n  \n\n\n\nCode\nairports %&gt;% select(faa, name, lat, lon) %&gt;% head()\n\n\n\n  \n\n\n\nCode\nweather %&gt;% select(origin, year:day, hour, temp, wind_speed) %&gt;% head()\n\n\n\n  \n\n\n\nEach table contains different but related information:\n\nflights records each departure, including date, time, origin, destination, tail number, and carrier code.\nairlines provides the full airline names corresponding to carrier codes.\nairports gives the name and geographic location for each airport.\nplanes contains aircraft details such as manufacturer and model.\nweather records hourly weather data for each origin airport.\n\nThese tables are linked by shared variables. For example:\n\ncarrier links flights to airlines.\ntailnum links flights to planes.\norigin and dest link flights to airports.\nyear, month, day, hour, and origin link flights to weather.\n\nA natural question to ask is: how can we combine information across these tables to answer questions such as:\n\nWhat type of aircraft fly most often from JFK?\nWhich airlines experience the longest delays?\nAre certain weather conditions associated with longer delays?\n\nTo answer these questions, we must understand keys and joins.\n\n\n8.4.2 Keys and Relationships\nA key identifies how observations in one table relate to those in another.\n\nA primary key uniquely identifies each observation within a table.\nA foreign key refers to a primary key in another table.\n\nFor example, in planes, the variable tailnum serves as a primary key because each aircraft has a unique tail number. In flights, the same variable acts as a foreign key since many flights can share the same aircraft.\nWe can verify whether a variable is a primary key by checking for uniqueness:\n\n\nCode\nplanes %&gt;% count(tailnum) %&gt;% filter(n &gt; 1)\n\n\n\n  \n\n\n\nNo duplicates imply that tailnum is a valid primary key. We can also check whether every tailnum in flights appears in planes once we learn about filtering joins below.\n\n\n\n\n\n\nThink: Diagnosing Key Issues\n\n\n\nIt’s common for foreign keys to have missing matches. This may happen when aircraft were retired, renamed, or missing from the record. Always check unmatched keys before joining tables.\n\n\n\n\n8.4.3 Mutating Joins\nMutating joins combine variables from two tables based on matching key values. They allow us to enrich one dataset with information from another.\nThere are four types of mutating Joins:\n\ninner_join(x, y): keeps only rows with matching keys in both tables.\nleft_join(x, y): keeps all rows from x, adding matches from y.\nright_join(x, y): keeps all rows from y, adding matches from x.\nfull_join(x, y): keeps all rows from both tables.\n\n\n\nCode\nflights2 &lt;- flights %&gt;% select(year:day, hour, origin, dest, tailnum, carrier)\n\nresult &lt;- flights2 %&gt;% left_join(planes, by = \"tailnum\")\n\n## compare dimensions\nnrow(flights2)\n\n\n[1] 336776\n\n\nCode\nnrow(result)\n\n\n[1] 336776\n\n\nThe number of rows in the result matches flights2 because left_join() keeps all flights, even those without matching tailnum records. Missing values appear for unmatched aircraft.\n\n\n\n\n\n\nThink: Choosing the Right Join\n\n\n\nleft_join() is most common because it preserves the main dataset. Use inner_join() only when you are sure you want to exclude records that do not match.\n\n\n\n\n8.4.4 Filtering Joins\nFiltering joins keep or exclude observations in one table based on whether they have a match in another. They do not add new columns.\n\nsemi_join(x, y): keeps rows in x that have a match in y.\nanti_join(x, y): keeps rows in x that do not have a match in y.\n\nExample: Flights with Known Planes\n\n\nCode\nflights2 %&gt;% semi_join(planes, by = \"tailnum\") %&gt;% nrow()\n\n\n[1] 284170\n\n\nThis keeps only flights that have aircraft information in planes.\nExample: Flights with Unknown Planes\n\n\nCode\nflights2 %&gt;% anti_join(planes, by = \"tailnum\") %&gt;% nrow()\n\n\n[1] 52606\n\n\nThese flights are missing aircraft details. anti_join() is especially useful for finding unmatched keys and diagnosing data quality issues. For instance, you might use it to identify which flights are missing weather information or which airports have no corresponding records.\nThis is how we check how many flights lack corresponding aircraft information\n\n\nCode\nflights %&gt;% anti_join(planes, by = \"tailnum\") %&gt;% nrow()\n\n\n[1] 52606\n\n\n\n\n\n\n\n\nThink: Using semi_join() and anti_join()\n\n\n\nUse semi_join() to filter data based on membership, and anti_join() to find mismatches. They are efficient tools for quality control and exploratory checks.\n\n\n\n\n8.4.5 Summary\nRelational data represent multiple connected tables describing related entities. Keeping separate tables helps minimize redundancy, reduce errors, and maintain data consistency. Understanding how these tables are linked through keys allows us to integrate information accurately. The tidyverse join functions make it straightforward to enrich, filter, and explore relational datasets such as the nycflights13 example.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "08-tidy.html#strings",
    "href": "08-tidy.html#strings",
    "title": "8  Tidy Data",
    "section": "8.5 Strings",
    "text": "8.5 Strings\nText data are common in real datasets. In the NYC flights example, variables such as airline names, airport codes, or weather descriptions are stored as strings. Because text data often includes noise, inconsistent capitalization, or embedded structure (like “city–state”), knowing how how to inspect, clean, and manipulate string variables is essential before analysis. The stringr package from the tidyverse offers consistent, easy-to-remember functions for string operations.\n\n8.5.1 Vectorized String Operations\nString functions in R are vectorized, meaning they act on each element of a vector. Let us look at examples using the airline names from the flights data.\n\n\nCode\nairlines$name\n\n\n [1] \"Endeavor Air Inc.\"           \"American Airlines Inc.\"     \n [3] \"Alaska Airlines Inc.\"        \"JetBlue Airways\"            \n [5] \"Delta Air Lines Inc.\"        \"ExpressJet Airlines Inc.\"   \n [7] \"Frontier Airlines Inc.\"      \"AirTran Airways Corporation\"\n [9] \"Hawaiian Airlines Inc.\"      \"Envoy Air\"                  \n[11] \"SkyWest Airlines Inc.\"       \"United Air Lines Inc.\"      \n[13] \"US Airways Inc.\"             \"Virgin America\"             \n[15] \"Southwest Airlines Co.\"      \"Mesa Airlines Inc.\"         \n\n\nCode\nstr_length(airlines$name)\n\n\n [1] 17 22 20 15 20 24 22 27 22  9 21 21 15 14 22 18\n\n\nThe function str_length() returns the number of characters in each element. Vectorization allows us to compute results for all airlines at once.\nWe can combine strings with str_c().\n\n\nCode\nstr_c(\"Flight to\", airlines$name)\n\n\n [1] \"Flight toEndeavor Air Inc.\"          \n [2] \"Flight toAmerican Airlines Inc.\"     \n [3] \"Flight toAlaska Airlines Inc.\"       \n [4] \"Flight toJetBlue Airways\"            \n [5] \"Flight toDelta Air Lines Inc.\"       \n [6] \"Flight toExpressJet Airlines Inc.\"   \n [7] \"Flight toFrontier Airlines Inc.\"     \n [8] \"Flight toAirTran Airways Corporation\"\n [9] \"Flight toHawaiian Airlines Inc.\"     \n[10] \"Flight toEnvoy Air\"                  \n[11] \"Flight toSkyWest Airlines Inc.\"      \n[12] \"Flight toUnited Air Lines Inc.\"      \n[13] \"Flight toUS Airways Inc.\"            \n[14] \"Flight toVirgin America\"             \n[15] \"Flight toSouthwest Airlines Co.\"     \n[16] \"Flight toMesa Airlines Inc.\"         \n\n\nThis is not exactly what we wanted, because the default sep = \"\". We should add sep = \" \".\nWe can extract substrings with str_sub().\n\n\nCode\nstr_sub(airlines$name, 1, 3)\n\n\n [1] \"End\" \"Ame\" \"Ala\" \"Jet\" \"Del\" \"Exp\" \"Fro\" \"Air\" \"Haw\" \"Env\" \"Sky\" \"Uni\"\n[13] \"US \" \"Vir\" \"Sou\" \"Mes\"\n\n\nSimply put, str_c() joins elements with an optional separator. str_sub() extracts a substring between given positions. Both are vectorized.\n\n\n8.5.2 Pattern Detection and Replacement\nPattern matching is powerful for cleaning string variables. Suppose we want to know which airlines have names containing the word Air.\n\n\nCode\nstr_detect(airlines$name, \"Air\")\n\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[13]  TRUE FALSE  TRUE  TRUE\n\n\nCode\nstr_subset(airlines$name, \"Air\")\n\n\n [1] \"Endeavor Air Inc.\"           \"American Airlines Inc.\"     \n [3] \"Alaska Airlines Inc.\"        \"JetBlue Airways\"            \n [5] \"Delta Air Lines Inc.\"        \"ExpressJet Airlines Inc.\"   \n [7] \"Frontier Airlines Inc.\"      \"AirTran Airways Corporation\"\n [9] \"Hawaiian Airlines Inc.\"      \"Envoy Air\"                  \n[11] \"SkyWest Airlines Inc.\"       \"United Air Lines Inc.\"      \n[13] \"US Airways Inc.\"             \"Southwest Airlines Co.\"     \n[15] \"Mesa Airlines Inc.\"         \n\n\nTo replace part of a string, use str_replace():\n\n\nCode\nstr_replace(airlines$name, \"Air\", \"Sky\")\n\n\n [1] \"Endeavor Sky Inc.\"           \"American Skylines Inc.\"     \n [3] \"Alaska Skylines Inc.\"        \"JetBlue Skyways\"            \n [5] \"Delta Sky Lines Inc.\"        \"ExpressJet Skylines Inc.\"   \n [7] \"Frontier Skylines Inc.\"      \"SkyTran Airways Corporation\"\n [9] \"Hawaiian Skylines Inc.\"      \"Envoy Sky\"                  \n[11] \"SkyWest Skylines Inc.\"       \"United Sky Lines Inc.\"      \n[13] \"US Skyways Inc.\"             \"Virgin America\"             \n[15] \"Southwest Skylines Co.\"      \"Mesa Skylines Inc.\"         \n\n\nNote the differences + str_replace() replaces only the first occurrence of a pattern in each string. + str_replace_all() replaces all occurrences of the pattern in each string.\nRegular expressions can be used for flexible pattern specification. The following example finds airline names ending with Lines:\n\n\nCode\nstr_subset(airlines$name, \"Air$\")\n\n\n[1] \"Envoy Air\"\n\n\n\n\n8.5.3 Splitting and Joining Strings\nWhen a variable contains multiple pieces of information in one column, we can split it into parts. For instance, separate origin and destination in a route.\n\n\nCode\nroutes &lt;- flights %&gt;%\n  mutate(route = str_c(origin, dest, sep = \"-&gt;\")) %&gt;%\n  select(route)\nhead(routes$route)\n\n\n[1] \"EWR-&gt;IAH\" \"LGA-&gt;IAH\" \"JFK-&gt;MIA\" \"JFK-&gt;BQN\" \"LGA-&gt;ATL\" \"EWR-&gt;ORD\"\n\n\nCode\nstr_split(head(routes$route), \"-&gt;\")\n\n\n[[1]]\n[1] \"EWR\" \"IAH\"\n\n[[2]]\n[1] \"LGA\" \"IAH\"\n\n[[3]]\n[1] \"JFK\" \"MIA\"\n\n[[4]]\n[1] \"JFK\" \"BQN\"\n\n[[5]]\n[1] \"LGA\" \"ATL\"\n\n[[6]]\n[1] \"EWR\" \"ORD\"\n\n\nFunction str_split() returns a list because each element may contain a different number of splits. Use str_c() or str_flatten() to join pieces again.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "08-tidy.html#factors",
    "href": "08-tidy.html#factors",
    "title": "8  Tidy Data",
    "section": "8.6 Factors",
    "text": "8.6 Factors\nString variables often represent categorical information, such as airline names, weather conditions, or airport codes. In R, categorical data are better stored as factors, which record the unique categories (called levels) and store the data as integer codes internally. The forcats package provide utilities to handle such tasks.\n\n\nCode\nlibrary(forcats)\nflights$carrier %&gt;% head()\n\n\n[1] \"UA\" \"UA\" \"AA\" \"B6\" \"DL\" \"UA\"\n\n\nThe carrier variable is currently a string. We can convert it to a factor using as_factor() or factor().\n\n\nCode\nflights &lt;- flights %&gt;% mutate(carrier = as_factor(carrier))\nlevels(flights$carrier)\n\n\n [1] \"UA\" \"AA\" \"B6\" \"DL\" \"EV\" \"MQ\" \"US\" \"WN\" \"VX\" \"FL\" \"AS\" \"9E\" \"F9\" \"HA\" \"YV\"\n[16] \"OO\"\n\n\nEach unique airline code becomes a level. Internally, R stores these levels as integers, which saves memory and allows efficient grouping.\n\n8.6.1 Ordering Factors\nSometimes categories have a natural order, such as flight status (early, on time, late). Ordered factors preserve that ordering for display and modeling.\n\n\nCode\nstatus &lt;- factor(c(\"on time\", \"late\", \"early\"),\n                 levels = c(\"early\", \"on time\", \"late\"),\n                 ordered = TRUE)\nstatus\n\n\n[1] on time late    early  \nLevels: early &lt; on time &lt; late\n\n\n\n\n8.6.2 Relabeling Factors\nWe often need to rename levels for clarity. The forcats package provides functions like fct_recode() and fct_collapse().\n\n\nCode\nflights &lt;- flights %&gt;%\n  mutate(carrier = fct_recode(carrier,\n    \"United\" = \"UA\",\n    \"Delta\" = \"DL\",\n    \"American\" = \"AA\",\n    \"Southwest\" = \"WN\"\n  ))\nlevels(flights$carrier)\n\n\n [1] \"United\"    \"American\"  \"B6\"        \"Delta\"     \"EV\"        \"MQ\"       \n [7] \"US\"        \"Southwest\" \"VX\"        \"FL\"        \"AS\"        \"9E\"       \n[13] \"F9\"        \"HA\"        \"YV\"        \"OO\"       \n\n\n\n\n8.6.3 Frequency and Reordering\nFactors can be reordered to reflect frequencies or another variable. For instance, we can order airlines by the number of flights.\n\n\nCode\nflights %&gt;%\n  count(carrier) %&gt;%\n  mutate(carrier = fct_reorder(carrier, n)) %&gt;%\n  ggplot(aes(x = carrier, y = n)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\nThis visualization shows the most frequent carriers at the top. The fct_reorder() function makes the plot easier to interpret.\n\n\n8.6.4 Dropping Unused Levels\nIf we filter a dataset, some factor levels may no longer appear. We can remove them using fct_drop().\n\n\nCode\nflights %&gt;%\n  filter(carrier %in% c(\"Delta\", \"United\")) %&gt;%\n  mutate(carrier = fct_drop(carrier)) %&gt;%\n  count(carrier)\n\n\n\n  \n\n\n\nProper factor handling ensures consistent grouping and clean visualization.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "08-tidy.html#datetime",
    "href": "08-tidy.html#datetime",
    "title": "8  Tidy Data",
    "section": "8.7 Date/Time",
    "text": "8.7 Date/Time\nFlight departure and arrival times are examples of temporal data. The lubridate package makes it easier to parse and manipulate date-time values.\n\n8.7.1 Parsing Date and Time\nPackage lubridate automatically recognizes common formats. Depending on whether the string starts with year, month, or day, we use different functions.\n\n\nCode\nymd(\"2025-10-07\")\n\n\n[1] \"2025-10-07\"\n\n\nCode\nmdy(\"October 7, 2025\")\n\n\n[1] \"2025-10-07\"\n\n\nCode\ndmy(\"07-10-2025\")\n\n\n[1] \"2025-10-07\"\n\n\nThese functions convert text to proper Date objects and eliminate format ambiguities.\nThe flights data include year, month, and day variables. The function make_date() combines them into a single Date column.\n\n\nCode\nflights &lt;- flights %&gt;% mutate(fdate = make_date(year, month, day))\nflights %&gt;% select(year, month, day, fdate) %&gt;% head()\n\n\n\n  \n\n\n\nFor date-times that include hours and minutes, make_datetime() is used:\n\n\nCode\nflights &lt;- flights %&gt;%\n    mutate(dep_dt = make_datetime(year, month, day,\n                                  dep_time %/% 100, dep_time %% 100))\nflights %&gt;% select(dep_time, dep_dt) %&gt;% head()\n\n\n\n  \n\n\n\n\n\n8.7.2 Extracting Components\nOnce a date-time object is created, components such as year, month, and hour can be extracted easily.\n\n\nCode\ndate_example &lt;- ymd_hms(\"2025-10-07 14:30:00\")\nyear(date_example)\n\n\n[1] 2025\n\n\nCode\nmonth(date_example, label = TRUE)\n\n\n[1] Oct\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\n\nCode\nday(date_example)\n\n\n[1] 7\n\n\nCode\nhour(date_example)\n\n\n[1] 14\n\n\n\n\n8.7.3 Date Arithmetic\nArithmetic operations work naturally on date-time objects.\n\n\nCode\ndate_example + days(10)\n\n\n[1] \"2025-10-17 14:30:00 UTC\"\n\n\nCode\ndate_example - hours(5)\n\n\n[1] \"2025-10-07 09:30:00 UTC\"\n\n\nDelays between departure and arrival can be computed by subtraction:\n\n\nCode\nflights %&gt;%\n    mutate(arr_dt = make_datetime(year, month, day,\n                                  arr_time %/% 100, arr_time %% 100),\n           delay_mins = as.numeric(difftime(arr_dt, dep_dt,\n                                            units = \"mins\"))) %&gt;%\n    select(dep_dt, arr_dt, delay_mins) %&gt;% head()\n\n\n\n  \n\n\n\n\n\n8.7.4 Example: Average Delay by Hour\nWe can examine how departure delays vary by hour of the day.\n\n\nCode\nflights %&gt;%\n    mutate(dep_dt = make_datetime(year, month, day,\n                                  dep_time %/% 100, dep_time %% 100)) %&gt;%\n    group_by(hour = hour(dep_dt)) %&gt;%\n    summarize(mean_delay = mean(dep_delay, na.rm = TRUE)) %&gt;%\n    arrange(hour)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "08-tidy.html#case-study-nyc-snow-extremes",
    "href": "08-tidy.html#case-study-nyc-snow-extremes",
    "title": "8  Tidy Data",
    "section": "8.8 Case Study: NYC Snow Extremes",
    "text": "8.8 Case Study: NYC Snow Extremes\nLee & Lee (2020) investigate the snow storms in NYC with extreme value theory, quantifying linear trends in extreme snowfall and assess how severe the 2016 blizzard is in terms of return levels. The authors have nicely published their data and R code in their online supplementary material, which facilitates reproducibility.\n\nFind the data source and download the csv file. Let’s focus on the Central Park Station.\nImport the data; read the product info; and process it.\nPlot the time series.\nGet another station’s data.\nHow should we store the data from multiple sites.\n\n\n\n\n\nLee, M., & Lee, J. (2020). Trend and return level of extreme snow events in New York City. The American Statistician, 74(3), 282–293. https://doi.org/10.1080/00031305.2019.1592780",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tidy Data</span>"
    ]
  },
  {
    "objectID": "09-geospat.html",
    "href": "09-geospat.html",
    "title": "9  Geospatial Data",
    "section": "",
    "text": "9.1 Introduction\nOutline:\nCode\n## install.packages(\"leaflet\")\nlibrary(leaflet)\n\npopup = c(\"Storrs, Connecticut\")\nleaflet() %&gt;% \n  setView(lng = -72.251194, lat = 41.807417, zoom = 1)%&gt;%\n  addProviderTiles(\"NASAGIBS.ViirsEarthAtNight2012\") %&gt;%\n    addMarkers(lng = c(-72.251194),\n             lat = c(41.807417), \n             popup = popup)\nCode\npopup = c(\"SSH Building\")\nleaflet() %&gt;% \n  setView(lng = -72.251194, lat = 41.807417, zoom = 12)%&gt;%\n  addTiles() %&gt;%\n  addMarkers(lng = c(-72.251194),\n             lat = c(41.807417), \n             popup = popup)\nThere are many ways to handle geographic data in R, with dozens of packages in the area. The ones that we will focus on in this course are:\nHonorable mentions:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "09-geospat.html#introduction",
    "href": "09-geospat.html#introduction",
    "title": "9  Geospatial Data",
    "section": "",
    "text": "Geographic data corresponds to data that have been linked to a geographic coordinate system\n\nPoint-referenced: e.g., latitude and longitude\nAreal: polygons; grid; counties/states\n\nWorking with geographically located data is a relatively new phenomenon\n\nIt requires a significant amount of computing power and specialized programs\n\nNowadays such lack of geographic data is hard to imagine\n\nEvery smartphone has a global positioning (GPS) receiver and a multitude of sensors on devices ranging from satellites and semi-autonomous vehicles to citizen scientists incessantly measure every part of the world\n\n\n\n\n\n\nsp\nsf (very functional with the tidyverse)\nraster\n\n\n\nmaptools\nrgdal",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "09-geospat.html#the-sp-package",
    "href": "09-geospat.html#the-sp-package",
    "title": "9  Geospatial Data",
    "section": "9.2 The sp Package",
    "text": "9.2 The sp Package\nThe sp package provides the original infrastructure for handling spatial data in R. It defines formal S4 classes for representing geographic features—points, lines, polygons, and grids—together with their coordinate reference systems (CRS) and attribute data. These structures form the foundation for spatial analysis, visualization, and conversion between coordinate systems.\nAlthough newer packages like sf now offer simpler data-frame-based representations, sp remains widely used and is essential for understanding many legacy R spatial workflows.\n\nThe first general package to provide classes and methods for spatial data types that was developed for R is called sp.\nThe package provides classes and methods to create points, lines, polygons, and grids and to operate on them.\n\n\n9.2.1 Building a sp Object\nLet’s build a sp object:\n\n\nCode\n#install.packages(\"sp\")\nlibrary(sp)\n\n#create the highways \nln1 &lt;- Line(matrix(runif(6), ncol=2))\nln1\n\n\nAn object of class \"Line\"\nSlot \"coords\":\n          [,1]      [,2]\n[1,] 0.7516241 0.9845985\n[2,] 0.2691157 0.3526290\n[3,] 0.4781289 0.1047234\n\n\nCode\nln2 &lt;- Line(matrix(runif(6), ncol=2))\nln2\n\n\nAn object of class \"Line\"\nSlot \"coords\":\n          [,1]      [,2]\n[1,] 0.3596777 0.6510594\n[2,] 0.1847764 0.3938640\n[3,] 0.7886446 0.2952846\n\n\nCode\nstr(ln1)\n\n\nFormal class 'Line' [package \"sp\"] with 1 slot\n  ..@ coords: num [1:3, 1:2] 0.752 0.269 0.478 0.985 0.353 ...\n\n\nNote that the @coords slot holds the coordinates\n\n\nCode\n## combine the highways to a Lines object\nlns1 &lt;- Lines(list(ln1), ID = c(\"hwy1\")) \nlns2 &lt;- Lines(list(ln2), ID = c(\"hwy2\")) \n\nstr(lns1)\n\n\nFormal class 'Lines' [package \"sp\"] with 2 slots\n  ..@ Lines:List of 1\n  .. ..$ :Formal class 'Line' [package \"sp\"] with 1 slot\n  .. .. .. ..@ coords: num [1:3, 1:2] 0.752 0.269 0.478 0.985 0.353 ...\n  ..@ ID   : chr \"hwy1\"\n\n\nThe Line objects are now in a list and we have an additional ID slot, which uniquely identifies each Line object .\n\n\nCode\n# create a geospatial object with SpatialLines\nsp_lns &lt;- SpatialLines(list(lns1, lns2))\n\nstr(sp_lns)\n\n\nFormal class 'SpatialLines' [package \"sp\"] with 3 slots\n  ..@ lines      :List of 2\n  .. ..$ :Formal class 'Lines' [package \"sp\"] with 2 slots\n  .. .. .. ..@ Lines:List of 1\n  .. .. .. .. ..$ :Formal class 'Line' [package \"sp\"] with 1 slot\n  .. .. .. .. .. .. ..@ coords: num [1:3, 1:2] 0.752 0.269 0.478 0.985 0.353 ...\n  .. .. .. ..@ ID   : chr \"hwy1\"\n  .. ..$ :Formal class 'Lines' [package \"sp\"] with 2 slots\n  .. .. .. ..@ Lines:List of 1\n  .. .. .. .. ..$ :Formal class 'Line' [package \"sp\"] with 1 slot\n  .. .. .. .. .. .. ..@ coords: num [1:3, 1:2] 0.36 0.185 0.789 0.651 0.394 ...\n  .. .. .. ..@ ID   : chr \"hwy2\"\n  ..@ bbox       : num [1:2, 1:2] 0.185 0.105 0.789 0.985\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:2] \"x\" \"y\"\n  .. .. ..$ : chr [1:2] \"min\" \"max\"\n  ..@ proj4string:Formal class 'CRS' [package \"sp\"] with 1 slot\n  .. .. ..@ projargs: chr NA\n\n\nA data frame can be created with a column of SpatialLines.\n\n\nCode\n## create a SpatialLinesDataframe\ndfr &lt;- data.frame(id = c(\"hwy1\", \"hwy2\"), # note how we use the same IDs from above!\n                  cars_per_hour = c(78, 22)) \n\nsp_lns_dfr &lt;- SpatialLinesDataFrame(sp_lns, dfr, match.ID = \"id\")\n\nstr(sp_lns_dfr)\n\n\nFormal class 'SpatialLinesDataFrame' [package \"sp\"] with 4 slots\n  ..@ data       :'data.frame': 2 obs. of  2 variables:\n  .. ..$ id           : chr [1:2] \"hwy1\" \"hwy2\"\n  .. ..$ cars_per_hour: num [1:2] 78 22\n  ..@ lines      :List of 2\n  .. ..$ :Formal class 'Lines' [package \"sp\"] with 2 slots\n  .. .. .. ..@ Lines:List of 1\n  .. .. .. .. ..$ :Formal class 'Line' [package \"sp\"] with 1 slot\n  .. .. .. .. .. .. ..@ coords: num [1:3, 1:2] 0.752 0.269 0.478 0.985 0.353 ...\n  .. .. .. ..@ ID   : chr \"hwy1\"\n  .. ..$ :Formal class 'Lines' [package \"sp\"] with 2 slots\n  .. .. .. ..@ Lines:List of 1\n  .. .. .. .. ..$ :Formal class 'Line' [package \"sp\"] with 1 slot\n  .. .. .. .. .. .. ..@ coords: num [1:3, 1:2] 0.36 0.185 0.789 0.651 0.394 ...\n  .. .. .. ..@ ID   : chr \"hwy2\"\n  ..@ bbox       : num [1:2, 1:2] 0.185 0.105 0.789 0.985\n  .. ..- attr(*, \"dimnames\")=List of 2\n  .. .. ..$ : chr [1:2] \"x\" \"y\"\n  .. .. ..$ : chr [1:2] \"min\" \"max\"\n  ..@ proj4string:Formal class 'CRS' [package \"sp\"] with 1 slot\n  .. .. ..@ projargs: chr NA\n\n\nCode\nspplot(sp_lns_dfr, \"cars_per_hour\")\n\n\n\n\n\n\n\n\n\n\n\n9.2.2 Useful things to know in sp\n\nSpatial object — Any data object with geographic coordinates\n(points, lines, polygons, or grids).\nCRS (Coordinate Reference System) — Describes how coordinates map to Earth. Example:\nCRS(\"+proj=longlat +datum=WGS84\").\nproj4string — Text form of CRS stored in an sp object.\n@data — Attribute table (like columns in a data frame).\nAccess with obj@data.\n@coords — Coordinate matrix of locations.\nbbox — Bounding box (spatial extent) of the object.\nspTransform() — Converts coordinates to another CRS.\nover() — Spatial join (find which polygon a point belongs to).\n\n\nTip: The modern package sf replaces sp and uses a simpler data frame structure with a geometry column.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "09-geospat.html#the-sf-package",
    "href": "09-geospat.html#the-sf-package",
    "title": "9  Geospatial Data",
    "section": "9.3 The sf Package",
    "text": "9.3 The sf Package\n\nsf implements a formal standard called “Simple Features” that specifies a storage and access model of spatial geometries (point, line, polygon).\nA feature geometry is called simple when it consists of points connected by straight line pieces, and does not intersect itself.\n\nThis standard has been adopted widely.\n\nData are structured and conceptualized very differently from the sp approach.\n\nLet’s recreate the sp object from above in sf\n\n\nCode\n## install.packages(\"sf\")\nlibrary(sf)\n\n## create the highways \nlnstr_sfg1 &lt;- st_linestring(matrix(runif(6), ncol=2)) \nlnstr_sfg1\n\nlnstr_sfg2 &lt;- st_linestring(matrix(runif(6), ncol=2)) \nlnstr_sfg2\n\nclass(lnstr_sfg1)\n\n\n[1] \"XY\"         \"LINESTRING\" \"sfg\"       \n\n\nWe would then combine this into a simple feature collection.\n\n\nCode\n#combine the highways into a simple feature\nlnstr_sfc &lt;- st_sfc(lnstr_sfg1, lnstr_sfg2) # just one feature here\nlnstr_sfc\n\n\nGeometry set for 2 features \nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 0.2430178 ymin: 0.251209 xmax: 0.9703125 ymax: 0.8376872\nCRS:           NA\n\n\n\n\nCode\n#create our spatial data frame\nlnstr_sf &lt;- st_sf(dfr , lnstr_sfc)\nlnstr_sf\n\n\n\n  \n\n\n\nCode\nplot(lnstr_sf[\"cars_per_hour\"])\n\n\n\n\n\n\n\n\n\nCode\nggplot(lnstr_sf) + \n  geom_sf(aes(color = cars_per_hour))\n\n\n\n\n\n\n\n\n\nUseful methods in sf:\n\n\nCode\nmethods(class = \"sf\")\n\n\n  [1] [                            [[&lt;-                        \n  [3] [&lt;-                          $&lt;-                         \n  [5] aggregate                    anti_join                   \n  [7] arrange                      as.data.frame               \n  [9] cbind                        coerce                      \n [11] dbDataType                   dbWriteTable                \n [13] distinct                     dplyr_reconstruct           \n [15] drop_na                      duplicated                  \n [17] filter                       full_join                   \n [19] gather                       group_by                    \n [21] group_split                  identify                    \n [23] initialize                   inner_join                  \n [25] left_join                    merge                       \n [27] mutate                       nest                        \n [29] pivot_longer                 pivot_wider                 \n [31] plot                         points                      \n [33] print                        rbind                       \n [35] rename_with                  rename                      \n [37] right_join                   rowwise                     \n [39] sample_frac                  sample_n                    \n [41] select                       semi_join                   \n [43] separate_rows                separate                    \n [45] show                         slice                       \n [47] slotsFromS3                  spread                      \n [49] st_agr                       st_agr&lt;-                    \n [51] st_area                      st_as_s2                    \n [53] st_as_sf                     st_as_sfc                   \n [55] st_bbox                      st_boundary                 \n [57] st_break_antimeridian        st_buffer                   \n [59] st_cast                      st_centroid                 \n [61] st_collection_extract        st_concave_hull             \n [63] st_convex_hull               st_coordinates              \n [65] st_crop                      st_crs                      \n [67] st_crs&lt;-                     st_difference               \n [69] st_drop_geometry             st_exterior_ring            \n [71] st_filter                    st_geometry                 \n [73] st_geometry&lt;-                st_inscribed_circle         \n [75] st_interpolate_aw            st_intersection             \n [77] st_intersects                st_is_full                  \n [79] st_is_valid                  st_is                       \n [81] st_join                      st_line_merge               \n [83] st_m_range                   st_make_valid               \n [85] st_minimum_bounding_circle   st_minimum_rotated_rectangle\n [87] st_nearest_points            st_node                     \n [89] st_normalize                 st_point_on_surface         \n [91] st_polygonize                st_precision                \n [93] st_reverse                   st_sample                   \n [95] st_segmentize                st_set_precision            \n [97] st_shift_longitude           st_simplify                 \n [99] st_snap                      st_sym_difference           \n[101] st_transform                 st_triangulate_constrained  \n[103] st_triangulate               st_union                    \n[105] st_voronoi                   st_wrap_dateline            \n[107] st_write                     st_z_range                  \n[109] st_zm                        summarise                   \n[111] text                         transform                   \n[113] transmute                    ungroup                     \n[115] unite                        unnest                      \nsee '?methods' for accessing help and source code",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "09-geospat.html#raster-data",
    "href": "09-geospat.html#raster-data",
    "title": "9  Geospatial Data",
    "section": "9.4 Raster Data",
    "text": "9.4 Raster Data\nRaster data represent spatial information as a grid of cells (or pixels), where each cell stores a value for a specific variable such as elevation, temperature, or land cover. The cells are arranged in rows and columns, covering a continuous area of space.\nRaster data are typically used for continuous phenomena that vary smoothly across space. The spatial resolution depends on the cell size: smaller cells capture finer detail but require more storage.\nCommon raster formats include GeoTIFF and .grd. In R, raster data are handled by packages such as raster and terra.\n\nRaster files, as you might know, have a much more compact data structure than vectors.\nA raster is defined by:\n\na CRS\ncoordinates of its origin\na distance or cell size in each direction\na dimension or numbers of cells in each direction\nan array of cell values\n\n\n\n\nCode\n## install.packages(\"raster\")\nlibrary(raster)\n\nHARV &lt;- raster(\"data/HARV_RGB_Ortho.tif\")\n\nHARV\n\n\nclass      : RasterLayer \nband       : 1  (of  3  bands)\ndimensions : 2317, 3073, 7120141  (nrow, ncol, ncell)\nresolution : 0.25, 0.25  (x, y)\nextent     : 731998.5, 732766.8, 4712956, 4713536  (xmin, xmax, ymin, ymax)\ncrs        : +proj=utm +zone=18 +datum=WGS84 +units=m +no_defs \nsource     : HARV_RGB_Ortho.tif \nnames      : HARV_RGB_Ortho_1 \nvalues     : 0, 255  (min, max)\n\n\nCode\nplot(HARV)\n\n\n\n\n\n\n\n\n\nThis raster is generated as part of the NEON Harvard Forest field site .\n\n\nCode\n## useful commands\n\ncrs(HARV)\n\n\nCoordinate Reference System:\nDeprecated Proj.4 representation:\n +proj=utm +zone=18 +datum=WGS84 +units=m +no_defs \nWKT2 2019 representation:\nPROJCRS[\"unknown\",\n    BASEGEOGCRS[\"unknown\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8901]]],\n    CONVERSION[\"UTM zone 18N\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",-75,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",0.9996,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",500000,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]],\n        ID[\"EPSG\",16018]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]] \n\n\nCode\nmethods(class = class(HARV))\n\n\n  [1] !                     !=                    [                    \n  [4] [[                    [[&lt;-                  [&lt;-                  \n  [7] %in%                  ==                    $                    \n [10] $&lt;-                   addLayer              adjacent             \n [13] aggregate             all.equal             area                 \n [16] Arith                 as.array              as.character         \n [19] as.data.frame         as.factor             as.integer           \n [22] as.list               as.logical            as.matrix            \n [25] as.raster             as.vector             asFactor             \n [28] atan2                 bandnr                barplot              \n [31] bbox                  blockSize             boundaries           \n [34] boxplot               brick                 buffer               \n [37] calc                  cellFromRowCol        cellFromRowColCombine\n [40] cellFromXY            cellStats             clamp                \n [43] click                 clump                 coerce               \n [46] colFromCell           colFromX              colSums              \n [49] Compare               contour               coordinates          \n [52] corLocal              couldBeLonLat         cover                \n [55] crop                  crosstab              crs&lt;-                \n [58] cut                   cv                    density              \n [61] dim                   dim&lt;-                 direction            \n [64] disaggregate          distance              extend               \n [67] extent                extract               flip                 \n [70] focal                 freq                  getValues            \n [73] getValuesBlock        getValuesFocal        gridDistance         \n [76] hasValues             head                  hist                 \n [79] image                 init                  initialize           \n [82] inMemory              interpolate           intersect            \n [85] is.factor             is.finite             is.infinite          \n [88] is.na                 is.nan                isLonLat             \n [91] KML                   labels                layerize             \n [94] length                levels                levels&lt;-             \n [97] lines                 localFun              log                  \n[100] Logic                 mask                  match                \n[103] Math                  Math2                 maxValue             \n[106] mean                  merge                 metadata             \n[109] minValue              modal                 mosaic               \n[112] names                 names&lt;-               ncell                \n[115] ncol                  ncol&lt;-                nlayers              \n[118] nrow                  nrow&lt;-                origin               \n[121] origin&lt;-              overlay               persp                \n[124] plot                  predict               print                \n[127] proj4string           proj4string&lt;-         quantile             \n[130] raster                rasterize             ratify               \n[133] readAll               readStart             readStop             \n[136] reclassify            rectify               res                  \n[139] res&lt;-                 resample              RGB                  \n[142] rotate                rowColFromCell        rowFromCell          \n[145] rowFromY              rowSums               sampleRandom         \n[148] sampleRegular         sampleStratified      scale                \n[151] select                setMinMax             setValues            \n[154] shift                 show                  spplot               \n[157] stack                 stackSelect           stretch              \n[160] subs                  subset                Summary              \n[163] summary               t                     tail                 \n[166] terrain               text                  trim                 \n[169] unique                update                values               \n[172] values&lt;-              Which                 which.max            \n[175] which.min             wkt                   writeRaster          \n[178] writeStart            writeStop             writeValues          \n[181] xFromCell             xFromCol              xmax                 \n[184] xmax&lt;-                xmin                  xmin&lt;-               \n[187] xres                  xyFromCell            yFromCell            \n[190] yFromRow              ymax                  ymax&lt;-               \n[193] ymin                  ymin&lt;-                yres                 \n[196] zonal                 zoom                 \nsee '?methods' for accessing help and source code",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "09-geospat.html#maps",
    "href": "09-geospat.html#maps",
    "title": "9  Geospatial Data",
    "section": "9.5 Maps",
    "text": "9.5 Maps\n\nWe are going to create a few different types of maps, which will require different R packages.\n\nStatic Maps\n\nMost common type of visual output\n\nplot() most commonly used command from base are\nCan utilize ggplot() and tmap()\n\n\nAnimated Maps\n\nAre becoming more widely used than in the past\n\nWe will make these with tmap()\n\n\nInteractive Maps\n\nWhile static and animated maps can enliven geographic datasets, interactive maps can take them to a new level\n\nleaflet()\n\n\n\n\n\n9.5.1 Plotting Simple Features (sf)\n\nAs we have already briefly seen, the sf package extends the base plot command, so it can be used on sf objects.\nWe are going to create a choropleth map with sf\n\n\n\nCode\nlibrary(sf)\n\nphilly_crimes_sf &lt;- st_read(\"data/PhillyCrimerate.shp\")\n\n\nReading layer `PhillyCrimerate' from data source \n  `/Users/junyan/work/teaching/1010-f25/1010f25/data/PhillyCrimerate.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 384 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 1739497 ymin: 457343.7 xmax: 1764030 ymax: 490544.9\nProjected CRS: Albers\n\n\nCode\nglimpse(philly_crimes_sf)\n\n\nRows: 384\nColumns: 5\n$ GEOID10    &lt;chr&gt; \"42101000100\", \"42101000200\", \"42101000300\", \"42101000401\",…\n$ n_homic    &lt;int&gt; 10, 4, 7, 9, 6, 9, 1, 3, 1, 1, 1, 2, 1, 1, 4, 1, 1, 1, 3, 2…\n$ tract_area &lt;dbl&gt; 704688.5, 381197.4, 550949.2, 231327.5, 303868.9, 427352.7,…\n$ homic_rate &lt;dbl&gt; 14.190667, 10.493252, 12.705346, 38.905882, 19.745355, 21.0…\n$ geometry   &lt;MULTIPOLYGON [m]&gt; MULTIPOLYGON (((1752148 467..., MULTIPOLYGON (…\n\n\nCode\nplot(philly_crimes_sf)\n\n\n\n\n\n\n\n\n\nNotice that the plot function will try and provide a map of every variable in the data set.\nTo plot a single attribute, we need to provide an object of class sf.\n\n\nCode\nplot(philly_crimes_sf$homic_rate) # this is a numeric vector!\n\n\n\n\n\n\n\n\n\nCode\nplot(philly_crimes_sf[\"homic_rate\"])\n\n\n\n\n\n\n\n\n\nBased on the plot above, it would seem that we have a lot of “empty” geographic units or unevenly distributed\n\n\nCode\nggplot(philly_crimes_sf) +\n  geom_histogram(mapping = aes(x = homic_rate))\n\n\n\n\n\n\n\n\n\nLet’s break the plot up into quantiles to better distinguish the census tracts with low values.\n\n\nCode\nplot(philly_crimes_sf[\"homic_rate\"], \n     main = \"Philadelphia homicide density per square km\", \n     breaks = \"quantile\")\n\n\n\n\n\n\n\n\n\n\n\n9.5.2 Mapping with ggplot()\nPackage ggplot is a widely used and powerful plotting library for R as we have utilized over the course of the semester. It is not specifically geared towards mapping, but one can generate great maps.\n\n\nCode\nggplot(philly_crimes_sf) + \n  geom_sf(aes(fill=homic_rate))\n\n\n\n\n\n\n\n\n\nHomicide rate is a continuous variable and is plotted by ggplot as such. If we wanted to plot our map as a ‘true’ choropleth map we need to convert our continuous variable into a categorical one, according to whichever brackets we want to use.\n\n\nCode\nbreaks_qt &lt;- classInt::classIntervals(\n                           c(min(philly_crimes_sf$homic_rate) - .00001,\n                             philly_crimes_sf$homic_rate),\n                           n = 7, style = \"quantile\")\n\nphilly_crimes_sf &lt;- mutate(philly_crimes_sf,\n                           homic_rate_cat = cut(homic_rate, breaks_qt$brks)) \n\nggplot(philly_crimes_sf) + \n    geom_sf(aes(fill=homic_rate_cat)) +\n    scale_fill_brewer(palette = \"OrRd\") \n\n\n\n\n\n\n\n\n\n\n\n9.5.3 Mapping with tmap()\nPackage tmap is based on the idea of a ‘grammar of graphics’ similar to ggplot This involves a separation between the input data and the aesthetics (how data are visualized): each input dataset can be ‘mapped’ in a range of different ways including location on the map (defined by data’s geometry), color, and other visual variables. The package is specifically designed to make creation of thematic maps more convenient with ggplot syntax.\n\ntm_shape() defines the data, raster, and vector objects.\ntm_polygons() sets the attribute variable to map, the break style, and a title.\ntm_fill() & tm_borders() control the layers.\n\n\n\nCode\nlibrary(tmap)\n\nm &lt;- tm_shape(philly_crimes_sf) +\n    tm_polygons(\n        \"homic_rate\",\n        fill.scale = tm_scale_intervals(style = \"quantile\", n = 4),\n        fill.legend = tm_legend(\"Philadelphia \\nhomicide density \\nper sqKm\")\n    )\n\nm\n\n\n\n\n\n\n\n\n\nPackage tmap has a nice feature that allows us to give basic interactivity to the map. We can switch from plot mode into view mode and call the plot.\n\n\nCode\ntmap_mode(\"view\")\n\nm\n\n\n\n\n\n\n\nLayers can be controlled by tm_fill() & tm_borders().\n\n\nCode\n#\n#install.packages(\"spData\")\nlibrary(spData)\nlibrary(tmap)\n\n# Add fill layer to nz shape\ntm_shape(nz) +\n  tm_fill() \n\n\n\n\n\n\nCode\n# Add border layer to nz shape\ntm_shape(nz) +\n  tm_borders() \n\n\n\n\n\n\nCode\n# Add fill and border layers to nz shape\ntm_shape(nz) +\n  tm_fill() +\n  tm_borders() \n\n\n\n\n\n\nA useful feature of tmap is its ability to store objects representing maps\n\n\nCode\nmap_nz = tm_shape(nz) + tm_polygons()\n\nclass(map_nz)\n\n\n[1] \"tmap\"\n\n\n\nThis allows you to plot your maps later and/or add additional layers\n\n\n\nCode\nmap_nz1 &lt;- map_nz + \n    tm_polygons(\"Population\", style = \"quantile\", \n                palette = \"viridis\", \n                title = \"Population (by region)\") +\n    tm_layout(frame = FALSE)\n\nmap_nz1\n\n\n\n\n\n\n\nThere are no limits to the number of layers or shapes that can be added to tmap objects. Additionally, tmap allows multiple map objects to be arranged in a single plot with tmap_arrange()\n\n\nCode\nmap_nz2 &lt;- tm_shape(nz) +\n  tm_polygons(\"Median_income\", style = \"pretty\", palette = \"brewer.blues\",\n              title = \"Median income\")\n\n# combined\ntmap_arrange(map_nz1, map_nz2) #, map_nz3)\n\n\nAnimation is a feature we need to find a dataset to demonstrate, possibly through the NYC crash data.\nurb_1970_2030 &lt;- urban_agglomerations |&gt;\n  filter(year == 1970| year == 1990 | year == 2010 | year == 2030) \n\n\ntm_shape(world) +\n  tm_polygons() +\n  tm_shape(urb_1970_2030) +\n  tm_symbols(col = \"black\", border.col = \"white\", size = \"population_millions\") +\n  tm_facets(by = \"year\", nrow = 2, free.coords = FALSE)\n\n\nurb_anim = tm_shape(world) + tm_polygons() + \n  tm_shape(urban_agglomerations) + tm_dots(size = \"population_millions\") +\n  tm_facets(along = \"year\", free.coords = FALSE)\n\n\nlibrary(gifski)\n\ntmap_animation(urb_anim, filename = \"urb_anim.gif\", delay = 25)\n\n\n\n\n\n\n\n\n\n\n\n9.5.4 Mapping with leaflet()\nPackage leaflet provides bindings to the Leaflet JavaScript library, “the leading open-source JavaScript library for mobile-friendly interactive maps.” We have already seen a simple use of leaflet in the tmap example.\nLet’s build up the map step by step\n\n\nCode\nlibrary(leaflet) \n\n# reproject\nphilly_WGS84 &lt;- st_transform(philly_crimes_sf, 4326)\n\nleaflet(philly_WGS84) %&gt;%\n  addPolygons()\n\n\n\n\n\n\nTo map the homicide density we use addPolygons()\n\n\nCode\npal_fun &lt;- colorQuantile(\"YlOrRd\", NULL, n = 5)\n\np_popup &lt;- paste0(\"&lt;strong&gt;Homicide Density: &lt;/strong&gt;\", philly_WGS84$homic_rate)\n\nm &lt;- leaflet(philly_WGS84) %&gt;%\n    addPolygons(\n        stroke = FALSE, # remove polygon borders\n        fillColor = ~pal_fun(homic_rate), # set fill color with function from above and value\n        fillOpacity = 0.8, smoothFactor = 0.5, # make it nicer\n        popup = p_popup)  # add popup\n\nm\n\n\n\n\n\n\nWe can add a basemap, which defaults to OSM, with addTiles()\n\n\nCode\nm &lt;- addTiles(m)\nm\n\n\n\n\n\n\nLastly, we add a legend with addLegend()\n\n\nCode\nm &lt;-  addLegend(m,\n                \"bottomright\",  # location\n                pal=pal_fun,    # palette function\n                values=~homic_rate,  # value to be passed to palette function\n                title = 'Philadelphia homicide density per sqkm') # legend title\n\nm",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "09-geospat.html#section",
    "href": "09-geospat.html#section",
    "title": "9  Geospatial Data",
    "section": "9.6 ",
    "text": "9.6",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Geospatial Data</span>"
    ]
  },
  {
    "objectID": "10-shiny.html",
    "href": "10-shiny.html",
    "title": "10  Shiny Apps",
    "section": "",
    "text": "10.1 Introduction\nShiny is an R package for building interactive web applications without requiring any web development experience.\nIt is ideal for dashboards, teaching tools, and interactive reporting.\nLearning Resources:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shiny Apps</span>"
    ]
  },
  {
    "objectID": "10-shiny.html#introduction",
    "href": "10-shiny.html#introduction",
    "title": "10  Shiny Apps",
    "section": "",
    "text": "RStudio official tutorial\nMastering Shiny by Hadley Wickham\nShiny Gallery\n\n\n\n\n\n\n\nTip\n\n\n\nQuick Start\nTo try Shiny, open R and run:\nshiny::runExample(\"01_hello\")\nThis launches a simple interactive histogram app.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shiny Apps</span>"
    ]
  },
  {
    "objectID": "10-shiny.html#structure-of-a-shiny-app",
    "href": "10-shiny.html#structure-of-a-shiny-app",
    "title": "10  Shiny Apps",
    "section": "10.2 Structure of a Shiny App",
    "text": "10.2 Structure of a Shiny App\nA Shiny app links UI (User Interface) and Server (Computation)\nthrough reactive communication.\n# app.R: single-file app\nlibrary(shiny)\n\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(\"obs\", \"Observations:\", 1, 100, 50)\n    ),\n    mainPanel(\n      plotOutput(\"distPlot\")\n    )\n  )\n)\n\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    hist(rnorm(input$obs))\n  })\n}\n\nshinyApp(ui, server)\n\n\n\n\n\n\nNote\n\n\n\nHow it works\n\nThe UI defines layout and input/output elements.\n\nThe Server defines computations and reactivity.\n\nThe shinyApp(ui, server) call connects them.\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTwo-file alternative\nYou may also structure an app with two files in the same folder:\n\nui.R: defines ui\nserver.R: defines server\n\nThis helps with larger apps and collaboration.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shiny Apps</span>"
    ]
  },
  {
    "objectID": "10-shiny.html#inputs-and-outputs",
    "href": "10-shiny.html#inputs-and-outputs",
    "title": "10  Shiny Apps",
    "section": "10.3 Inputs and Outputs",
    "text": "10.3 Inputs and Outputs\nInputs are widgets through which users interact.\nOutputs display dynamically updated results.\n\n\n\nFunction\nType\nExample\n\n\n\n\ntextInput()\nInput\nText boxes\n\n\nsliderInput()\nInput\nSliders for numeric values\n\n\nselectInput()\nInput\nDrop-down menus\n\n\nrenderPlot()\nOutput\nDisplays plots\n\n\nrenderTable()\nOutput\nDisplays data tables\n\n\nrenderText()\nOutput\nDisplays text summaries\n\n\n\ninput$slider  # numeric\ninput$check   # logical\n\n\n\n\n\n\nTip\n\n\n\nWidget Gallery\nSee the full input/output list at\nhttps://gallery.shinyapps.io/081-widgets-gallery/.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shiny Apps</span>"
    ]
  },
  {
    "objectID": "10-shiny.html#reactivity",
    "href": "10-shiny.html#reactivity",
    "title": "10  Shiny Apps",
    "section": "10.4 Reactivity",
    "text": "10.4 Reactivity\nReactivity is the core concept of Shiny:\noutputs automatically update when inputs change.\n\n10.4.1 Reactive Expressions\nrv &lt;- reactive({ rnorm(input$obs) })\noutput$plot &lt;- renderPlot(hist(rv()))\n\n\n10.4.2 isolate()\nBreaks the reactive chain — useful when you want\na calculation to occur only on a trigger (e.g., a button).\noutput$summary &lt;- renderText({\n  paste0(\n    \"input$text is '\", input$text,\n    \"', and input$n is \", isolate(input$n)\n  )\n})\n\n\n10.4.3 observeEvent()\nObservers perform side-effect actions (e.g., pop-ups, saving files).\nobserveEvent(input$go, {\n  showModal(modalDialog(\"You clicked Go!\"))\n})\n\n\n\n\n\n\nNote\n\n\n\nTip:\nUse reactive() for values, observe() for actions.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shiny Apps</span>"
    ]
  },
  {
    "objectID": "10-shiny.html#deployment",
    "href": "10-shiny.html#deployment",
    "title": "10  Shiny Apps",
    "section": "10.5 Deployment",
    "text": "10.5 Deployment\nDeploying a Shiny app means making it available to others.\n\n10.5.1 Traditional Deployment\n\nshinyapps.io\nPosit Connect (enterprise hosting)\nPrivate server (e.g., university or company)\n\n\n\n\n10.5.2 Shinylive Deployment\nShinylive runs entirely in the browser (no R server needed).\ninstall.packages(\"shinylive\")\nshinylive::export(\"app\", \"site\")\nThen deploy the site folder like a static website.\n\n\n\n\n\n\nTip\n\n\n\nLimitations of Shinylive\n\nNot all R packages are supported.\n\nInitial loading may be slow for large apps.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shiny Apps</span>"
    ]
  },
  {
    "objectID": "10-shiny.html#best-practices",
    "href": "10-shiny.html#best-practices",
    "title": "10  Shiny Apps",
    "section": "10.6 Best Practices",
    "text": "10.6 Best Practices\n\nWrite automated tests with\nshinytest2\nBuild apps as packages for easier dependency management.\nUse modules to reuse UI and server components.\nLearn a little HTML/CSS/JS for customization.\nStart with a sketch—plan your app’s user experience before coding.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shiny Apps</span>"
    ]
  },
  {
    "objectID": "10-shiny.html#hands-on-exercise",
    "href": "10-shiny.html#hands-on-exercise",
    "title": "10  Shiny Apps",
    "section": "10.7 Hands-on Exercise",
    "text": "10.7 Hands-on Exercise\n\n\n\n\n\n\nTip\n\n\n\nExercise: Build Your First App\n\nCreate a new file called app.R\n\nDefine ui and server as shown above\n\nAdd shinyApp(ui, server) at the end\n\nRun the app and change the slider to see live updates",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Shiny Apps</span>"
    ]
  },
  {
    "objectID": "exercises.html",
    "href": "exercises.html",
    "title": "11  Exercises",
    "section": "",
    "text": "Setting up the computing environment Using the right computing tools and environment is a prerequisite of data science projects. Set up your computer for this course with the following steps. For each step, document what you did, the obstacles you encountered, and how you overcame them. If you used AI, document your prompts. Note that the steps you take may depend on your computer’s operating system.Think of this as a user manual for students who are new to this. Use the command line interface.\n\nInstall R.\nInstall Positron or RStudio.\nInstall Quarto.\nSet up SSH authentication between your computer and your GitHub account.\nRender your homework into an HTML file.\nPrint the HTML file into a pdf file and put it into the release of this homework assignment.\n\nGetting familiar with command line interface The command line interface (CLI) is widely used among computing professionals, even though graphical user interfaces (GUI) are more common for everyday users. Be clear and concise in your explanations. Provide short examples if you think they will help illustrate your points.\n\nResearch and explain why many professionals prefer using the CLI over a GUI. Consider aspects such as efficiency, automation, reproducibility, or remote access.\nIdentify your top five favorite commands in the Unix/Linux shell. For each command, explain what it does. If there is an option/flag you found particularly useful, describe it.\nIdentify your top five favorite Git commands. For each command, explain what it does. If there is an option/flag you found particularly useful, describe it.\n\nDo women promote different policies than men? Chattopadhyay & Duflo (2004) studied whether female policymakers make different choices than male policymakers by exploiting a unique natural experiment in India. India’s 1993 constitutional amendment required that one-third of village council head positions (pradhans) be randomly reserved for women, creating a setting where the assignment of female leaders was exogenous. In the data here, villages were randomly assigned to have a female council head. The dataset is in the source of the class notes “data/india.csv”. As shown in Table 11.1, the dataset contains four variables.\n\n\n\nTable 11.1: Variables in india.csv\n\n\n\n\n\n\n\n\n\nvariable\ndescription\n\n\n\n\nvillage\nvillage identifier (“Gram Panchayat number _ village number”)\n\n\nfemale\nwhether village was assigned a female politician: 1 = yes, 0 = no\n\n\nwater\nnumber of new (or repaired) drinking water facilities in the village\n\n\n\nsince random assignment\n\n\nirrigation\nnumber of new (or repaired) irrigation facilities in the village\n\n\n\nsince random assignment\n\n\n\n\n\n\n\nUsing the correct R code, set your working directory.\nLoad the tidyverse package.\nLoad the data as a data.frame and assign the name india to it.\nUtilizing either head() or glimpse() view the first few rows of the dataset. Substantively describe what these functions do.\nWhat does each observation in this dataset represent?\nSubstantively interpret the first observation in the dataset.\nFor each variable in the dataset, identify the type of variable (character vs. numeric binary vs. numeric non-binary).\nHow many observations are in the dataset? In other words, how many villages were part of this experiment? Additionally, provide a substantive answer.\n\nLand of opportunity in the US Chetty et al. (2014) show that children’s chances of rising out of poverty in the U.S. vary sharply across commuting zones, with higher mobility in areas with less segregation, less inequality, stronger schools, more two-parent families, and greater social capital. The data and its variable dictionary are available as data/Chetty_2014.csv and data/Chetty_2014_dict.csv. Here we look into a subset of the data, exploring the relationship between economic mobility and CZ characteristics: household income per capita (hhi_percap). The mobility measure that you will use in this analysis captures the probability that a child born to a parent in quintile 1 moves to income quintile 5 as an adult (prob_q1q5).\n\nRead and filter data to get data only for the 100 largest commuting zones (CZ). You will work with filtered data, chetty_top100, throughout this lab.\nMake a scatterplot with household income per capita (hhi_percap) on the x-axis, and mobility (abs_mobility) on the y-axis. (A) Describe the graph: what is the approximate range of the x-axis? (B) What is the approximate range of the y-axis? (C) Do you think there is a relationship between these two variables?\nUse color to represent the geographic region (region) to your scatterplot. (A) What patterns does this reveal? (B) Describe the distribution of the data, by region.\nRepresent geographic region (region) on your scatterplot using shape instead of color. Compare the use of color vs shape to represent the region: what are the benefits and drawbacks of each?\nGoing back to the graph you just made, which uses color to represent the geographic region, add another aesthetic to represent the size of the population (pop_2000, population from the 2000 Census). Describe any relationships between size and region.\nSplit your plot into facets to display scatterplots of your data by region. (A) Compare this split plot to the combined plot earlier. Are there aspects of the relationship between hhi_percap and mobility that are easier to detect in the faceted plot than in the combined plot? (B) Which regions appear to have a relatively stronger relationship between hhi_percap and mobility?\nAdd information on the census division (division) to your graph using the color aesthetic. (A) What does this reveal about divisional differences in the West?\nCreate a plot of the relationship between hhi_percap and abs_mobility with two layers: (1) A scatterplot colored by region, and (2) a smooth fit chart with no standard error also colored by region. (A) What patterns does this illustrate in the data?\nCreate a bar graph that displays the count of CZs by region and fill each bar using information on census division. What do you learn from this graph? (A) Make new bar graphs with position dodge. (B) Make new bar graphs with position fill.\n\n\nWhat is the relative advantage of each of the three bar graphs?\n\nConnecticut schools Data on schools from the Common Core of Data (CCD) are collected by the National Center for Education Statistics. Information about the CCD can be found here. We are working with a subset of the 2013-14 dataset, data/ct_schools in the classnotes repo. A variable codebook for the CCD, which will explain what each column represents in the data file is located in the flat file in the record layout column and 2013-14 row.\n\nLoad the appropriate packages and load the data.\nData manipulation.\n\nMake a new variable sch_type that has the value Charter, Magnet or TPS, to specify if the school is a charter, magnet or traditional public school.\nHow many schools of each of these three types are in our area?\nMake a table that show the number of schools of each of the three school types that are missing data for free lunch.\n\nDescriptives.\n\nHow many schools are elementary schools? Middle schools? High schools? Other? Missing?\nHow many schools are eligible for Title I status? Summarize the count of schools in each category\n\n\nRacial composition.\n\nCreate new variables that compute the percentage of students who are Black, White, Hispanic, Asian or another race (call this variable “other”), and the percentage of students receiving free OR reduced price lunch.\nVisualize these data to see the variability of each of these variables (Hint: Use geom_histogram).\n\nVisualize the variation in the percent of students receiving free lunch (frelch) for magnet schools, TPS, and charter schools (Hint: Use geom_boxplot).\nVisualize the variation in the percent of students of each race and ethnicity in the data file (Black, White, Hispanic, Asian and Other) for each of the type of schools (magnet, TPS and charter).\nWith more R skills what types of questions could you answer using this dataset?\nWhat are some questions you have about this dataset? What information would you add to the dataset, if you could?\n\nEarly care and education, NC, 2007–2014 The data on early care and education were carefully collected by Scott Latham and colleagues at the Stanford Center for Educational Policy Analysis, in support of a recent publication, available here. Additional background on North Carolina’s Quality Rating and Improvement System (QRIS) can be found here. We have a copy in the classnotes repo: data/NC_ECE_2007-2014.csv. You can download it into your homework repo, but please do not add it to your repo; it’s over 11MB.\nInstead of a formal online codebook, Professor Latham provided the following description:\n\n“We have a panel of all licensed child care providers in North Carolina from 2007 to 2014. This includes both center-based providers and family child care homes. For all providers, we have county identifiers and zip codes. We also know facility types (e.g., independent, Head Start, local public school), enrollment, capacity, and some zip-code-level demographics (e.g., percentage below poverty, percentage Black, percentage Hispanic). For most providers, we have information on quality as measured by North Carolina’s Quality Rating and Improvement System (QRIS). However, many of these indicators are not readily interpretable because they are tied to the QRIS rubric (e.g., a 1–7 measure of teacher/staff education and credentials). The one quality measure that is relatively straightforward to interpret is the ERS rating—a widely used measure of observed classroom quality. These are elective, so we only have them for a subset of providers.”\n\n\nLoad the needed packages and the data. Take a glimpse of the data.\nMake a new data frame called nc14 that contains all NC facilities in the year 2014, with the variables fname, zip, ftype, p_pov, med_income, QRIS_ERS, and a new variable p_cap = enroll/capacity. You will use this data frame in the remaining questions.\nMake a new data frame that shows the number of facilities evaluated in each zip code in 2014, and the value of p_pov for each zip code.\n\nSort by decreasing order in the number of facilities.\nMake a plot from the data frame you made to demonstrate variation in percent poverty across zip codes in NC.\nExplain why your plot in is different from a plot that demonstrates variation in p_pov in the original data frame.\n\nAdd a new variable to nc14 that lumps together various facility types into five groups, based on the value of ftype, as follows: Independent; Franchise; Religious sponsored; Federal, Head Start or Local public school; All others.\n\nMake a plot to show the variation in p_cap across these five groups.\n\nMake a boxplot to visualize covariation between QRIS_ERS and med_income, binning med_income to treat it as a categorical variable, and representing the number of observations in each category by the width of the corresponding box.\nVisualize the relationship between med_income and QRIS_ERS using a scatter plot.\n\nVisualize the same relationship using geom_bin2d.\nDiscuss the pros and cons of each of these visualizations.\n\n\nQuality of life, NC This project was created by the UNCC Urban Institute in collaboration with the city of Charlotte, Mecklenburg County, and nearby townships. This dataset describes how various aspects of Charlotte residents’ “quality of life” varies according to neighborhood. You can explore and learn more about this project here. The data is data/qol_PS3.csv. We explore how adolescent birth rate and income are related at the neighborhood level in Mecklenburg county.\n\nLoad the appropriate packages and load the data.\nVisualize the covariation between the rate of adolescent births and median household income. What do you observe about the relationship between these variables?\nCreate a boxplot of the rate of adolescent birth by household income quartile. Use the features of the boxplot (i.e, the bar, box, whiskers and dots) to interpret your findings and discuss the pros and cons of this visualization, including breaking income up by quartile.\nUse the “cut_width” strategy for binning the household income variable. Discuss the pros and cons of this visualization.\nCreate a visualization that compares the distributions of the rate of adolescent birth for low-income neighborhoods (with a median HHI of 24257), and high-income neighborhoods (with a median HHI of 48514).\nVisualize the combined distribution of adolescent births, household income, and access to adequate prenatal care. Note that you may want to bin one or more of these variables to visualize these relationships, and you will need to find a way to represent a third variable.\nDescribe why you decided to assign variables to the asthetics you chose. How did these choices help you tell a\ncompelling story with this visualization?\n\nQuality of life, NC (continued) Conduct your own exploratory data analysis (EDA). Your EDA will be graded on your choice of an appropriate research question, the clarity of your visualizations, and how well your visualizations allow you to recognize patterns in your data and provide insight into the question you have posed.\n\nPose and answer at minimum four iterative, related EDA questions, using a minimum of 3 different variables (as in the last exercise). Write as though you were guiding future students through an exploration of the data, along with an answer key. Be sure to clearly state your EDA question, explain why you chose the visualizations that you did, and describe your findings.\nWhat were the easiest and hardest parts of this assignment?\nWith more R skills what types of questions could you answer using this dataset?\nWhat are some questions you have about this dataset? What information would you add to the dataset, if you could?\n\nRole of colleges in economic mobility This exercise is based on a data set from Chetty et al., the same author team as the economic mobility data we have worked with, but a different paper and different data that examines the role of colleges in economic mobility; see data/mrc.csv. If you wish, you can read about this study here.\n\nLoad the appropriate packages and the data and take a glimpse of the data.\nGet to know the data.\n\nWhat does each row in this data set represent?\nMake a table of the number of colleges in each state, sorted by number.\nExplain why there are more than 50 rows in the table you just produced even though there are only 50 states, and write code to show what are in these extra rows.\nFor the four CZs in New York with the most colleges, output a table of the CZ name, number of colleges in the CZ, and the average value of par_median in the CZ, sorted by the number of colleges in the CZ.\n\nCompare parent and child median income.\n\nMake a new variable called pk_ratio that is the ratio of parent median income to child median income, and another new variable to indicate if this ratio is low (&lt;=2), medium (between 2 and 3), or high (&gt;= 3).\nGraphically display the number of colleges with low, medium and high parent to child median income by CZ in New York\n\nVisualize income mobility.\n\nGraphically display the number of colleges in each state.\nGraphically display the distribution of mr_kq5_pq1.\n\nVisualize the variability in mr_kq5_pq1 by CZ name in New York and describe your visualization.\nVisualize the relationship between pk_ratio, mr_kq5_pq1, and trend_parq1 using geom_boxplot Describe your visualization.\n\nNYC crashes in the week of Labor Day, 2025 The data come from the NYC Open Data portal: Motor Vehicle Collisions–Crashes. This dataset contains information reported to the NYPD for motor vehicle crashes, available as data/nyc_crashes_lbdwk_2025.csv. For this exercise, a subset covering Labor Day week 2025 was downloaded. It includes 29 variables, which are explained at the portal. The raw data contain missing values in several fields.\n\nImporting the data.\n\nRead the data into R and take a glimpse.\nThe variable names are all upper-cases, with spaces replaced with periods when imported. Rename the variables to use lower cases, with underscores in places of spaces (periods).\nAre all the observations really in our target time frame? If not, filter out those are not and use the filtered data for the rest of the exercise.\nDo you need the location variable in the data? What suggestion would you give to the data curator?\n\nMissing values.\n\nAre there borough values that should be coded as NA? If so, recode them.\nAre there unreasonable geocode (lattitude/longitude) that should be coded as NA? If so, recode them.\nHow many records have geocode missing?\nHow many zip code missing and borough missing occur simultaneously?\nCreate a logical variable fillable_zip, which is TRUE when the geocode is not missing but zip code is missing. Compare the rate of fillable zip across the seven days.\n\nData exploration.\n\nCreate a variable hour to store the hour in which a collision occurs.\nPlot the number of crashes by hour by borough, Summarize the patterns in the figure (e.g., are there more crashes during rush hours?).\nHow many crashes occurred at exactly midnight? How about whole hours? Is this a metter of bad luck when the clock strikes midnight?\nCreate a logical variable business_day, which is TRUE if the day is a business day and FALSE otherwise.\nPlot the number of crashes by business day by borough. Summarize your observations.\n\nSeverity analysis.\n\nCreate a logical variable severe, which is TRUE if a crash involved at least one person injured or one person killed.\nCreate a count variable n_vehicles to store the number of vehicles involved for the crashes.\nCreate a contingency table of severe and n_vehicles. Summarize your observations.\nCreate a contingency table of severe and hour. Summarize your observations.\nIdentify the top 10 severe crashes. Are some of them geographically clustered?\n\nContributing factors.\n\nCreate a frequency table for the contributing factors for vehicle 1.\nAre there contributing factors differ only in cases? If so, convert upper cases to lower cases.\nWhat are the top five contributing factors?\nShould any values in the frequency table be considered NA?\nWhat are the most frequent vehicle categories in vehicle 1 and vehicle 2? Comment on the results.\n\n\n\n\n\n\n\nChattopadhyay, R., & Duflo, E. (2004). Women as policy makers: Evidence from a randomized policy experiment in india. Econometrica, 72(5), 1409–1443. https://doi.org/10.1111/j.1468-0262.2004.00539.x\n\n\nChetty, R., Hendren, N., Kline, P., & Saez, E. (2014). Where is the land of opportunity? The geography of intergenerational mobility in the United States. Quarterly Journal of Economics, 129(4), 1553–1623. https://doi.org/10.1093/qje/qju022",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Exercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Chattopadhyay, R., & Duflo, E. (2004). Women as policy makers:\nEvidence from a randomized policy experiment in india.\nEconometrica, 72(5), 1409–1443. https://doi.org/10.1111/j.1468-0262.2004.00539.x\n\n\nChetty, R., Hendren, N., Kline, P., & Saez, E. (2014). Where is the\nland of opportunity? The geography of intergenerational mobility in the\nUnited States. Quarterly Journal of\nEconomics, 129(4), 1553–1623. https://doi.org/10.1093/qje/qju022\n\n\nLee, M., & Lee, J. (2020). Trend and return level of extreme snow\nevents in New York City. The\nAmerican Statistician, 74(3), 282–293. https://doi.org/10.1080/00031305.2019.1592780\n\n\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley.\n\n\nVanderPlas, J. (2016). Python data science handbook:\nEssential tools for working with data. O’Reilly Media,\nInc.\n\n\nWickham, H. (2016). ggplot2: Elegant\ngraphics for data analysis. Springer-Verlag New York. https://ggplot2.tidyverse.org\n\n\nWilkinson, L. (1999). The grammar of graphics. Springer-Verlag\nNew York, Inc.",
    "crumbs": [
      "References"
    ]
  }
]